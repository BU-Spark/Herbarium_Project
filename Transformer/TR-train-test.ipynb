{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75e32d10-27a0-4dcb-afa3-dbdb0183dc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n",
      "\u001b[K     |████████████████████████████████| 451 kB 592 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /share/pkg.7/tensorflow/2.5.0/install/lib/python3.8/site-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr4/dl523/dong760/.local/lib/python3.8/site-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /share/pkg.7/tensorflow/2.5.0/install/lib/python3.8/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /share/pkg.7/tensorflow/2.5.0/install/lib/python3.8/site-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: aiohttp in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: pandas in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from datasets) (1.2.4)\n",
      "Requirement already satisfied: multiprocess in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from datasets) (0.70.11.1)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 670 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dill<0.3.7 in /share/pkg.7/tensorflow/2.5.0/install/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: packaging in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from datasets) (20.9)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-10.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 36.0 MB 167 kB/s eta 0:00:01    |████▋                           | 5.2 MB 143 kB/s eta 0:03:34     |██████████████████████▌         | 25.3 MB 190 kB/s eta 0:00:57     |████████████████████████▌       | 27.5 MB 259 kB/s eta 0:00:33\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "\u001b[K     |████████████████████████████████| 139 kB 192 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: async-timeout<4.0,>=3.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /share/pkg.7/tensorflow/2.5.0/install/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /share/pkg.7/tensorflow/2.5.0/install/lib/python3.8/site-packages (from aiohttp->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /share/pkg.7/tensorflow/2.5.0/install/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: filelock in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/pkg.7/tensorflow/2.5.0/install/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/pkg.7/tensorflow/2.5.0/install/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /share/pkg.7/tensorflow/2.5.0/install/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /share/pkg.7/tensorflow/2.5.0/install/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: tqdm, fsspec, xxhash, responses, pyarrow, datasets\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/usr4/dl523/dong760/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script plasma_store is installed in '/usr4/dl523/dong760/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script datasets-cli is installed in '/usr4/dl523/dong760/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed datasets-2.7.1 fsspec-2022.11.0 pyarrow-10.0.1 responses-0.18.0 tqdm-4.64.1 xxhash-3.1.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/share/pkg.7/python3/3.8.10/install/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a22626",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9f2fa3d637da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "#!pip install accelerate\n",
    "# !pip install datasets\n",
    "# !pip install jiwer\n",
    "#!pip install evaluate\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import load_metric\n",
    "from transformers import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Reading the training file into a DataFrame\n",
    "IAM_lines = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/tesseract-training/training/IAM/gt/lines/'\n",
    "IAM_words = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/tesseract-training/training/IAM/gt/words/'\n",
    "IAM_sentences = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/tesseract-training/training/IAM/gt/sentences/'\n",
    "\n",
    "model_directory = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/'\n",
    "# All files and directories ending with .txt and that don't begin with a dot:\n",
    "def get_lists(directory,directory_percentage):\n",
    "    image_list = glob.glob(directory+\"*.png\")\n",
    "\n",
    "    text_list = []\n",
    "    for image in image_list:\n",
    "        text_list.extend(open(image.split('.')[0]+'.gt.txt','r').read().splitlines())\n",
    "    # Take a random percentage of the data\n",
    "#     image_list, text_list = zip(*random.sample(list(zip(image_list, text_list)), round(directory_percentage/100*len(image_list))))\n",
    "    image_list, text_list = zip(*random.sample(list(zip(image_list, text_list)),directory_percentage))\n",
    "    return image_list,text_list\n",
    "\n",
    "\n",
    "# Taken from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_native_PyTorch.ipynb\n",
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, df, processor, max_target_length=128):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        image = self.df['image'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(image).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding\n",
    "\n",
    "\n",
    "image_list = []\n",
    "text_list = []\n",
    "lines_percentage = 1000\n",
    "words_percentage = 1000\n",
    "sentences_percentage = 1000\n",
    "for directory,percentage in zip([IAM_lines, IAM_words, IAM_sentences],[lines_percentage, words_percentage, sentences_percentage]):\n",
    "    images,text = get_lists(directory,percentage)\n",
    "    image_list.extend(images)\n",
    "    text_list.extend(text)\n",
    "    \n",
    "\n",
    "df = pd.DataFrame({'image':image_list,'text':text_list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "757b62c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from accelerate import Accelerator\n",
    "    \n",
    "#Setting up the accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "#Loading model and processor \n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\") \n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4\n",
    "\n",
    "# Datasets and dataloaders for train validation\n",
    "train_dataset = IAMDataset(df=train_df,processor=processor)\n",
    "val_dataset = IAMDataset(df=val_df,processor=processor)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "# Getting the accelerator set up\n",
    "model, optimizer, train_dataloader, val_dataloader= accelerator.prepare(\n",
    "    model, optimizer, train_dataloader,val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27ff4f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [03:19<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8028169014084507\n",
      "0.6236559139784946\n",
      "0.5027624309392266\n",
      "0.5597014925373134\n",
      "0.5675675675675675\n",
      "0.5741935483870968\n",
      "0.6351351351351351\n",
      "0.20754716981132076\n",
      "0.4845360824742268\n",
      "0.46825396825396826\n",
      "0.5333333333333333\n",
      "0.32954545454545453\n",
      "0.5047619047619047\n",
      "0.47305389221556887\n",
      "0.3163265306122449\n",
      "0.5375\n",
      "0.8125\n",
      "0.41964285714285715\n",
      "0.6885245901639344\n",
      "0.673469387755102\n",
      "0.29850746268656714\n",
      "0.5547445255474452\n",
      "0.6134453781512605\n",
      "0.4647887323943662\n",
      "0.44776119402985076\n",
      "0.42990654205607476\n",
      "0.7216494845360825\n",
      "0.7816091954022989\n",
      "0.5407407407407407\n",
      "0.7272727272727273\n",
      "0.36231884057971014\n",
      "0.47474747474747475\n",
      "0.42857142857142855\n",
      "0.6530612244897959\n",
      "0.504424778761062\n",
      "0.6629213483146067\n",
      "0.425\n",
      "0.3697478991596639\n",
      "0.42574257425742573\n",
      "0.5714285714285714\n",
      "0.5\n",
      "0.3076923076923077\n",
      "0.5845070422535211\n",
      "0.625\n",
      "0.5135135135135135\n",
      "0.53125\n",
      "0.6933333333333334\n",
      "0.5373134328358209\n",
      "0.618421052631579\n",
      "0.55\n",
      "0.5189873417721519\n",
      "0.75\n",
      "0.6627906976744186\n",
      "0.7361111111111112\n",
      "0.6764705882352942\n",
      "0.5137614678899083\n",
      "0.6744186046511628\n",
      "0.5945945945945946\n",
      "0.5391304347826087\n",
      "0.3645833333333333\n",
      "0.6962962962962963\n",
      "1.2312925170068028\n",
      "0.5844155844155844\n",
      "0.6893203883495146\n",
      "0.6885245901639344\n",
      "0.49056603773584906\n",
      "0.8333333333333334\n",
      "0.9565217391304348\n",
      "0.6407766990291263\n",
      "0.5538461538461539\n",
      "0.7559523809523809\n",
      "0.2857142857142857\n",
      "0.6347826086956522\n",
      "0.8076923076923077\n",
      "0.6848484848484848\n",
      "0.5576923076923077\n",
      "0.379746835443038\n",
      "0.5185185185185185\n",
      "0.5833333333333334\n",
      "0.4056603773584906\n",
      "0.7222222222222222\n",
      "0.6041666666666666\n",
      "0.5511811023622047\n",
      "0.6818181818181818\n",
      "0.6125\n",
      "1.0845070422535212\n",
      "0.8125\n",
      "0.4270833333333333\n",
      "0.6091954022988506\n",
      "0.5346534653465347\n",
      "0.7318840579710145\n",
      "0.6\n",
      "0.6220472440944882\n",
      "0.509090909090909\n",
      "0.17307692307692307\n",
      "0.6333333333333333\n",
      "0.6326530612244898\n",
      "0.648936170212766\n",
      "0.2653061224489796\n",
      "0.5304878048780488\n",
      "0.475\n",
      "0.6976744186046512\n",
      "0.6148148148148148\n",
      "0.5454545454545454\n",
      "0.4891304347826087\n",
      "0.581081081081081\n",
      "0.6494252873563219\n",
      "0.7469135802469136\n",
      "0.25\n",
      "0.6416666666666667\n",
      "0.8421052631578947\n",
      "0.6065573770491803\n",
      "0.32967032967032966\n",
      "0.6753246753246753\n",
      "0.8148148148148148\n",
      "0.7017543859649122\n",
      "0.6818181818181818\n",
      "0.5683060109289617\n",
      "0.6256684491978609\n",
      "0.5806451612903226\n",
      "0.5700934579439252\n",
      "0.2988505747126437\n",
      "0.8189655172413793\n",
      "0.8653846153846154\n",
      "1.0060975609756098\n",
      "0.5772357723577236\n",
      "0.5232558139534884\n",
      "0.6146788990825688\n",
      "0.6434108527131783\n",
      "0.648936170212766\n",
      "0.5567010309278351\n",
      "0.584\n",
      "0.775\n",
      "0.5503875968992248\n",
      "0.6486486486486487\n",
      "0.6363636363636364\n",
      "0.5\n",
      "0.4700854700854701\n",
      "0.6388888888888888\n",
      "0.6981132075471698\n",
      "0.43661971830985913\n",
      "0.5662650602409639\n",
      "0.5384615384615384\n",
      "0.6752136752136753\n",
      "0.6796875\n",
      "0.6121212121212121\n",
      "1.2794117647058822\n",
      "0.5954198473282443\n",
      "0.5125\n",
      "0.47368421052631576\n",
      "Epoch: 0, Avg Loss: 0.004813399314880371, Avg Validation CER: 88.74288268171449\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from accelerate import Accelerator\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_cer(pred_ids, label_ids):\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return cer\n",
    "# Training\n",
    "avg_cer_list = []\n",
    "avg_loss_list = []\n",
    "min_val = float('inf')\n",
    "patience = 2\n",
    "EPOCH = 1\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        # Backward pass\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    avg_loss += loss.item() / len(train_dataloader)\n",
    "\n",
    "    # Evaluation\n",
    "    val_cer = 0\n",
    "    model.eval()\n",
    "    for batch in val_dataloader:\n",
    "        with torch.no_grad():\n",
    "#             outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
    "#             print(compute_cer(outputs,batch[\"labels\"]))\n",
    "            # Gather all predictions and targets\n",
    "\n",
    "            outputs = model.generate(batch[\"pixel_values\"])\n",
    "            outputs = accelerator.gather(outputs)\n",
    "            labels = accelerator.gather(batch[\"labels\"])\n",
    "\n",
    "            # compute metrics\n",
    "\n",
    "            cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "            val_cer += cer\n",
    "#             print(cer)\n",
    "    last_val = val_cer/len(val_dataloader)\n",
    "    avg_cer_list.append(last_val)\n",
    "\n",
    "    # Early stopping if the avg_cer does not decrease for patience epochs\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        if last_val > min_val:\n",
    "            counter += 1\n",
    "        else:\n",
    "            counter = 0\n",
    "            min_val = last_val\n",
    "       1     # saving the best model so far\n",
    "            torch.save(model.state_dict(), model_directory + 'best_model.pt')\n",
    "        if counter == patience:\n",
    "            break\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"Epoch: {}, Avg Loss: {}, Avg Validation CER: {}\".format(epoch, avg_loss, val_cer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47381d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-0af3b81e4a07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
