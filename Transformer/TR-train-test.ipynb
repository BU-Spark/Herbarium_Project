{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07a22626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate\n",
    "# !pip install datasets\n",
    "# !pip install jiwer\n",
    "#!pip install evaluate\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import load_metric\n",
    "from transformers import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Reading the training file into a DataFrame\n",
    "IAM_lines = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/tesseract-training/training/IAM/gt/lines/'\n",
    "IAM_words = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/tesseract-training/training/IAM/gt/words/'\n",
    "IAM_sentences = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/tesseract-training/training/IAM/gt/sentences/'\n",
    "\n",
    "model_directory = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/'\n",
    "# All files and directories ending with .txt and that don't begin with a dot:\n",
    "def get_lists(directory,directory_percentage):\n",
    "    image_list = glob.glob(directory+\"*.png\")\n",
    "\n",
    "    text_list = []\n",
    "    for image in image_list:\n",
    "        text_list.extend(open(image.split('.')[0]+'.gt.txt','r').read().splitlines())\n",
    "    # Take a random percentage of the data\n",
    "#     image_list, text_list = zip(*random.sample(list(zip(image_list, text_list)), round(directory_percentage/100*len(image_list))))\n",
    "    image_list, text_list = zip(*random.sample(list(zip(image_list, text_list)),directory_percentage))\n",
    "    return image_list,text_list\n",
    "\n",
    "\n",
    "# Taken from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_native_PyTorch.ipynb\n",
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, df, processor, max_target_length=128):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        image = self.df['image'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(image).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding\n",
    "\n",
    "\n",
    "image_list = []\n",
    "text_list = []\n",
    "lines_percentage = 1000\n",
    "words_percentage = 1000\n",
    "sentences_percentage = 1000\n",
    "for directory,percentage in zip([IAM_lines, IAM_words, IAM_sentences],[lines_percentage, words_percentage, sentences_percentage]):\n",
    "    images,text = get_lists(directory,percentage)\n",
    "    image_list.extend(images)\n",
    "    text_list.extend(text)\n",
    "    \n",
    "\n",
    "df = pd.DataFrame({'image':image_list,'text':text_list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "757b62c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from accelerate import Accelerator\n",
    "    \n",
    "#Setting up the accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "#Loading model and processor \n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\") \n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4\n",
    "\n",
    "# Datasets and dataloaders for train validation\n",
    "train_dataset = IAMDataset(df=train_df,processor=processor)\n",
    "val_dataset = IAMDataset(df=val_df,processor=processor)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "# Getting the accelerator set up\n",
    "model, optimizer, train_dataloader, val_dataloader= accelerator.prepare(\n",
    "    model, optimizer, train_dataloader,val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27ff4f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [03:19<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8028169014084507\n",
      "0.6236559139784946\n",
      "0.5027624309392266\n",
      "0.5597014925373134\n",
      "0.5675675675675675\n",
      "0.5741935483870968\n",
      "0.6351351351351351\n",
      "0.20754716981132076\n",
      "0.4845360824742268\n",
      "0.46825396825396826\n",
      "0.5333333333333333\n",
      "0.32954545454545453\n",
      "0.5047619047619047\n",
      "0.47305389221556887\n",
      "0.3163265306122449\n",
      "0.5375\n",
      "0.8125\n",
      "0.41964285714285715\n",
      "0.6885245901639344\n",
      "0.673469387755102\n",
      "0.29850746268656714\n",
      "0.5547445255474452\n",
      "0.6134453781512605\n",
      "0.4647887323943662\n",
      "0.44776119402985076\n",
      "0.42990654205607476\n",
      "0.7216494845360825\n",
      "0.7816091954022989\n",
      "0.5407407407407407\n",
      "0.7272727272727273\n",
      "0.36231884057971014\n",
      "0.47474747474747475\n",
      "0.42857142857142855\n",
      "0.6530612244897959\n",
      "0.504424778761062\n",
      "0.6629213483146067\n",
      "0.425\n",
      "0.3697478991596639\n",
      "0.42574257425742573\n",
      "0.5714285714285714\n",
      "0.5\n",
      "0.3076923076923077\n",
      "0.5845070422535211\n",
      "0.625\n",
      "0.5135135135135135\n",
      "0.53125\n",
      "0.6933333333333334\n",
      "0.5373134328358209\n",
      "0.618421052631579\n",
      "0.55\n",
      "0.5189873417721519\n",
      "0.75\n",
      "0.6627906976744186\n",
      "0.7361111111111112\n",
      "0.6764705882352942\n",
      "0.5137614678899083\n",
      "0.6744186046511628\n",
      "0.5945945945945946\n",
      "0.5391304347826087\n",
      "0.3645833333333333\n",
      "0.6962962962962963\n",
      "1.2312925170068028\n",
      "0.5844155844155844\n",
      "0.6893203883495146\n",
      "0.6885245901639344\n",
      "0.49056603773584906\n",
      "0.8333333333333334\n",
      "0.9565217391304348\n",
      "0.6407766990291263\n",
      "0.5538461538461539\n",
      "0.7559523809523809\n",
      "0.2857142857142857\n",
      "0.6347826086956522\n",
      "0.8076923076923077\n",
      "0.6848484848484848\n",
      "0.5576923076923077\n",
      "0.379746835443038\n",
      "0.5185185185185185\n",
      "0.5833333333333334\n",
      "0.4056603773584906\n",
      "0.7222222222222222\n",
      "0.6041666666666666\n",
      "0.5511811023622047\n",
      "0.6818181818181818\n",
      "0.6125\n",
      "1.0845070422535212\n",
      "0.8125\n",
      "0.4270833333333333\n",
      "0.6091954022988506\n",
      "0.5346534653465347\n",
      "0.7318840579710145\n",
      "0.6\n",
      "0.6220472440944882\n",
      "0.509090909090909\n",
      "0.17307692307692307\n",
      "0.6333333333333333\n",
      "0.6326530612244898\n",
      "0.648936170212766\n",
      "0.2653061224489796\n",
      "0.5304878048780488\n",
      "0.475\n",
      "0.6976744186046512\n",
      "0.6148148148148148\n",
      "0.5454545454545454\n",
      "0.4891304347826087\n",
      "0.581081081081081\n",
      "0.6494252873563219\n",
      "0.7469135802469136\n",
      "0.25\n",
      "0.6416666666666667\n",
      "0.8421052631578947\n",
      "0.6065573770491803\n",
      "0.32967032967032966\n",
      "0.6753246753246753\n",
      "0.8148148148148148\n",
      "0.7017543859649122\n",
      "0.6818181818181818\n",
      "0.5683060109289617\n",
      "0.6256684491978609\n",
      "0.5806451612903226\n",
      "0.5700934579439252\n",
      "0.2988505747126437\n",
      "0.8189655172413793\n",
      "0.8653846153846154\n",
      "1.0060975609756098\n",
      "0.5772357723577236\n",
      "0.5232558139534884\n",
      "0.6146788990825688\n",
      "0.6434108527131783\n",
      "0.648936170212766\n",
      "0.5567010309278351\n",
      "0.584\n",
      "0.775\n",
      "0.5503875968992248\n",
      "0.6486486486486487\n",
      "0.6363636363636364\n",
      "0.5\n",
      "0.4700854700854701\n",
      "0.6388888888888888\n",
      "0.6981132075471698\n",
      "0.43661971830985913\n",
      "0.5662650602409639\n",
      "0.5384615384615384\n",
      "0.6752136752136753\n",
      "0.6796875\n",
      "0.6121212121212121\n",
      "1.2794117647058822\n",
      "0.5954198473282443\n",
      "0.5125\n",
      "0.47368421052631576\n",
      "Epoch: 0, Avg Loss: 0.004813399314880371, Avg Validation CER: 88.74288268171449\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from accelerate import Accelerator\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_cer(pred_ids, label_ids):\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return cer\n",
    "# Training\n",
    "avg_cer_list = []\n",
    "avg_loss_list = []\n",
    "min_val = float('inf')\n",
    "patience = 2\n",
    "EPOCH = 1\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        # Backward pass\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    avg_loss += loss.item() / len(train_dataloader)\n",
    "\n",
    "    # Evaluation\n",
    "    val_cer = 0\n",
    "    model.eval()\n",
    "    for batch in val_dataloader:\n",
    "        with torch.no_grad():\n",
    "#             outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
    "#             print(compute_cer(outputs,batch[\"labels\"]))\n",
    "            # Gather all predictions and targets\n",
    "\n",
    "            outputs = model.generate(batch[\"pixel_values\"])\n",
    "            outputs = accelerator.gather(outputs)\n",
    "            labels = accelerator.gather(batch[\"labels\"])\n",
    "\n",
    "            # compute metrics\n",
    "\n",
    "            cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "            val_cer += cer\n",
    "#             print(cer)\n",
    "    last_val = val_cer/len(val_dataloader)\n",
    "    avg_cer_list.append(last_val)\n",
    "\n",
    "    # Early stopping if the avg_cer does not decrease for patience epochs\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        if last_val > min_val:\n",
    "            counter += 1\n",
    "        else:\n",
    "            counter = 0\n",
    "            min_val = last_val\n",
    "            # saving the best model so far\n",
    "            torch.save(model.state_dict(), model_directory + 'best_model.pt')\n",
    "        if counter == patience:\n",
    "            break\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"Epoch: {}, Avg Loss: {}, Avg Validation CER: {}\".format(epoch, avg_loss, val_cer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47381d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-0af3b81e4a07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
