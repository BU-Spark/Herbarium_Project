{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xJWLfmp8MLnk"
   },
   "outputs": [],
   "source": [
    "#Imports and installs\n",
    "# !pip install -q transformers\n",
    "import transformers\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "# !pip install craft-text-detector\n",
    "import transformers\n",
    "from craft_text_detector import Craft\n",
    "import requests \n",
    "import torch\n",
    "import os, random\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imghdr\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-ByKwM7_JNFH"
   },
   "outputs": [],
   "source": [
    "# Set COLAB = False if running on SCC\n",
    "COLAB = False\n",
    "\n",
    "#suppressing all the huggingface warnings\n",
    "SUPPRESS = True\n",
    "if SUPPRESS:\n",
    "    from transformers.utils import logging\n",
    "    logging.set_verbosity(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SEQY21wtMgX5",
    "outputId": "4a95d6c3-6fb1-47a3-b959-418061662f15"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive',force_remount = True)\n",
    "    workdir = '/content/gdrive/MyDrive/testing-trocr'\n",
    "    output_dir_craft = '/content/gdrive/MyDrive/craft/'\n",
    "else:\n",
    "    workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata' # update this to the desired directory on scc\n",
    "    output_dir_craft = '/usr4/dl523/dong760/CS549_Herbarium_Project/ml-herbarium/Tr_OCR/result_craft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XpfRibZm2tzb"
   },
   "outputs": [],
   "source": [
    "# initialize the CRAFT model\n",
    "craft = Craft(output_dir = output_dir_craft,export_extra = False, text_threshold = .8,link_threshold = .6, crop_type=\"poly\",low_text = .5,cuda = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running craft and saving the segmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1018 [00:00<?, ?it/s]/share/pkg.7/pytorch/1.9.0/install/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "  0%|          | 1/1018 [00:01<23:05,  1.36s/it]/usr4/dl523/dong760/.local/lib/python3.8/site-packages/craft_text_detector/craft_utils.py:415: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  polys = np.array(polys)\n",
      "/usr4/dl523/dong760/.local/lib/python3.8/site-packages/craft_text_detector/predict.py:110: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  polys_as_ratio = np.array(polys_as_ratio)\n",
      "  7%|▋         | 70/1018 [01:33<19:31,  1.24s/it]/share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:763: UserWarning: Possibly corrupt EXIF data.  Expecting to read 11 bytes but only got 6. Skipping tag 42037\n",
      "  warnings.warn(\n",
      " 39%|███▉      | 401/1018 [08:38<11:22,  1.11s/it]/share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:763: UserWarning: Possibly corrupt EXIF data.  Expecting to read 14 bytes but only got 8. Skipping tag 42036\n",
      "  warnings.warn(\n",
      " 40%|████      | 410/1018 [08:51<13:07,  1.30s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-974b8aeb2731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Check that the image is valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mbounding_areas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcraft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbounding_areas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#check that a segmentation was found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mno_segmentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/craft_text_detector/__init__.py\u001b[0m in \u001b[0;36mdetect_text\u001b[0;34m(self, image, image_path)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"image\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             exported_file_paths = export_detected_regions(\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mregions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/craft_text_detector/file_utils.py\u001b[0m in \u001b[0;36mexport_detected_regions\u001b[0;34m(image, regions, file_name, output_dir, rectify)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;31m# read/convert image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# deepcopy image so that original is not altered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/craft_text_detector/image_utils.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CRAFT on images to get bounding boxes\n",
    "images = []\n",
    "corrupted_images = []\n",
    "no_segmentations = []\n",
    "boxes = {}\n",
    "count= 0\n",
    "file_types = (\".jpg\", \".jpeg\",\".png\")\n",
    "for filename in tqdm(os.listdir(workdir)):\n",
    "    if filename.endswith(file_types):\n",
    "        image = workdir+'/'+filename\n",
    "        try:\n",
    "            img = Image.open(image) \n",
    "            img.verify() # Check that the image is valid\n",
    "            bounding_areas = craft.detect_text(image)\n",
    "            if not bounding_areas: #check that a segmentation was found\n",
    "                no_segmentations.append(image)\n",
    "            else:\n",
    "                images.append(image)\n",
    "                boxes[image] = bounding_areas['boxes']\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            corrupted_images.append(image)\n",
    "\n",
    "            \n",
    "#     count +=1\n",
    "#     # Using count for time being to get things working, remove once setup is complete\n",
    "#     if count == 5:\n",
    "#         break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting all the segemnted images into a dataloader, and loading model and processor for trocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting empty folders\n",
    "root = output_dir_craft\n",
    "folders = list(os.walk(root))[1:]\n",
    "deleted = []\n",
    "for folder in folders:\n",
    "    # folder example: ('FOLDER/3', [], ['file'])\n",
    "    if not folder[2]:\n",
    "        deleted.append(folder)\n",
    "        os.rmdir(folder[0])\n",
    "\n",
    "# Setting up the Tr-OCR model (using base model currently, large takes much longer)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\") \n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# Use all available gpu's\n",
    "model_gpu= nn.DataParallel(model).to(device)\n",
    "\n",
    "# Dataloader for working with gpu's\n",
    "trainset = datasets.ImageFolder(output_dir_craft, transform = processor)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)# for i, data in enumerate(trainloader):\n",
    "\n",
    "# For matching words to image\n",
    "filenames = [s.replace('_crops', '') for s in list(trainset.class_to_idx)]\n",
    "word_log_dic = {k: v for k,v in enumerate(filenames)}\n",
    "words_identified = {k: [] for v,k in enumerate(filenames)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2b19fedd48e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yoinked https://github.com/rsommerfeld/trocr/blob/main/src/scripts.py\n",
    "\n",
    "def get_confidence_scores(generated_ids):\n",
    "    # Get raw logits, with shape (examples,tokens,token_vals)\n",
    "    logits = generated_ids.scores\n",
    "    logits = torch.stack(list(logits),dim=1)\n",
    "\n",
    "    # Transform logits to softmax and keep only the highest (chosen) p for each token\n",
    "    logit_probs = F.softmax(logits, dim=2)\n",
    "    char_probs = logit_probs.max(dim=2)[0]\n",
    "\n",
    "    # Only tokens of val>2 should influence the confidence. Thus, set probabilities to 1 for tokens 0-2\n",
    "    mask = generated_ids.sequences[:,:-1] > 2\n",
    "    char_probs[mask] = 1\n",
    "\n",
    "    # Confidence of each example is cumulative product of token probs\n",
    "    batch_confidence_scores = char_probs.cumprod(dim=1)[:, -1]\n",
    "    return [v.item() for v in batch_confidence_scores]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_craft_seg(model,trainloader):\n",
    "    results = []\n",
    "    confidence = []\n",
    "    label = []\n",
    "\n",
    "    model_gpu.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx,data in enumerate(tqdm(trainloader)):\n",
    "\n",
    "            images, labels = data\n",
    "            images, labels = images['pixel_values'][0].to(device), labels.to(device)\n",
    "\n",
    "            decoded = model_gpu.module.generate(images,return_dict_in_generate = True, output_scores = True) \n",
    "            final_values = processor.batch_decode(decoded.sequences, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "            confidences = get_confidence_scores(decoded)\n",
    "\n",
    "            for idx,value in enumerate(labels.cpu().numpy()):\n",
    "#           if confidences[idx]>.8: # to cull some of the really terrible guesses, probably want to do this in the search function instead\n",
    "                words_identified[word_log_dic[value]].append(final_values[idx])\n",
    "            \n",
    "            results.extend(final_values)\n",
    "            confidence.extend(confidences)\n",
    "            label.extend(labels.cpu().numpy())\n",
    "\n",
    "    return results,confidence,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/420 [00:00<?, ?it/s]/usr4/dl523/dong760/.local/lib/python3.8/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 420/420 [06:22<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "#Storing the outputs\n",
    "pickle_name = 'results7.pkl'\n",
    "results,confidence,labels = evaluate_craft_seg(model, trainloader)\n",
    "df = pd.DataFrame(,columns = ['Results','Confidence','Labels'])\n",
    "df.to_pickle(pickle_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filepath):\n",
    "    filepkl = open(filepath, \"rb\")\n",
    "    # Unpickle the objects\n",
    "    unpickled = pickle.load(filepkl)\n",
    "    return unpickled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_name = 'results7.pkl'\n",
    "a = load_pickle('/usr4/dl523/dong760/CS549_Herbarium_Project/ml-herbarium/Tr_OCR/' + pickle_name)\n",
    "# '/usr4/dl523/dong760/CS549_Herbarium_Project/ml-herbarium/Tr_OCR/result_craft'\n",
    "results =pd.Series(a.Results)\n",
    "labels = list(a['Labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String matching installs\n",
    "# !pip install --force-reinstall numpy==1.18.5 # need this to work the string grouper\n",
    "# !pip install numpy\n",
    "# !pip install string-grouper\n",
    "\n",
    "from string_grouper import match_strings, match_most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    taxon_file = '/content/gdrive/MyDrive/corpus_taxon.txt'\n",
    "else:\n",
    "    taxon_file = workdir+'/taxon_corpus.txt'\n",
    "    geography_file = workdir+'/geography_corpus.txt'\n",
    "    collector_file = workdir+'/collector_corpus.txt'\n",
    "taxon = pd.read_csv(taxon_file, delimiter = \"\\t\", names=[\"Taxon\"]).squeeze()\n",
    "geography = pd.read_csv(geography_file, delimiter = \"\\t\", names=[\"Geography\"]).squeeze()\n",
    "collector = pd.read_csv(collector_file, delimiter = \"\\t\", names=[\"Collector\"]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 0.17265844345092773\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_side</th>\n",
       "      <th>right_side</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Euphrasia officinalis</td>\n",
       "      <td>Valeriana officinalis</td>\n",
       "      <td>0.486098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Euphrasia officinalis</td>\n",
       "      <td>Graniola officinalis</td>\n",
       "      <td>0.485812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Euphrasia officinalis</td>\n",
       "      <td>euphrasia flustuumbiana</td>\n",
       "      <td>0.408641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Euphrasia officinalis</td>\n",
       "      <td>office.</td>\n",
       "      <td>0.344202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bryoerythrophyllum recurvirostrum</td>\n",
       "      <td>colophyllum.</td>\n",
       "      <td>0.320190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4027</th>\n",
       "      <td>Populus alba</td>\n",
       "      <td>Propulsion conquestum Bong</td>\n",
       "      <td>0.165190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4028</th>\n",
       "      <td>Cymbalaria muralis</td>\n",
       "      <td>naturalis</td>\n",
       "      <td>0.324213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4029</th>\n",
       "      <td>Cymbalaria muralis</td>\n",
       "      <td>naturalis</td>\n",
       "      <td>0.324213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4030</th>\n",
       "      <td>Cymbalaria muralis</td>\n",
       "      <td>naturalis</td>\n",
       "      <td>0.324213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4031</th>\n",
       "      <td>Cymbalaria muralis</td>\n",
       "      <td>naturalis</td>\n",
       "      <td>0.324213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4032 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              left_side                  right_side  \\\n",
       "0                 Euphrasia officinalis       Valeriana officinalis   \n",
       "1                 Euphrasia officinalis        Graniola officinalis   \n",
       "2                 Euphrasia officinalis     euphrasia flustuumbiana   \n",
       "3                 Euphrasia officinalis                     office.   \n",
       "4     Bryoerythrophyllum recurvirostrum                colophyllum.   \n",
       "...                                 ...                         ...   \n",
       "4027                       Populus alba  Propulsion conquestum Bong   \n",
       "4028                 Cymbalaria muralis                   naturalis   \n",
       "4029                 Cymbalaria muralis                   naturalis   \n",
       "4030                 Cymbalaria muralis                   naturalis   \n",
       "4031                 Cymbalaria muralis                   naturalis   \n",
       "\n",
       "      similarity  \n",
       "0       0.486098  \n",
       "1       0.485812  \n",
       "2       0.408641  \n",
       "3       0.344202  \n",
       "4       0.320190  \n",
       "...          ...  \n",
       "4027    0.165190  \n",
       "4028    0.324213  \n",
       "4029    0.324213  \n",
       "4030    0.324213  \n",
       "4031    0.324213  \n",
       "\n",
       "[4032 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "# String matching\n",
    "minimum_similarity = .1\n",
    "results_series = pd.Series(results)\n",
    "taxon = pd.read_csv(taxon_file, delimiter = \"\\t\", names=[\"Taxon\"]).squeeze()\n",
    " \n",
    "start = time.time()\n",
    "matches = match_strings(taxon, results_series, min_similarity = minimum_similarity, max_n_matches = 4)\n",
    "end = time.time()\n",
    "print('time',end-start)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   Euphrasia officinalis\n",
       "1       Bryoerythrophyllum recurvirostrum\n",
       "2                     Carduus tenuiflorus\n",
       "3                     Agoseris parviflora\n",
       "4                       Spiraea canescens\n",
       "                      ...                \n",
       "1003                   Cleidion javanicum\n",
       "1004                        Lotus alpinus\n",
       "1005              Diplazium polypodioides\n",
       "1006                         Populus alba\n",
       "1007                   Cymbalaria muralis\n",
       "Name: Taxon, Length: 1008, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                THE NEW YORK\n",
       "1                            suppressis Prose\n",
       "2                                        c.a.\n",
       "3               Ops.. It, demonstrate certain\n",
       "4                                         0 0\n",
       "                        ...                  \n",
       "6712                                      0 0\n",
       "6713                     gree's Week 2 (, 18.\n",
       "6714    Herbier du Jardin Botanique de l'Etat\n",
       "6715                               geslacht :\n",
       "6716         first. More, also, major vendors\n",
       "Name: Results, Length: 6717, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(main, comparison_file, minimum_similarity):\n",
    "     # Function takes a main file containing strings, a comparison file to match against main,\n",
    "     #  and a minimum similarity confidence level. Returns a list of matches based on similarity.\n",
    "\n",
    "    if not isinstance(comparison_file, pd.Series):\n",
    "\n",
    "        comparison_file = pd.Series(comparison_file)\n",
    "\n",
    "    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)\n",
    "\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_score_per_image(df,labels,filenames,minimum_similarity):\n",
    "    # Getting the highest score for each individual image \n",
    "    index_to_labels = df.copy()\n",
    "    for a in index_to_labels.right_index.unique():\n",
    "        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]\n",
    "    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]\n",
    "\n",
    "#     print(\"Of the\",len(filenames),\"images evaluated\",len(unique_labels), \"have a prediction score above\",minimum_similarity*100, \"percent.\")\n",
    "    return unique_labels\n",
    "# unique_labels = highest_score_per_image(ascending_df,labels,filenames,minimum_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooled_match(comparison_file, labels, filenames, minimum_similarity = .7,**kwargs):\n",
    "    # Take in any number of files containing strings to match against and return a dictionary\n",
    "    # with keys the same name as input and values as the dataframe with matching information\n",
    "   \n",
    "    corpus_list = []\n",
    "    corpus_name = []\n",
    "    \n",
    "    for k,v in kwargs.items():\n",
    "        # Convert to series (string-grouper requires this type), will work if input is list, array, or series\n",
    "        if not isinstance(v, pd.Series):\n",
    "            v = pd.Series(v)\n",
    "        corpus_list.append(v)\n",
    "        corpus_name.append(k)\n",
    "    \n",
    "    func = partial(match, comparison_file = results_series,  minimum_similarity = minimum_similarity)\n",
    "    pool = multiprocessing.Pool()\n",
    "\n",
    "    result_dic = {}\n",
    "    for i,result in enumerate(pool.map(func,corpus_list)):\n",
    "        result.columns.values[1] = corpus_name[i]+' Corpus'\n",
    "        result.columns.values[3] = \"Predictions\"\n",
    "        result = result.drop('left_index', axis=1)\n",
    "        result = highest_score_per_image(result,labels,filenames,minimum_similarity)\n",
    "        result_dic[corpus_name[i]] = result\n",
    "\n",
    "   \n",
    "    return result_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6717 399\n"
     ]
    }
   ],
   "source": [
    "print(len(labels),len(filenames))\n",
    "results_series = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-f9ba9f66affe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpooled_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_series\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminimum_similarity\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTaxon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtaxon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGeography\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCollector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# for k,v in results.items():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "results = pooled_match(results_series,labels, filenames,minimum_similarity =.01,Taxon = taxon,Geography = g,Collector = collector)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "# for k,v in results.items():\n",
    "#     display(v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in results.items():\n",
    "    display(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganizing the df\n",
    "ascending_df = matches.sort_values(by=['similarity'],ascending=False)\n",
    "pd.set_option('display.width', 150)\n",
    "display(ascending_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matches_above_x(df,x):\n",
    "    return df.loc[df['similarity']>=.75]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "above_75 = matches_above_x(unique_labels,75)\n",
    "display(above_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the ground truth values\n",
    "gt_t = workdir+'/taxon_gt.txt'\n",
    "Taxon_truth = { line.split(\":\")[0] : line.split(\": \")[1].strip() for line in open(gt_t) }\n",
    "\n",
    "gt_g = workdir+'/geography_gt.txt'\n",
    "Geography_truth = { line.split(\":\")[0] : line.split(\": \")[1].strip() for line in open(gt_g) }\n",
    "\n",
    "gt_c = workdir+'/collector_gt.txt'\n",
    "Collector_truth = { line.split(\":\")[0] : line.split(\": \")[1].strip() for line in open(gt_c) }\n",
    "\n",
    "comparison_file = {\"Taxon\":Taxon_truth,\"Geography\":Geography_truth,\"Collector\":Collector_truth}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-1d7f3d9ad970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# need to add in the check for other species names, quite a few would match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprediction_and_imagenumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_Taxon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_number\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction_and_imagenumber\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unique_labels' is not defined"
     ]
    }
   ],
   "source": [
    "# need to add in the check for other species names, quite a few would match\n",
    "prediction_and_imagenumber = list(zip(unique_labels.left_Taxon, unique_labels.right_index))\n",
    "count = 0\n",
    "for prediction,image_number in prediction_and_imagenumber:\n",
    "    try:\n",
    "        image = word_log_dic[image_number]\n",
    "        gt = ground_truth[image]\n",
    "        if gt == prediction:\n",
    "            count +=1\n",
    "#             print(gt,\"||\",prediction,'||',image)\n",
    "        else:\n",
    "            print('*',gt,\"||\",prediction,'||',image)\n",
    "    except KeyError as e:\n",
    "        print(\"Ground Truth Not Found for:\",word_log_dic[image_number])\n",
    "acc = count/len(unique_labels)\n",
    "print(\"Accuracy on Predicted:\",acc)\n",
    "print(\"Total accuracy: \",count/len(filenames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prRed(skk): print(\"\\033[91m{}\\033[00m\" .format(skk))\n",
    "    \n",
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-981d60fd2adb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# need to add in the check for other species names, quite a few would match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprediction_and_imagenumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' Corpus'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Evaluation for'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "for k,v in results.items():    \n",
    "    # need to add in the check for other species names, quite a few would match\n",
    "    prediction_and_imagenumber = list(zip(v[k+' Corpus'], v.right_index))\n",
    "    count = 0\n",
    "    print('Evaluation for',k)\n",
    "    for prediction,image_number in prediction_and_imagenumber:\n",
    "        try:\n",
    "            image = word_log_dic[image_number]\n",
    "            gt = comparison_file[k][image]\n",
    "            if gt == prediction:\n",
    "                count +=1\n",
    "                print(gt,\"||\",prediction,'||',image)\n",
    "            else:\n",
    "                print(color.RED+gt+\"||\"+prediction+'||'+image+color.END)\n",
    "        except KeyError as e:\n",
    "            print(\"Ground Truth Not Found for:\",word_log_dic[image_number])\n",
    "    \n",
    "    acc = count/len(v)\n",
    "    print(color.BOLD+\"Accuracy on Predicted:\"+str(acc)+color.END)\n",
    "    print(color.BOLD+\"Total accuracy: \"+str(count/len(filenames))+color.END)\n",
    "    print(color.BOLD+\"Total Guessed:\"+str(len(prediction_and_imagenumber))+color.END)\n",
    "    print('\\n\\n********************************\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in words_identified.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0j2-HKSMLK3"
   },
   "outputs": [],
   "source": [
    "# CRAFT on images to get bounding boxes\n",
    "images = []\n",
    "corrupted_images = []\n",
    "boxes = {}\n",
    "count= 0\n",
    "file_types = (\".jpg\", \".jpeg\",\".png\")\n",
    "for filename in tqdm(os.listdir(workdir)):\n",
    "    if filename.endswith(file_types):\n",
    "        image = workdir+'/'+filename\n",
    "        try:\n",
    "            img = Image.open(image) # open the image file\n",
    "            img.verify() # verify that it is, in fact an image\n",
    "            images.append(image)\n",
    "            bounding_areas = craft.detect_text(image)\n",
    "            boxes[image] = bounding_areas['boxes']\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            corrupted_images.append(image)\n",
    "\n",
    "            \n",
    "#     count +=1\n",
    "#     # Using count for time being to get things working, remove once setup is complete\n",
    "#     if count == 5:\n",
    "#         break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset2 = datasets.ImageFolder(workdir, transform = processor)\n",
    "# trainloader2 = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)# for i, data in enumerate(trainloader):\n",
    "\n",
    "# Checking size of all the segmentation images (about 1/6 input)\n",
    "root_directory = Path(workdir)\n",
    "sum(f.stat().st_size for f in root_directory.glob('**/*') if f.is_file())\n",
    "\n",
    "def save_bounding(filename,bounding_boxes):\n",
    "# function to save the boudning box outputs from CRAFT segmentation\n",
    "\n",
    "    f = open(filneame+\".pkl\",\"wb\")\n",
    "\n",
    "    # write the python object (dict) to pickle file\n",
    "    pickle.dump(bounding_boxes,f)\n",
    "\n",
    "    # close file\n",
    "    f.close()\n",
    "    \n",
    "def display_segmentations(boxes_dict): # Check this later if we dont want to save all the image segementations\n",
    "    # Function takes in the dictionary of image_path and bounding boxes and displays all of the segmentations\n",
    "    \n",
    "    count = 0\n",
    "    for image_path,bounding_boxes in boxes_dict.items():\n",
    "        original_image = cv2.imread(image_path)\n",
    "        for box in v:\n",
    "            segmentation = original_image[int(box[0][1]): int(box[2][1]), \n",
    "                  int(box[0][0]): int(box[2][0])]\n",
    "\n",
    "        segmented_image = Image.fromarray(segmentation).convert(\"RGB\")\n",
    "        display(segmented_image)\n",
    "        count+=1\n",
    "        if count>5:\n",
    "            break\n",
    "\n",
    "            \n",
    "# Downloading the tr-ocr model and processor\n",
    "# processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\") \n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# Training set creation\n",
    "# trainset = datasets.ImageFolder(output_dir_craft, transform = processor)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Put model on the gpu \n",
    "# model.to(device)\n",
    "\n",
    "# img_dir = '/usr3/graduate/colejh/craft3/'\n",
    "# for filename in os.listdir(img_dir):\n",
    "#     for filename2 in os.listdir(img_dir+'/'+filename):\n",
    "#         image = Image.open(img_dir+'/'+filename+'/'+filename2)\n",
    "#         pixel_values = processor(image, return_tensors=\"pt\").pixel_values \n",
    "#         generated_ids = model.generate(pixel_values)\n",
    "#         generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "#         display(image)\n",
    "#         print(\"Text: \"+generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking size of all the segmentation images (about 1/6 input)\n",
    "root_directory = Path(workdir)\n",
    "sum(f.stat().st_size for f in root_directory.glob('**/*') if f.is_file())\n",
    "\n",
    "\n",
    "# saving the bounding boxes dictionary \n",
    "f = open(\"boundingboxes.pkl\",\"wb\")\n",
    "\n",
    "# write the python object (dict) to pickle file\n",
    "pickle.dump(dict,f)\n",
    "\n",
    "# close file\n",
    "f.close()\n",
    "\n",
    "#Check this later if we dont want to save all the image segementations\n",
    "count = 0\n",
    "for k,v in boxes.items():\n",
    "    full_image = cv2.imread(k)\n",
    "    for box in v:\n",
    "        segmentation = full_image[int(box[0][1]): int(box[2][1]), \n",
    "              int(box[0][0]): int(box[2][0])]\n",
    "    \n",
    "    #Drop this later, just for checking stuff\n",
    "    image = Image.fromarray(segmentation).convert(\"RGB\")\n",
    "    display(image)\n",
    "   \n",
    "    count+=1\n",
    "    if count>5:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
