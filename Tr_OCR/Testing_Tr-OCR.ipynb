{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xJWLfmp8MLnk"
   },
   "outputs": [],
   "source": [
    "#Imports and installs\n",
    "# !pip install -q transformers\n",
    "import transformers\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "# !pip install craft-text-detector\n",
    "import transformers\n",
    "from craft_text_detector import Craft\n",
    "import requests \n",
    "import torch\n",
    "import os, random\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imghdr\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-ByKwM7_JNFH"
   },
   "outputs": [],
   "source": [
    "# Set COLAB = False if running on SCC\n",
    "COLAB = False\n",
    "\n",
    "#suppressing all the huggingface warnings\n",
    "SUPPRESS = True\n",
    "if SUPPRESS:\n",
    "    from transformers.utils import logging\n",
    "    logging.set_verbosity(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SEQY21wtMgX5",
    "outputId": "4a95d6c3-6fb1-47a3-b959-418061662f15"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive',force_remount = True)\n",
    "    workdir = '/content/gdrive/MyDrive/testing-trocr'\n",
    "    output_dir_craft = '/content/gdrive/MyDrive/craft/'\n",
    "else:\n",
    "    workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata' # update this to the desired directory on scc\n",
    "    output_dir_craft = '//usr4/dl523/dong760/CS549_Herbarium_Project/ml-herbarium/Tr_OCR/result_craft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XpfRibZm2tzb"
   },
   "outputs": [],
   "source": [
    "# initialize the CRAFT model\n",
    "craft = Craft(output_dir = output_dir_craft,export_extra = False, text_threshold = .8,link_threshold = .6, crop_type=\"poly\",low_text = .5,cuda = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running craft and saving the segmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1018 [00:00<?, ?it/s]/share/pkg.7/pytorch/1.9.0/install/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "  0%|          | 1/1018 [00:01<23:05,  1.36s/it]/usr4/dl523/dong760/.local/lib/python3.8/site-packages/craft_text_detector/craft_utils.py:415: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  polys = np.array(polys)\n",
      "/usr4/dl523/dong760/.local/lib/python3.8/site-packages/craft_text_detector/predict.py:110: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  polys_as_ratio = np.array(polys_as_ratio)\n",
      "  7%|▋         | 70/1018 [01:33<19:31,  1.24s/it]/share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:763: UserWarning: Possibly corrupt EXIF data.  Expecting to read 11 bytes but only got 6. Skipping tag 42037\n",
      "  warnings.warn(\n",
      " 39%|███▉      | 401/1018 [08:38<11:22,  1.11s/it]/share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages/PIL/TiffImagePlugin.py:763: UserWarning: Possibly corrupt EXIF data.  Expecting to read 14 bytes but only got 8. Skipping tag 42036\n",
      "  warnings.warn(\n",
      " 40%|████      | 410/1018 [08:51<13:07,  1.30s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-974b8aeb2731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Check that the image is valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mbounding_areas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcraft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbounding_areas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#check that a segmentation was found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mno_segmentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/craft_text_detector/__init__.py\u001b[0m in \u001b[0;36mdetect_text\u001b[0;34m(self, image, image_path)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"image\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             exported_file_paths = export_detected_regions(\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mregions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/craft_text_detector/file_utils.py\u001b[0m in \u001b[0;36mexport_detected_regions\u001b[0;34m(image, regions, file_name, output_dir, rectify)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;31m# read/convert image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# deepcopy image so that original is not altered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/craft_text_detector/image_utils.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CRAFT on images to get bounding boxes\n",
    "images = []\n",
    "corrupted_images = []\n",
    "no_segmentations = []\n",
    "boxes = {}\n",
    "count= 0\n",
    "file_types = (\".jpg\", \".jpeg\",\".png\")\n",
    "for filename in tqdm(os.listdir(workdir)):\n",
    "    if filename.endswith(file_types):\n",
    "        image = workdir+'/'+filename\n",
    "        try:\n",
    "            img = Image.open(image) \n",
    "            img.verify() # Check that the image is valid\n",
    "            bounding_areas = craft.detect_text(image)\n",
    "            if not bounding_areas: #check that a segmentation was found\n",
    "                no_segmentations.append(image)\n",
    "            else:\n",
    "                images.append(image)\n",
    "                boxes[image] = bounding_areas['boxes']\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            corrupted_images.append(image)\n",
    "\n",
    "            \n",
    "#     count +=1\n",
    "#     # Using count for time being to get things working, remove once setup is complete\n",
    "#     if count == 5:\n",
    "#         break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting all the segemnted images into a dataloader, and loading model and processor for trocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting empty folders\n",
    "root = output_dir_craft\n",
    "folders = list(os.walk(root))[1:]\n",
    "deleted = []\n",
    "for folder in folders:\n",
    "    # folder example: ('FOLDER/3', [], ['file'])\n",
    "    if not folder[2]:\n",
    "        deleted.append(folder)\n",
    "        os.rmdir(folder[0])\n",
    "# Setting up the Tr-OCR model (using base model currently, large takes much longer)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\") \n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# Use all available gpu's\n",
    "model_gpu= nn.DataParallel(model).to(device)\n",
    "\n",
    "# Dataloader for working with gpu's\n",
    "trainset = datasets.ImageFolder(output_dir_craft, transform = processor)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)# for i, data in enumerate(trainloader):\n",
    "\n",
    "# For matching words to image\n",
    "filenames = [s.replace('_crops', '') for s in list(trainset.class_to_idx)]\n",
    "word_log_dic = {k: v for k,v in enumerate(filenames)}\n",
    "words_identified = {k: [] for v,k in enumerate(filenames)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yoinked https://github.com/rsommerfeld/trocr/blob/main/src/scripts.py\n",
    "\n",
    "def get_confidence_scores(generated_ids):\n",
    "    # Get raw logits, with shape (examples,tokens,token_vals)\n",
    "    logits = generated_ids.scores\n",
    "    logits = torch.stack(list(logits),dim=1)\n",
    "\n",
    "    # Transform logits to softmax and keep only the highest (chosen) p for each token\n",
    "    logit_probs = F.softmax(logits, dim=2)\n",
    "    char_probs = logit_probs.max(dim=2)[0]\n",
    "\n",
    "    # Only tokens of val>2 should influence the confidence. Thus, set probabilities to 1 for tokens 0-2\n",
    "    mask = generated_ids.sequences[:,:-1] > 2\n",
    "    char_probs[mask] = 1\n",
    "\n",
    "    # Confidence of each example is cumulative product of token probs\n",
    "    batch_confidence_scores = char_probs.cumprod(dim=1)[:, -1]\n",
    "    return [v.item() for v in batch_confidence_scores]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_craft_seg(model,trainloader):\n",
    "    results = []\n",
    "    confidence = []\n",
    "    label = []\n",
    "\n",
    "    model_gpu.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx,data in enumerate(tqdm(trainloader)):\n",
    "\n",
    "            images, labels = data\n",
    "            images, labels = images['pixel_values'][0].to(device), labels.to(device)\n",
    "\n",
    "            decoded = model_gpu.module.generate(images,return_dict_in_generate = True, output_scores = True) \n",
    "            final_values = processor.batch_decode(decoded.sequences, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "            confidences = get_confidence_scores(decoded)\n",
    "\n",
    "            for idx,value in enumerate(labels.cpu().numpy()):\n",
    "#           if confidences[idx]>.8: # to cull some of the really terrible guesses, probably want to do this in the search function instead\n",
    "                words_identified[word_log_dic[value]].append(final_values[idx])\n",
    "            \n",
    "            results.extend(final_values)\n",
    "            confidence.extend(confidences)\n",
    "            label.extend(labels.cpu().numpy())\n",
    "\n",
    "    return results,confidence,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the outputs\n",
    "results,confidence,labels = evaluate_craft_seg(model,trainloader)\n",
    "df = pd.DataFrame(list(zip(results,confidence,labels)),columns = ['Results','Confidence','Labels'])\n",
    "df.to_pickle('results7.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filepath):\n",
    "    filepkl = open(filepath, \"rb\")\n",
    "\n",
    "    # Unpickle the objects\n",
    "\n",
    "    unpickled = pickle.load(filepkl)\n",
    "    \n",
    "    return unpickled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_pickle('/usr3/graduate/colejh/'+'results5.pkl')\n",
    "results =pd.Series(a.Results)\n",
    "labels = list(a['Labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String matching installs\n",
    "# !pip install --force-reinstall numpy==1.18.5 # need this to work the string grouper\n",
    "# !pip install numpy\n",
    "# !pip install string-grouper\n",
    "\n",
    "from string_grouper import match_strings, match_most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    taxon_file = '/content/gdrive/MyDrive/corpus_taxon.txt'\n",
    "else:\n",
    "    taxon_file = workdir+'/taxon_corpus.txt'\n",
    "    geography_file = workdir+'/geography_corpus.txt'\n",
    "    collector_file = workdir+'/collector_corpus.txt'\n",
    "taxon = pd.read_csv(taxon_file, delimiter = \"\\t\", names=[\"Taxon\"]).squeeze()\n",
    "geography = pd.read_csv(geography_file, delimiter = \"\\t\", names=[\"Geography\"]).squeeze()\n",
    "collector = pd.read_csv(collector_file, delimiter = \"\\t\", names=[\"Collector\"]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# String matching\n",
    "minimum_similarity = .1\n",
    "results_series = pd.Series(results)\n",
    "taxon = pd.read_csv(taxon_file, delimiter = \"\\t\", names=[\"Taxon\"]).squeeze()\n",
    " \n",
    "start = time.time()\n",
    "matches = match_strings(taxon,results_series,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 4)\n",
    "end = time.time()\n",
    "print('time',end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(main,comparison_file,minimum_similarity):\n",
    "     # Function takes a main file containing strings, a comparison file to match against main,\n",
    "     #  and a minimum similarity confidence level. Returns a list of matches based on similarity.\n",
    "\n",
    "    if not isinstance(comparison_file, pd.Series):\n",
    "\n",
    "        comparison_file = pd.Series(comparison_file)\n",
    "\n",
    "    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)\n",
    "\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_score_per_image(df,labels,filenames,minimum_similarity):\n",
    "    # Getting the highest score for each individual image \n",
    "    index_to_labels = df.copy()\n",
    "    for a in index_to_labels.right_index.unique():\n",
    "        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]\n",
    "    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]\n",
    "\n",
    "#     print(\"Of the\",len(filenames),\"images evaluated\",len(unique_labels), \"have a prediction score above\",minimum_similarity*100, \"percent.\")\n",
    "    return unique_labels\n",
    "# unique_labels = highest_score_per_image(ascending_df,labels,filenames,minimum_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooled_match(comparison_file,labels,filenames, minimum_similarity = .7,**kwargs):\n",
    "    # Take in any number of files containing strings to match against and return a dictionary\n",
    "    # with keys the same name as input and values as the dataframe with matching information\n",
    "   \n",
    "    corpus_list = []\n",
    "    corpus_name = []\n",
    "    \n",
    "    for k,v in kwargs.items():\n",
    "        # Convert to series (string-grouper requires this type), will work if input is list, array, or series\n",
    "        if not isinstance(v, pd.Series):\n",
    "            v = pd.Series(v)\n",
    "        corpus_list.append(v)\n",
    "        corpus_name.append(k)\n",
    "    \n",
    "    func = partial(match, comparison_file = results_series,  minimum_similarity = minimum_similarity)\n",
    "    pool = multiprocessing.Pool()\n",
    "\n",
    "    result_dic = {}\n",
    "    for i,result in enumerate(pool.map(func,corpus_list)):\n",
    "        result.columns.values[1] = corpus_name[i]+' Corpus'\n",
    "        result.columns.values[3] = \"Predictions\"\n",
    "        result = result.drop('left_index', axis=1)\n",
    "        result = highest_score_per_image(result,labels,filenames,minimum_similarity)\n",
    "        result_dic[corpus_name[i]] = result\n",
    "\n",
    "   \n",
    "    return result_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels),len(filenames))\n",
    "results_series = pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "results = pooled_match(results_series,labels, filenames,minimum_similarity =.01,Taxon = taxon,Geography = g,Collector = collector)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "# for k,v in results.items():\n",
    "#     display(v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in results.items():\n",
    "    display(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganizing the df\n",
    "ascending_df = matches.sort_values(by=['similarity'],ascending=False)\n",
    "pd.set_option('display.width', 150)\n",
    "display(ascending_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matches_above_x(df,x):\n",
    "    return df.loc[df['similarity']>=.75]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "above_75 = matches_above_x(unique_labels,75)\n",
    "display(above_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the ground truth values\n",
    "gt_t = workdir+'/taxon_gt.txt'\n",
    "Taxon_truth = { line.split(\":\")[0] : line.split(\": \")[1].strip() for line in open(gt_t) }\n",
    "\n",
    "gt_g = workdir+'/geography_gt.txt'\n",
    "Geography_truth = { line.split(\":\")[0] : line.split(\": \")[1].strip() for line in open(gt_g) }\n",
    "\n",
    "gt_c = workdir+'/collector_gt.txt'\n",
    "Collector_truth = { line.split(\":\")[0] : line.split(\": \")[1].strip() for line in open(gt_c) }\n",
    "\n",
    "comparison_file = {\"Taxon\":Taxon_truth,\"Geography\":Geography_truth,\"Collector\":Collector_truth}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add in the check for other species names, quite a few would match\n",
    "prediction_and_imagenumber = list(zip(unique_labels.left_Taxon, unique_labels.right_index))\n",
    "count = 0\n",
    "for prediction,image_number in prediction_and_imagenumber:\n",
    "    try:\n",
    "        image = word_log_dic[image_number]\n",
    "        gt = ground_truth[image]\n",
    "        if gt == prediction:\n",
    "            count +=1\n",
    "#             print(gt,\"||\",prediction,'||',image)\n",
    "        else:\n",
    "            print('*',gt,\"||\",prediction,'||',image)\n",
    "    except KeyError as e:\n",
    "        print(\"Ground Truth Not Found for:\",word_log_dic[image_number])\n",
    "acc = count/len(unique_labels)\n",
    "print(\"Accuracy on Predicted:\",acc)\n",
    "print(\"Total accuracy: \",count/len(filenames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prRed(skk): print(\"\\033[91m{}\\033[00m\" .format(skk))\n",
    "    \n",
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in results.items():    \n",
    "    # need to add in the check for other species names, quite a few would match\n",
    "    prediction_and_imagenumber = list(zip(v[k+' Corpus'], v.right_index))\n",
    "    count = 0\n",
    "    print('Evaluation for',k)\n",
    "    for prediction,image_number in prediction_and_imagenumber:\n",
    "        try:\n",
    "            image = word_log_dic[image_number]\n",
    "            gt = comparison_file[k][image]\n",
    "            if gt == prediction:\n",
    "                count +=1\n",
    "                print(gt,\"||\",prediction,'||',image)\n",
    "            else:\n",
    "                print(color.RED+gt+\"||\"+prediction+'||'+image+color.END)\n",
    "        except KeyError as e:\n",
    "            print(\"Ground Truth Not Found for:\",word_log_dic[image_number])\n",
    "    \n",
    "    acc = count/len(v)\n",
    "    print(color.BOLD+\"Accuracy on Predicted:\"+str(acc)+color.END)\n",
    "    print(color.BOLD+\"Total accuracy: \"+str(count/len(filenames))+color.END)\n",
    "    print(color.BOLD+\"Total Guessed:\"+str(len(prediction_and_imagenumber))+color.END)\n",
    "    print('\\n\\n********************************\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in words_identified.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0j2-HKSMLK3"
   },
   "outputs": [],
   "source": [
    "# CRAFT on images to get bounding boxes\n",
    "images = []\n",
    "corrupted_images = []\n",
    "boxes = {}\n",
    "count= 0\n",
    "file_types = (\".jpg\", \".jpeg\",\".png\")\n",
    "for filename in tqdm(os.listdir(workdir)):\n",
    "    if filename.endswith(file_types):\n",
    "        image = workdir+'/'+filename\n",
    "        try:\n",
    "            img = Image.open(image) # open the image file\n",
    "            img.verify() # verify that it is, in fact an image\n",
    "            images.append(image)\n",
    "            bounding_areas = craft.detect_text(image)\n",
    "            boxes[image] = bounding_areas['boxes']\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            corrupted_images.append(image)\n",
    "\n",
    "            \n",
    "#     count +=1\n",
    "#     # Using count for time being to get things working, remove once setup is complete\n",
    "#     if count == 5:\n",
    "#         break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset2 = datasets.ImageFolder(workdir, transform = processor)\n",
    "# trainloader2 = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)# for i, data in enumerate(trainloader):\n",
    "\n",
    "# Checking size of all the segmentation images (about 1/6 input)\n",
    "root_directory = Path(workdir)\n",
    "sum(f.stat().st_size for f in root_directory.glob('**/*') if f.is_file())\n",
    "\n",
    "def save_bounding(filename,bounding_boxes):\n",
    "# function to save the boudning box outputs from CRAFT segmentation\n",
    "\n",
    "    f = open(filneame+\".pkl\",\"wb\")\n",
    "\n",
    "    # write the python object (dict) to pickle file\n",
    "    pickle.dump(bounding_boxes,f)\n",
    "\n",
    "    # close file\n",
    "    f.close()\n",
    "    \n",
    "def display_segmentations(boxes_dict): # Check this later if we dont want to save all the image segementations\n",
    "    # Function takes in the dictionary of image_path and bounding boxes and displays all of the segmentations\n",
    "    \n",
    "    count = 0\n",
    "    for image_path,bounding_boxes in boxes_dict.items():\n",
    "        original_image = cv2.imread(image_path)\n",
    "        for box in v:\n",
    "            segmentation = original_image[int(box[0][1]): int(box[2][1]), \n",
    "                  int(box[0][0]): int(box[2][0])]\n",
    "\n",
    "        segmented_image = Image.fromarray(segmentation).convert(\"RGB\")\n",
    "        display(segmented_image)\n",
    "        count+=1\n",
    "        if count>5:\n",
    "            break\n",
    "\n",
    "            \n",
    "# Downloading the tr-ocr model and processor\n",
    "# processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\") \n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# Training set creation\n",
    "# trainset = datasets.ImageFolder(output_dir_craft, transform = processor)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Put model on the gpu \n",
    "# model.to(device)\n",
    "\n",
    "# img_dir = '/usr3/graduate/colejh/craft3/'\n",
    "# for filename in os.listdir(img_dir):\n",
    "#     for filename2 in os.listdir(img_dir+'/'+filename):\n",
    "#         image = Image.open(img_dir+'/'+filename+'/'+filename2)\n",
    "#         pixel_values = processor(image, return_tensors=\"pt\").pixel_values \n",
    "#         generated_ids = model.generate(pixel_values)\n",
    "#         generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "#         display(image)\n",
    "#         print(\"Text: \"+generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking size of all the segmentation images (about 1/6 input)\n",
    "root_directory = Path(workdir)\n",
    "sum(f.stat().st_size for f in root_directory.glob('**/*') if f.is_file())\n",
    "\n",
    "\n",
    "# saving the bounding boxes dictionary \n",
    "f = open(\"boundingboxes.pkl\",\"wb\")\n",
    "\n",
    "# write the python object (dict) to pickle file\n",
    "pickle.dump(dict,f)\n",
    "\n",
    "# close file\n",
    "f.close()\n",
    "\n",
    "#Check this later if we dont want to save all the image segementations\n",
    "count = 0\n",
    "for k,v in boxes.items():\n",
    "    full_image = cv2.imread(k)\n",
    "    for box in v:\n",
    "        segmentation = full_image[int(box[0][1]): int(box[2][1]), \n",
    "              int(box[0][0]): int(box[2][0])]\n",
    "    \n",
    "    #Drop this later, just for checking stuff\n",
    "    image = Image.fromarray(segmentation).convert(\"RGB\")\n",
    "    display(image)\n",
    "   \n",
    "    count+=1\n",
    "    if count>5:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
