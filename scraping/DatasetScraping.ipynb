{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZTvvxKTKvFx"
      },
      "source": [
        "# Dataset Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCLIuKlvMZEF"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "AsJd5HlHNRUH"
      },
      "outputs": [],
      "source": [
        "from dwca.read import DwCAReader\n",
        "from dwca.darwincore.utils import qualname as qn\n",
        "import requests\n",
        "import shutil\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import pandas\n",
        "import mimetypes\n",
        "import json\n",
        "import zipfile\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUl1OcBR3Kfu"
      },
      "source": [
        "## Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "VULrSpbU3PZX"
      },
      "outputs": [],
      "source": [
        "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "PERCENT_TO_SCRAPE = 0.00002\n",
        "NUMBER_TO_SKIP = 40000\n",
        "DATASET_PATH = \"/projectnb/sparkgrp/ml-herbarium-data/\"\n",
        "DATASET_ARCHIVE = \"data.zip\"\n",
        "DATASET_CSV = \"data.csv\"\n",
        "OUTPUT_PATH = \"/projectnb/sparkgrp/ml-herbarium-data/scraped-data/\" + timestr + \"/\"\n",
        "DATASET_URL = (\n",
        "    \"https://api.gbif.org/v1/occurrence/download/request/0195391-210914110416597.zip\"\n",
        ")\n",
        "DATASET_TYPE = \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Dataset\n",
        "#### Only run this if the dataset needs to be redownladed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if os.path.exists(DATASET_PATH+DATASET_ARCHIVE):\n",
        "    os.remove(DATASET_PATH+DATASET_ARCHIVE)\n",
        "ds = requests.get(DATASET_URL, stream=True)\n",
        "with open(DATASET_PATH+DATASET_ARCHIVE, \"wb\") as f:\n",
        "    shutil.copyfileobj(ds.raw, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## For DWCA files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM3ZY6mUK1fG"
      },
      "source": [
        "### Open DWCA File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "mUo2pWfBMUCq"
      },
      "outputs": [],
      "source": [
        "dwca = DwCAReader(DATASET_PATH+DATASET_ARCHIVE)\n",
        "DATASET_TYPE = \"dwca\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-0PwbMjWVR8"
      },
      "source": [
        "### Test DWCA\n",
        "Will throw an error if the file is not opened correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI63ypo6WUgd",
        "outputId": "a8d75915-f9b7-4166-afe7-2c46873e54f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<dwca.rows.CoreRow at 0x7f226d896e80>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dwca.get_corerow_by_position(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_fmenuIx0_I"
      },
      "source": [
        "### Save DWCA Rows to Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J4XGVmeRx0_K"
      },
      "outputs": [],
      "source": [
        "# df = dwca.pd_read('occurrence.txt')\n",
        "df = dwca.pd_read(\"occurrence.txt\", low_memory=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3UvVB5Oahgc"
      },
      "source": [
        "#### Close the archive to free resources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "VevtWlmRajVo",
        "outputId": "57b4760f-7d08-4495-fd93-ae667d6edb6e"
      },
      "outputs": [],
      "source": [
        "dwca.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## For CSV files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save CSV Rows to Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(DATASET_PATH+DATASET_ARCHIVE, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(DATASET_PATH+DATASET_CSV)\n",
        "pandas.read_csv(DATASET_PATH+DATASET_CSV)\n",
        "DATASET_TYPE = \"csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print Pandas Column Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['id', 'abstract', 'accessRights', 'accrualMethod', 'accrualPeriodicity', 'accrualPolicy', 'alternative', 'audience', 'available', 'bibliographicCitation', 'conformsTo', 'contributor', 'coverage', 'created', 'creator', 'date', 'dateAccepted', 'dateCopyrighted', 'dateSubmitted', 'description', 'educationLevel', 'extent', 'format', 'hasFormat', 'hasPart', 'hasVersion', 'identifier', 'instructionalMethod', 'isFormatOf', 'isPartOf', 'isReferencedBy', 'isReplacedBy', 'isRequiredBy', 'isVersionOf', 'issued', 'language', 'license', 'mediator', 'medium', 'modified', 'provenance', 'publisher', 'references', 'relation', 'replaces', 'requires', 'rights', 'rightsHolder', 'source', 'spatial', 'subject', 'tableOfContents', 'temporal', 'title', 'type', 'valid', 'institutionID', 'collectionID', 'datasetID', 'institutionCode', 'collectionCode', 'datasetName', 'ownerInstitutionCode', 'basisOfRecord', 'informationWithheld', 'dataGeneralizations', 'dynamicProperties', 'occurrenceID', 'catalogNumber', 'recordNumber', 'recordedBy', 'recordedByID', 'individualCount', 'organismQuantity', 'organismQuantityType', 'sex', 'lifeStage', 'reproductiveCondition', 'behavior', 'establishmentMeans', 'degreeOfEstablishment', 'pathway', 'georeferenceVerificationStatus', 'occurrenceStatus', 'preparations', 'disposition', 'associatedOccurrences', 'associatedReferences', 'associatedSequences', 'associatedTaxa', 'otherCatalogNumbers', 'occurrenceRemarks', 'organismID', 'organismName', 'organismScope', 'associatedOrganisms', 'previousIdentifications', 'organismRemarks', 'materialSampleID', 'eventID', 'parentEventID', 'fieldNumber', 'eventDate', 'eventTime', 'startDayOfYear', 'endDayOfYear', 'year', 'month', 'day', 'verbatimEventDate', 'habitat', 'samplingProtocol', 'sampleSizeValue', 'sampleSizeUnit', 'samplingEffort', 'fieldNotes', 'eventRemarks', 'locationID', 'higherGeographyID', 'higherGeography', 'continent', 'waterBody', 'islandGroup', 'island', 'countryCode', 'stateProvince', 'county', 'municipality', 'locality', 'verbatimLocality', 'verbatimElevation', 'verticalDatum', 'verbatimDepth', 'minimumDistanceAboveSurfaceInMeters', 'maximumDistanceAboveSurfaceInMeters', 'locationAccordingTo', 'locationRemarks', 'decimalLatitude', 'decimalLongitude', 'coordinateUncertaintyInMeters', 'coordinatePrecision', 'pointRadiusSpatialFit', 'verbatimCoordinateSystem', 'verbatimSRS', 'footprintWKT', 'footprintSRS', 'footprintSpatialFit', 'georeferencedBy', 'georeferencedDate', 'georeferenceProtocol', 'georeferenceSources', 'georeferenceRemarks', 'geologicalContextID', 'earliestEonOrLowestEonothem', 'latestEonOrHighestEonothem', 'earliestEraOrLowestErathem', 'latestEraOrHighestErathem', 'earliestPeriodOrLowestSystem', 'latestPeriodOrHighestSystem', 'earliestEpochOrLowestSeries', 'latestEpochOrHighestSeries', 'earliestAgeOrLowestStage', 'latestAgeOrHighestStage', 'lowestBiostratigraphicZone', 'highestBiostratigraphicZone', 'lithostratigraphicTerms', 'group', 'formation', 'member', 'bed', 'identificationID', 'verbatimIdentification', 'identificationQualifier', 'typeStatus', 'identifiedBy', 'identifiedByID', 'dateIdentified', 'identificationReferences', 'identificationVerificationStatus', 'identificationRemarks', 'taxonID', 'scientificNameID', 'acceptedNameUsageID', 'parentNameUsageID', 'originalNameUsageID', 'nameAccordingToID', 'namePublishedInID', 'taxonConceptID', 'scientificName', 'acceptedNameUsage', 'parentNameUsage', 'originalNameUsage', 'nameAccordingTo', 'namePublishedIn', 'namePublishedInYear', 'higherClassification', 'kingdom', 'phylum', 'class', 'order', 'family', 'subfamily', 'genus', 'genericName', 'subgenus', 'infragenericEpithet', 'specificEpithet', 'infraspecificEpithet', 'cultivarEpithet', 'taxonRank', 'verbatimTaxonRank', 'vernacularName', 'nomenclaturalCode', 'taxonomicStatus', 'nomenclaturalStatus', 'taxonRemarks', 'datasetKey', 'publishingCountry', 'lastInterpreted', 'elevation', 'elevationAccuracy', 'depth', 'depthAccuracy', 'distanceAboveSurface', 'distanceAboveSurfaceAccuracy', 'issue', 'mediaType', 'hasCoordinate', 'hasGeospatialIssues', 'taxonKey', 'acceptedTaxonKey', 'kingdomKey', 'phylumKey', 'classKey', 'orderKey', 'familyKey', 'genusKey', 'subgenusKey', 'speciesKey', 'species', 'acceptedScientificName', 'verbatimScientificName', 'typifiedName', 'protocol', 'lastParsed', 'lastCrawled', 'repatriated', 'relativeOrganismQuantity', 'level0Gid', 'level0Name', 'level1Gid', 'level1Name', 'level2Gid', 'level2Name', 'level3Gid', 'level3Name', 'iucnRedListCategory', 'geodeticDatum']\n",
            "occurrenceID\n"
          ]
        }
      ],
      "source": [
        "colnames = []\n",
        "for col in df.columns:\n",
        "    colnames.append(col)\n",
        "print(colnames)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YStKKZKkjjyW"
      },
      "source": [
        "## Get Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export GBIF URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "160 IDs will be scraped.\n",
            "Successfully scraped 160 IDs.\n"
          ]
        }
      ],
      "source": [
        "data = {}\n",
        "\n",
        "NUMBER_TO_SKIP = math.floor(df.shape[0] / (df.shape[0] * PERCENT_TO_SCRAPE))\n",
        "NUMBER_TO_SCRAPE = math.ceil(df.shape[0] / NUMBER_TO_SKIP)\n",
        "print(str(NUMBER_TO_SCRAPE) + \" IDs will be scraped.\")\n",
        "for i in range(1, df.shape[0], NUMBER_TO_SKIP):\n",
        "    id = df.at[i, 'id']\n",
        "    if DATASET_TYPE == \"dwca\":\n",
        "        data[i] = {'id':str(id)}\n",
        "    elif DATASET_TYPE == \"csv\":\n",
        "        data[i] = {'id':str(id)}\n",
        "print('Successfully scraped ' + str(len(data)) + ' IDs.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fetch Image URLs and Specimen Data from GBIF API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data will be fetched for 160 occurrences.\n",
            "Progress: 160/160Successfully fetched data for 160 occurrences.\n"
          ]
        }
      ],
      "source": [
        "print('Data will be fetched for', len(data), 'occurrences.')\n",
        "i = 1\n",
        "for idx in data:\n",
        "    print(\"\\rProgress: \" + str(i)+'/'+str(len(data)), end=\"\")\n",
        "    rq = requests.get('https://api.gbif.org/v1/occurrence/' + str(data[idx]['id']))\n",
        "    data[idx]['img_url']=(json.loads(rq.content)['media'][0]['identifier'])\n",
        "    data[idx]['img_type']=(json.loads(rq.content)['media'][0]['format'])\n",
        "    data[idx]['country']=(json.loads(rq.content)['country'])\n",
        "    data[idx]['genus']=(json.loads(rq.content)['genus'])\n",
        "    data[idx]['species']=(json.loads(rq.content)['species'])\n",
        "    i+=1\n",
        "print('\\nSuccessfully fetched data for', len(data), 'occurrences.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 160/160"
          ]
        }
      ],
      "source": [
        "i=1\n",
        "if not os.path.exists(OUTPUT_PATH):\n",
        "    os.makedirs(OUTPUT_PATH)\n",
        "for idx in data:\n",
        "    img = requests.get(data[idx]['img_url'], stream=True)\n",
        "    with open(OUTPUT_PATH+str(idx)+mimetypes.guess_extension(data[idx]['img_type']),'wb') as f:\n",
        "        shutil.copyfileobj(img.raw, f)\n",
        "    print(\"\\rProgress: \" + str(i)+'/'+str(len(data)), end=\"\")\n",
        "    i+=1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export Geograpy Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Successfully wrote countries to file.\n"
          ]
        }
      ],
      "source": [
        "with open(OUTPUT_PATH+'countries.txt', 'w') as f:\n",
        "    for idx in data:\n",
        "        f.write(data[idx]['country']+'\\n')\n",
        "print('\\nSuccessfully wrote countries to file.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export Taxon Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Successfully wrote taxon to file.\n"
          ]
        }
      ],
      "source": [
        "with open(OUTPUT_PATH+'taxon.txt', 'w') as f:\n",
        "    for idx in data:\n",
        "        f.write(data[idx]['genus']+' '+data[idx]['species']+'\\n')\n",
        "print('\\nSuccessfully wrote taxon to file.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DatasetScraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
