{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZTvvxKTKvFx"
      },
      "source": [
        "# Dataset Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCLIuKlvMZEF"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "AsJd5HlHNRUH"
      },
      "outputs": [],
      "source": [
        "from dwca.read import DwCAReader\n",
        "from dwca.darwincore.utils import qualname as qn\n",
        "import requests\n",
        "import shutil\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import pandas\n",
        "import json\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import multiprocessing as mp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUl1OcBR3Kfu"
      },
      "source": [
        "## Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 401,
      "metadata": {
        "id": "VULrSpbU3PZX"
      },
      "outputs": [],
      "source": [
        "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "PERCENT_TO_SCRAPE = 0.00015\n",
        "NUMBER_TO_SKIP = 40000\n",
        "DATASET_PATH = \"/projectnb/sparkgrp/ml-herbarium-data/\"\n",
        "DATASET_ARCHIVE = \"data.zip\"\n",
        "DATASET_CSV = \"data.csv\"\n",
        "OUTPUT_PATH = \"/projectnb/sparkgrp/ml-herbarium-data/scraped-data/\" + timestr + \"/\"\n",
        "DATASET_URL = \"https://occurrence-download.gbif.org/occurrence/download/request/0196625-210914110416597.zip\"\n",
        "DATASET_TYPE = \"\"\n",
        "NUM_CORES = 50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Dataset\n",
        "#### Only run this if the dataset needs to be redownladed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|▉         | 113M/1.16G [00:20<03:10, 5.52MiB/s]\n",
            "100%|██████████| 1.16G/1.16G [01:48<00:00, 10.7MiB/s]\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(DATASET_PATH + DATASET_ARCHIVE):\n",
        "    os.remove(DATASET_PATH + DATASET_ARCHIVE)\n",
        "ds = requests.get(DATASET_URL, stream=True)\n",
        "total_size_in_bytes = int(ds.headers.get(\"content-length\", 0))\n",
        "block_size = 1024  # 1 Kibibyte\n",
        "progress_bar = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n",
        "with open(DATASET_PATH + DATASET_ARCHIVE, \"wb\") as f:\n",
        "    for data in ds.iter_content(block_size):\n",
        "        progress_bar.update(len(data))\n",
        "        f.write(data)\n",
        "progress_bar.close()\n",
        "if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
        "    print(\"ERROR, something went wrong\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## For DWCA files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM3ZY6mUK1fG"
      },
      "source": [
        "### Open DWCA File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "mUo2pWfBMUCq"
      },
      "outputs": [],
      "source": [
        "dwca = DwCAReader(DATASET_PATH + DATASET_ARCHIVE)\n",
        "DATASET_TYPE = \"dwca\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-0PwbMjWVR8"
      },
      "source": [
        "### Test DWCA\n",
        "Will throw an error if the file is not opened correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI63ypo6WUgd",
        "outputId": "a8d75915-f9b7-4166-afe7-2c46873e54f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<dwca.rows.CoreRow at 0x7f226d896e80>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dwca.get_corerow_by_position(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_fmenuIx0_I"
      },
      "source": [
        "### Save DWCA Rows to Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J4XGVmeRx0_K"
      },
      "outputs": [],
      "source": [
        "# df = dwca.pd_read('occurrence.txt')\n",
        "df = dwca.pd_read(\"occurrence.txt\", low_memory=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3UvVB5Oahgc"
      },
      "source": [
        "#### Close the archive to free resources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "VevtWlmRajVo",
        "outputId": "57b4760f-7d08-4495-fd93-ae667d6edb6e"
      },
      "outputs": [],
      "source": [
        "dwca.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## For CSV files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract CSV from zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if os.path.exists(DATASET_PATH + \"*.csv\"):\n",
        "    os.remove(DATASET_PATH + \"*.csv\")\n",
        "with zipfile.ZipFile(DATASET_PATH + DATASET_ARCHIVE, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(DATASET_PATH)\n",
        "f = glob(os.path.join(DATASET_PATH, \"*-*.csv\"))[0]\n",
        "os.rename(f, DATASET_PATH + DATASET_CSV)\n",
        "print(\"CSV file extracted\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save CSV Rows to Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file extracted\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr4/ugrad/en/ml-herbarium/env/lib64/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (2,14,16,17,19,21,24,25,26,32,33,34,36,37,38,39,40,41,43,45,46) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ],
      "source": [
        "df = pandas.read_csv(DATASET_PATH + DATASET_CSV, sep=\"\\t\")\n",
        "DATASET_TYPE = \"csv\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print Pandas Column Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['gbifID', 'datasetKey', 'occurrenceID', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species', 'infraspecificEpithet', 'taxonRank', 'scientificName', 'verbatimScientificName', 'verbatimScientificNameAuthorship', 'countryCode', 'locality', 'stateProvince', 'occurrenceStatus', 'individualCount', 'publishingOrgKey', 'decimalLatitude', 'decimalLongitude', 'coordinateUncertaintyInMeters', 'coordinatePrecision', 'elevation', 'elevationAccuracy', 'depth', 'depthAccuracy', 'eventDate', 'day', 'month', 'year', 'taxonKey', 'speciesKey', 'basisOfRecord', 'institutionCode', 'collectionCode', 'catalogNumber', 'recordNumber', 'identifiedBy', 'dateIdentified', 'license', 'rightsHolder', 'recordedBy', 'typeStatus', 'establishmentMeans', 'lastInterpreted', 'mediaType', 'issue']\n"
          ]
        }
      ],
      "source": [
        "colnames = []\n",
        "for col in df.columns:\n",
        "    colnames.append(col)\n",
        "print(colnames)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YStKKZKkjjyW"
      },
      "source": [
        "## Get Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export GBIF Ocurrence IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 434,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1198 IDs will be scraped.\n",
            "Successfully scraped 1198 IDs.\n"
          ]
        }
      ],
      "source": [
        "data = {}\n",
        "\n",
        "NUMBER_TO_SKIP = math.floor(df.shape[0] / (df.shape[0] * PERCENT_TO_SCRAPE))\n",
        "NUMBER_TO_SCRAPE = math.ceil(df.shape[0] / NUMBER_TO_SKIP)\n",
        "print(str(NUMBER_TO_SCRAPE) + \" IDs will be scraped.\")\n",
        "for i in range(1, df.shape[0], NUMBER_TO_SKIP):\n",
        "    if DATASET_TYPE == \"dwca\":\n",
        "        id = df.at[i, \"id\"]\n",
        "    elif DATASET_TYPE == \"csv\":\n",
        "        id = df.at[i, \"gbifID\"]\n",
        "    data[i] = {\"id\": str(id)}\n",
        "print(\"Successfully scraped \" + str(len(data)) + \" IDs.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fetch Image URLs and Specimen Data from GBIF API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 435,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data will be fetched for 1198 occurrences.\n",
            "Starting multiprocessing...\n",
            "Fetching data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1198/1198 [00:16<00:00, 73.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Successfully fetched data for 1198 occurrences.\n"
          ]
        }
      ],
      "source": [
        "print(\"Data will be fetched for\", len(data), \"occurrences.\")\n",
        "\n",
        "\n",
        "def scrape_occurrence(key):\n",
        "    rq = requests.get(\"https://api.gbif.org/v1/occurrence/\" + str(data[key][\"id\"]))\n",
        "    return_dict = {}\n",
        "    return_dict[key] = {}\n",
        "    return_dict[key][\"img_url\"] = json.loads(rq.content)[\"media\"][0][\"identifier\"]\n",
        "    return_dict[key][\"img_type\"] = json.loads(rq.content)[\"media\"][0][\"format\"]\n",
        "    return_dict[key][\"country\"] = json.loads(rq.content)[\"country\"]\n",
        "    return_dict[key][\"genus\"] = json.loads(rq.content)[\"genus\"]\n",
        "    return_dict[key][\"species\"] = json.loads(rq.content)[\"species\"]\n",
        "    return return_dict\n",
        "\n",
        "print(\"Starting multiprocessing...\")\n",
        "pool = mp.Pool(NUM_CORES)\n",
        "print(\"Fetching data...\")\n",
        "for item in tqdm(pool.imap(scrape_occurrence, data), total=len(data)):\n",
        "    data.update(item)\n",
        "pool.close()\n",
        "pool.join()\n",
        "\n",
        "print(\"\\nSuccessfully fetched data for\", len(data), \"occurrences.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 427,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting multiprocessing...\n",
            "Downloading images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1198/1198 [00:22<00:00, 54.02it/s]\n"
          ]
        }
      ],
      "source": [
        "def download_images(key):\n",
        "    img = requests.get(data[key][\"img_url\"], stream=True)\n",
        "    with open(\n",
        "        OUTPUT_PATH + str(key) + \".\" + data[1][\"img_type\"].split(\"/\", 1)[1], \"wb\"\n",
        "    ) as f:\n",
        "        shutil.copyfileobj(img.raw, f)\n",
        "\n",
        "\n",
        "if not os.path.exists(OUTPUT_PATH):\n",
        "    os.makedirs(OUTPUT_PATH)\n",
        "\n",
        "print(\"Starting multiprocessing...\")\n",
        "pool = mp.Pool(NUM_CORES)\n",
        "print(\"Downloading images...\")\n",
        "for _ in tqdm(pool.imap(download_images, data), total=len(data)):\n",
        "    pass\n",
        "pool.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export Geograpy Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 428,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Successfully wrote countries to file.\n"
          ]
        }
      ],
      "source": [
        "with open(OUTPUT_PATH + \"countries.txt\", \"w\") as f:\n",
        "    for idx in data:\n",
        "        f.write(data[idx][\"country\"] + \"\\n\")\n",
        "print(\"\\nSuccessfully wrote countries to file.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export Taxon Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 429,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Successfully wrote taxon to file.\n"
          ]
        }
      ],
      "source": [
        "with open(OUTPUT_PATH + \"taxon.txt\", \"w\") as f:\n",
        "    for idx in data:\n",
        "        f.write(data[idx][\"genus\"] + \" \" + data[idx][\"species\"] + \"\\n\")\n",
        "print(\"\\nSuccessfully wrote taxon to file.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DatasetScraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
