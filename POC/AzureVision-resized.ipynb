{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f17ba8c",
   "metadata": {},
   "source": [
    "# Azure Vision Implementaion - Dima "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd18180f",
   "metadata": {},
   "source": [
    "This notebook utilizes Azure AI Document Intelligence Studio to extract text from a set of Herbarium specimens. There was a previous issue with high quality images being too large for Azure to process, this has been fixed through the resize_image function in this notebook that converts all images to 4mb or less. We have now been able to resize all images in the /projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/ folder, same folder used by the previous semester's group testing their TROCR models. \n",
    "\n",
    "Currently: the notebook takes an input image from: /projectnb/sparkgrp/ml-herbarium-grp/fall2023/LLM_Specimens, runs it through Azure Vision, analyzes all text, creates a pdf with the original image, an annotated image that has boxes around identified words and predicted words written over the original text. Below the image the entire text identified is printed along with the confidence score for each identified term. All this is saved and stored in: /projectnb/sparkgrp/ml-herbarium-grp/fall2023/AzureVision-results.\n",
    "\n",
    "-Previously there was a text recognition issue with images that have both text and the plant itself, this has been resolved.\n",
    "-Experimentation was conducted to see if a custom Azure AI model could be trained to extract Taxon, collector, date, and geography data, this proved to be inefficient and the quality was poor.\n",
    "-As a result the model will focus on extracting all the text from each image and using Open AI's ChatGPT to process the text into a Darwin JSON format. \n",
    "\n",
    "We are also looking into validating our results. We are looking into the validation dataset used by the group that created the TROCR model and we are also considering creating our own validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff2d218",
   "metadata": {},
   "source": [
    "For the sake of presentation we are also looking into creating a simple user friendly demo app that will enable user to input a Herbarium sample, press a button, and see the processed result- the Taxon, Collection Date, Collector Name, and the Geography. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc1c7278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install azure-ai-formrecognizer --pre\n",
    "#!pip install opencv-python-headless matplotlib\n",
    "#!pip install matplotlib pillow\n",
    "#!pip install ipywidgets\n",
    "#!pip install shapely\n",
    "#!pip install openai\n",
    "#!pip install reportlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179830cd",
   "metadata": {},
   "source": [
    "# Resizing images to smaller size for API to accept them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f398fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def resize_image(input_path, output_path, max_size_mb, quality=85):\n",
    "    \"\"\"\n",
    "    Resize the image found at input_path and save it to output_path.\n",
    "    The image is resized to be under max_size_mb megabytes.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    with Image.open(input_path) as img:\n",
    "        # Calculate target size to maintain aspect ratio\n",
    "        ratio = img.width / img.height\n",
    "        target_width = int((max_size_mb * 1024 * 1024 * ratio) ** 0.5)\n",
    "        target_height = int(target_width / ratio)\n",
    "\n",
    "        # Resize the image\n",
    "        resized_img = img.resize((target_width, target_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "        # Save the resized image\n",
    "        resized_img.save(output_path, quality=quality)\n",
    "\n",
    "input_folder = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/'\n",
    "output_folder = '/projectnb/sparkgrp/ml-herbarium-grp/fall2023/resized-images'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for file_name in os.listdir(input_folder):\n",
    "    file_path = os.path.join(input_folder, file_name)\n",
    "    output_file_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "    # Check if the file is an image\n",
    "    if file_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        resize_image(file_path, output_file_path, max_size_mb=4)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cebe57",
   "metadata": {},
   "source": [
    "The code below sets up a connection to Azure Cognitive Services for document analysis, and includes functions for sanitizing filenames and formatting bounding boxes.\n",
    "\n",
    "It defines a function to annotate images with extracted text and bounding boxes, and another function to parse document content using GPT-4.\n",
    "\n",
    "The main function, analyze_read, reads images, extracts text using Azure, annotates these images, and creates a PDF report that includes both the original and annotated images, along with the extracted text.\n",
    "\n",
    "The results are saved in /projectnb/sparkgrp/ml-herbarium-grp/fall2023/AzureVision-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1566288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import openai\n",
    "import re\n",
    "import os\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.units import inch\n",
    "\n",
    "\n",
    "# Azure Cognitive Services endpoint and key\n",
    "endpoint = \"https://herbariumsamplerecognition.cognitiveservices.azure.com/\"\n",
    "key = \"AZURE KEY HERE\"\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Remove characters that are not alphanumeric, spaces, dots, or underscores\n",
    "    return re.sub(r'[^\\w\\s\\.-]', '', filename)\n",
    "\n",
    "def format_bounding_box(bounding_box):\n",
    "    if not bounding_box:\n",
    "        return \"N/A\"\n",
    "    return \", \".join([\"[{}, {}]\".format(p.x, p.y) for p in bounding_box])\n",
    "\n",
    "\n",
    "def draw_boxes(image_path, words):\n",
    "    original_image = Image.open(image_path)\n",
    "    annotated_image = original_image.copy()\n",
    "    draw = ImageDraw.Draw(annotated_image)\n",
    "\n",
    "    for word in words:\n",
    "        polygon = word['polygon']\n",
    "        if polygon:\n",
    "            bbox = [(point.x, point.y) for point in polygon]\n",
    "            try:\n",
    "                # Replace special characters that cannot be encoded in 'latin-1'\n",
    "                text_content = word['content'].encode('ascii', 'ignore').decode('ascii')\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text {word['content']}: {e}\")\n",
    "                text_content = \"Error\"\n",
    "            draw.polygon(bbox, outline=\"red\")\n",
    "            draw.text((bbox[0][0], bbox[0][1]), text_content, fill=\"green\")\n",
    "\n",
    "    \n",
    "    return annotated_image\n",
    "\n",
    "page_width, page_height = letter \n",
    "\n",
    "# Function to calculate scale to fit the image within page dimensions\n",
    "def calculate_scale(image, max_width, max_height):\n",
    "    scale_w = max_width / image.width\n",
    "    scale_h = max_height / image.height\n",
    "    return min(scale_w, scale_h)\n",
    "\n",
    "\n",
    "# Needs to be fixed\n",
    "def parse_document_content(content):\n",
    "    openai.api_key = 'your-api-key'\n",
    "\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            model=\"gpt-4\",\n",
    "            prompt=f\"Extract specific information from the following text: {content}\\n\\nSpecies Name: \",\n",
    "            max_tokens=100\n",
    "            # Add additional parameters as needed\n",
    "        )\n",
    "        parsed_data = response.choices[0].text.strip()\n",
    "        return parsed_data\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def analyze_read(image_path, output_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            image_stream = f.read()\n",
    "\n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "        )\n",
    "\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-read\", image_stream)\n",
    "        result = poller.result()\n",
    "\n",
    "       # Collect words, their polygon data, and confidence\n",
    "        words = []\n",
    "        confidence_text = \"\"\n",
    "        for page in result.pages:\n",
    "            for word in page.words:\n",
    "                words.append({\n",
    "                    'content': word.content,\n",
    "                    'polygon': word.polygon\n",
    "                })\n",
    "                confidence_text += \"'{}' confidence {}\\n\".format(word.content, word.confidence)\n",
    "\n",
    "        document_content = result.content + \"\\n\\nConfidence Metrics:\\n\" + confidence_text\n",
    "        #parsed_info = parse_document_content(document_content)\n",
    "\n",
    "        original_image = Image.open(image_path)\n",
    "        annotated_img = draw_boxes(image_path, words)\n",
    "\n",
    "        # Set up PDF\n",
    "        output_filename = os.path.join(output_path, sanitize_filename(os.path.basename(image_path).replace('.jpg', '.pdf')))\n",
    "        c = canvas.Canvas(output_filename, pagesize=letter)\n",
    "        width, height = letter  # usually 612 x 792\n",
    "\n",
    "        # Draw original image\n",
    "        if original_image.height <= height:\n",
    "            c.drawImage(image_path, 0, height - original_image.height, width=original_image.width, height=original_image.height, mask='auto')\n",
    "            y_position = height - original_image.height\n",
    "        else:\n",
    "            # Handle large images or add scaling logic here\n",
    "            pass\n",
    "\n",
    "        \n",
    "        # Draw original image\n",
    "        scale = calculate_scale(original_image, page_width, page_height)\n",
    "        img_width, img_height = original_image.width * scale, original_image.height * scale\n",
    "        c.drawImage(image_path, 0, page_height - img_height, width=img_width, height=img_height, mask='auto')\n",
    "        y_position = page_height - img_height\n",
    "\n",
    "        # Draw annotated image\n",
    "        annotated_image_path = '/tmp/annotated_image.png'\n",
    "        annotated_img.save(annotated_image_path)\n",
    "        scale = calculate_scale(annotated_img, page_width, page_height)\n",
    "        annotated_img_width, annotated_img_height = annotated_img.width * scale, annotated_img.height * scale\n",
    "        if y_position - annotated_img_height >= 0:\n",
    "            c.drawImage(annotated_image_path, 0, y_position - annotated_img_height, width=annotated_img_width, height=annotated_img_height, mask='auto')\n",
    "        else:\n",
    "            c.showPage()\n",
    "            c.drawImage(annotated_image_path, 0, page_height - annotated_img_height, width=annotated_img_width, height=annotated_img_height, mask='auto')\n",
    "        \n",
    "\n",
    "        # Add text\n",
    "        textobject = c.beginText()\n",
    "        textobject.setTextOrigin(10, y_position - 15)\n",
    "        textobject.setFont(\"Times-Roman\", 12)\n",
    "\n",
    "        for line in document_content.split('\\n'):\n",
    "            if textobject.getY() - 15 < 0:  # Check if new page is needed for more text\n",
    "                c.drawText(textobject)\n",
    "                c.showPage()\n",
    "                textobject = c.beginText()\n",
    "                textobject.setTextOrigin(10, height - 15)\n",
    "                textobject.setFont(\"Times-Roman\", 12)\n",
    "            textobject.textLine(line)\n",
    "        \n",
    "        c.drawText(textobject)\n",
    "        c.save()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {image_path}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = '/projectnb/sparkgrp/ml-herbarium-grp/fall2023/resized-images'\n",
    "    output_folder = '/projectnb/sparkgrp/ml-herbarium-grp/fall2023/AzureVision-results'\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    image_counter = 0  # Initialize a counter for the number of images processed\n",
    "\n",
    "    # Iterate over each image in the input folder\n",
    "    for image_file in os.listdir(input_folder):\n",
    "        image_path = os.path.join(input_folder, image_file)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            analyze_read(image_path, output_folder)\n",
    "            image_counter += 1  # Increment the counter\n",
    "\n",
    "            if image_counter >= 25:  # Stop after processing 5 images\n",
    "                break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e791a84b",
   "metadata": {},
   "source": [
    "# Experimenting With Cropping Images\n",
    "\n",
    "So far results have been better for images that have not been cropped (cropping function needs improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0998c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import openai\n",
    "import re\n",
    "import os\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.units import inch\n",
    "\n",
    "\n",
    "# Azure Cognitive Services endpoint and key\n",
    "endpoint = \"https://herbariumsamplerecognition.cognitiveservices.azure.com/\"\n",
    "key = \"AZURE KEY HERE\"\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Remove characters that are not alphanumeric, spaces, dots, or underscores\n",
    "    return re.sub(r'[^\\w\\s\\.-]', '', filename)\n",
    "\n",
    "def format_bounding_box(bounding_box):\n",
    "    if not bounding_box:\n",
    "        return \"N/A\"\n",
    "    return \", \".join([\"[{}, {}]\".format(p.x, p.y) for p in bounding_box])\n",
    "\n",
    "def draw_boxes(image_path, words):\n",
    "    original_image = Image.open(image_path)\n",
    "    annotated_image = original_image.copy()\n",
    "    draw = ImageDraw.Draw(annotated_image)\n",
    "\n",
    "    for word in words:\n",
    "        polygon = word['polygon']\n",
    "        if polygon:\n",
    "            bbox = [(point.x, point.y) for point in polygon]\n",
    "            try:\n",
    "                # Replace special characters that cannot be encoded in 'latin-1'\n",
    "                text_content = word['content'].encode('ascii', 'ignore').decode('ascii')\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text {word['content']}: {e}\")\n",
    "                text_content = \"Error\"\n",
    "            draw.polygon(bbox, outline=\"red\")\n",
    "            draw.text((bbox[0][0], bbox[0][1]), text_content, fill=\"green\")\n",
    "    \n",
    "    return annotated_image\n",
    "\n",
    "page_width, page_height = letter \n",
    "\n",
    "# Function to calculate scale to fit the image within page dimensions\n",
    "def calculate_scale(image, max_width, max_height):\n",
    "    scale_w = max_width / image.width\n",
    "    scale_h = max_height / image.height\n",
    "    return min(scale_w, scale_h)\n",
    "\n",
    "\n",
    "def get_text_density_map(pages):\n",
    "    density_map = {}\n",
    "    for page in pages:\n",
    "        for line in page.lines:\n",
    "            points = line.polygon\n",
    "            if points:\n",
    "                x_center = sum(point.x for point in points) / len(points)\n",
    "                y_center = sum(point.y for point in points) / len(points)\n",
    "                density_map[(x_center, y_center)] = density_map.get((x_center, y_center), 0) + 1\n",
    "    return density_map\n",
    "\n",
    "def find_highest_density_area(density_map):\n",
    "    # This function will find the center of the area with the highest text density\n",
    "    # For simplicity, this example just returns the center with the highest count\n",
    "    # In a real scenario, you might want to consider a more sophisticated method\n",
    "    # that takes into account the size and proximity of the high-density areas\n",
    "    return max(density_map, key=density_map.get)\n",
    "\n",
    "def crop_image_to_text(image_path, density_center, crop_size):\n",
    "    with Image.open(image_path) as img:\n",
    "        # Calculate the coordinates for the crop\n",
    "        left = max(density_center[0] - crop_size[0] // 2, 0)\n",
    "        upper = max(density_center[1] - crop_size[1] // 2, 0)\n",
    "        right = min(density_center[0] + crop_size[0] // 2, img.width)\n",
    "        lower = min(density_center[1] + crop_size[1] // 2, img.height)\n",
    "\n",
    "        # Debug output\n",
    "        print(f\"Cropping coordinates: left={left}, upper={upper}, right={right}, lower={lower}\")\n",
    "\n",
    "        # Perform the crop\n",
    "        cropped_img = img.crop((left, upper, right, lower))\n",
    "        return cropped_img\n",
    "    \n",
    "    \n",
    "def get_text_bounding_boxes(pages):\n",
    "    bounding_boxes = []\n",
    "    for page in pages:\n",
    "        for line in page.lines:\n",
    "            if line.polygon:\n",
    "                box = [(point.x, point.y) for point in line.polygon]\n",
    "                bounding_boxes.append(box)\n",
    "    return bounding_boxes\n",
    "\n",
    "def combine_text_regions(image_path, bounding_boxes):\n",
    "    original_image = Image.open(image_path)\n",
    "\n",
    "    # Calculate the combined bounding box\n",
    "    min_x = min(min(box, key=lambda x: x[0])[0] for box in bounding_boxes)\n",
    "    min_y = min(min(box, key=lambda x: x[1])[1] for box in bounding_boxes)\n",
    "    max_x = max(max(box, key=lambda x: x[0])[0] for box in bounding_boxes)\n",
    "    max_y = max(max(box, key=lambda x: x[1])[1] for box in bounding_boxes)\n",
    "\n",
    "    # Create a new blank image with integer dimensions\n",
    "    combined_image = Image.new('RGB', (int(max_x - min_x), int(max_y - min_y)), (255, 255, 255))\n",
    "    \n",
    "    for box in bounding_boxes:\n",
    "        cropped_region = original_image.crop((min(box, key=lambda x: x[0])[0], \n",
    "                                              min(box, key=lambda x: x[1])[1], \n",
    "                                              max(box, key=lambda x: x[0])[0], \n",
    "                                              max(box, key=lambda x: x[1])[1]))\n",
    "        # Paste the cropped region at integer coordinates\n",
    "        combined_image.paste(cropped_region, (int(box[0][0] - min_x), int(box[0][1] - min_y)))\n",
    "\n",
    "    return combined_image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################\n",
    "def parse_document_content(content):\n",
    "    openai.api_key = 'your-api-key'\n",
    "\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            model=\"gpt-4\",\n",
    "            prompt=f\"Extract specific information from the following text: {content}\\n\\nSpecies Name: \",\n",
    "            max_tokens=100\n",
    "            # Add additional parameters as needed\n",
    "        )\n",
    "        parsed_data = response.choices[0].text.strip()\n",
    "        return parsed_data\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None\n",
    "####################################################\n",
    "\n",
    "def analyze_text_density_and_crop(image_path):\n",
    "    document_analysis_client = DocumentAnalysisClient(endpoint=endpoint, credential=AzureKeyCredential(key))\n",
    "    \n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_stream = f.read()\n",
    "\n",
    "    poller = document_analysis_client.begin_analyze_document(\"prebuilt-read\", image_stream)\n",
    "    result = poller.result()\n",
    "\n",
    "    # Get bounding boxes of text regions\n",
    "    bounding_boxes = get_text_bounding_boxes(result.pages)\n",
    "\n",
    "    # Combine the text regions into one image\n",
    "    combined_image = combine_text_regions(image_path, bounding_boxes)\n",
    "\n",
    "    # Save the combined image temporarily and return its path\n",
    "    combined_image_path = '/tmp/combined_image.png'\n",
    "    combined_image.save(combined_image_path)\n",
    "    return combined_image_path\n",
    "\n",
    "\n",
    "def analyze_read(image_path, output_path, show_first_output=False):\n",
    "    combined_image_path = analyze_text_density_and_crop(image_path)\n",
    "\n",
    "    try:\n",
    "        # Process the combined image with Azure Form Recognizer\n",
    "        with open(combined_image_path, \"rb\") as f:\n",
    "            combined_image_stream = f.read()\n",
    "\n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "        )\n",
    "\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-read\", combined_image_stream)\n",
    "        result = poller.result()\n",
    "\n",
    "        # Collect words, their polygon data, and confidence\n",
    "        words = []\n",
    "        for page in result.pages:\n",
    "            for word in page.words:\n",
    "                words.append({\n",
    "                    'content': word.content,\n",
    "                    'polygon': word.polygon\n",
    "                })\n",
    "\n",
    "        # Prepare annotated image\n",
    "        annotated_img = draw_boxes(combined_image_path, words)\n",
    "\n",
    "        # Set up PDF\n",
    "        output_filename = os.path.join(output_path, sanitize_filename(os.path.basename(image_path).replace('.jpg', '.pdf')))\n",
    "        c = canvas.Canvas(output_filename, pagesize=letter)\n",
    "\n",
    "        # Draw original image\n",
    "        original_image = Image.open(image_path)\n",
    "        scale = calculate_scale(original_image, page_width, page_height)\n",
    "        img_width, img_height = original_image.width * scale, original_image.height * scale\n",
    "        c.drawImage(image_path, 0, page_height - img_height, width=img_width, height=img_height, mask='auto')\n",
    "        y_position = page_height - img_height\n",
    "\n",
    "        # Draw annotated combined image\n",
    "        annotated_image_path = '/tmp/annotated_image.png'\n",
    "        annotated_img.save(annotated_image_path)\n",
    "        scale = calculate_scale(annotated_img, page_width, page_height)\n",
    "        annotated_img_width, annotated_img_height = annotated_img.width * scale, annotated_img.height * scale\n",
    "        if y_position - annotated_img_height >= 0:\n",
    "            c.drawImage(annotated_image_path, 0, y_position - annotated_img_height, width=annotated_img_width, height=annotated_img_height, mask='auto')\n",
    "        else:\n",
    "            c.showPage()  # Start a new page if not enough space\n",
    "            c.drawImage(annotated_image_path, 0, page_height - annotated_img_height, width=annotated_img_width, height=annotated_img_height, mask='auto')\n",
    "\n",
    "        # Add text\n",
    "        textobject = c.beginText()\n",
    "        textobject.setTextOrigin(10, y_position - 15)\n",
    "        textobject.setFont(\"Times-Roman\", 12)\n",
    "\n",
    "        document_content = '\\n'.join([word['content'] for word in words])\n",
    "        for line in document_content.split('\\n'):\n",
    "            if textobject.getY() - 15 < 0:  # Check if new page is needed for more text\n",
    "                c.drawText(textobject)\n",
    "                c.showPage()\n",
    "                textobject = c.beginText()\n",
    "                textobject.setTextOrigin(10, page_height - 15)\n",
    "                textobject.setFont(\"Times-Roman\", 12)\n",
    "            textobject.textLine(line)\n",
    "\n",
    "        c.drawText(textobject)\n",
    "        c.save()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {combined_image_path}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = '/projectnb/sparkgrp/ml-herbarium-grp/fall2023/resized-images'\n",
    "    output_folder = '/projectnb/sparkgrp/ml-herbarium-grp/fall2023/AzureVision-results'\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    image_counter = 0  # Initialize a counter for the number of images processed\n",
    "\n",
    "    # Iterate over each image in the input folder\n",
    "    for image_file in os.listdir(input_folder):\n",
    "        image_path = os.path.join(input_folder, image_file)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            analyze_read(image_path, output_folder, show_first_output=not first_output_shown)\n",
    "            image_counter += 1  # Increment the counter\n",
    "\n",
    "            if image_counter >= 1:  # Stop after processing 5 images\n",
    "                break\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
