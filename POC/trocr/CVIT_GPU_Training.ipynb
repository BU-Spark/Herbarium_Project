{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de25c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Getting all the models off my home directory for space concerns\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/projectnb/sparkgrp/colejh'\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "from transformers import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "\n",
    "#suppressing all the huggingface warnings\n",
    "SUPPRESS = True\n",
    "if SUPPRESS:\n",
    "    from transformers.utils import logging\n",
    "    logging.set_verbosity(40)\n",
    "\n",
    "#others\n",
    "#ignoring UserWarning and FutureWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d80235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catch_invalid(img):\n",
    "    try:\n",
    "        a = Image.open(img)\n",
    "        a.close()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Check for valid images\n",
    "\n",
    "def catch_invalid2(img):\n",
    "    try:\n",
    "        a = Image.open(img)\n",
    "        a.close()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False,img\n",
    "    return True\n",
    "try:\n",
    "    #reading in saved cvit file\n",
    "    save_dir = '/projectnb/sparkgrp/colejh/saved_results/'\n",
    "    df = pd.read_pickle(save_dir+'cvit.pkl')\n",
    "except:\n",
    "    directory = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/tesseract-training/training/CVIT/Images_90K_Normalized/'\n",
    "    count = 0\n",
    "    all_img = []\n",
    "    all_labels = []\n",
    "    no_tag = []\n",
    "    # list_dir = os.listdir(directory)\n",
    "\n",
    "    for (dirpath, dirnames, filenames) in os.walk(directory):\n",
    "        for file in filenames:\n",
    "            if file.endswith('.png'):\n",
    "                image = os.path.join(dirpath, file)\n",
    "    #             display(Image.open(image))\n",
    "    #             if catch_invalid(image):\n",
    "                try:\n",
    "                    with open(image.split('.')[0]+'.gt.txt') as f:\n",
    "                        contents = f.readlines()\n",
    "                    all_img.append(image)\n",
    "                    all_labels.append(contents[0])\n",
    "                except FileNotFoundError as f:\n",
    "    #                             print(f)\n",
    "                        no_tag.append(image)\n",
    "                        break # some folders have no ground truth\n",
    "\n",
    "            count+=1\n",
    "            if count%10000 == 0:\n",
    "                print(count)\n",
    "\n",
    "\n",
    "    # create dataframe from all_img and all_labels\n",
    "    df = pd.DataFrame({'image':all_img, 'label':all_labels})\n",
    "\n",
    "    save_dir = '/projectnb/sparkgrp/colejh/saved_results/'\n",
    "    # save dataframe to pickle at specified directory\n",
    "    df.to_pickle(save_dir+'cvit.pkl')\n",
    "\n",
    "\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "    bad = []\n",
    "    for output in tqdm(pool.imap(catch_invalid2, df['image']), total=len(df['image']),desc = 'Checking for Valid Images'):\n",
    "        if output == False:\n",
    "    #         bad.append(output[1])\n",
    "            print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ab78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CVIT dataset class  \n",
    "class CVITDataset(Dataset):\n",
    "    def __init__(self, df, processor, max_target_length=128):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        image = self.df['image'][idx]\n",
    "        text = self.df['label'][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(image).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4abfdc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "model_directory = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/'\n",
    "def create_CVIT(df,processor):\n",
    "    # Training and validaiton splits\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    val_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    train_dataset = CVITDataset(df=train_df,processor=processor)\n",
    "    val_dataset = CVITDataset(df=val_df,processor=processor)\n",
    "\n",
    "    return train_dataset,val_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e63cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "cer_metric = load_metric(\"cer\")\n",
    "def compute_cer(pred_ids, label_ids,processor):\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "#     print(pred_str,label_str)\n",
    "    return cer\n",
    "\n",
    "\n",
    "from accelerate import Accelerator\n",
    "import wandb\n",
    "from accelerate import notebook_launcher\n",
    "from accelerate.utils import set_seed\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "def training_loop(df,mixed_precision=\"fp16\", seed: int = 42, batch_size: int = 32,num_processes=4):\n",
    "    set_seed(seed)\n",
    "\n",
    "    num_epochs = 3\n",
    "    # Initialize accelerator\n",
    "    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "    accelerator = Accelerator(mixed_precision=mixed_precision,kwargs_handlers=[ddp_kwargs],log_with=\"wandb\")\n",
    "    accelerator.init_trackers(\"CVIT_TRAIN\")\n",
    "    # Build dataloaders\n",
    "    # Instantiate the model (you build the model here so that the seed also controls new weight initaliziations)\n",
    "    device = accelerator.device\n",
    "    model_name = 'microsoft/trocr-base-printed'\n",
    "    processor = TrOCRProcessor.from_pretrained(model_name) \n",
    "    train_dataset,val_dataset = create_CVIT(df,processor)\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "    model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "    model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "    model.config.vocab_size = model.config.decoder.vocab_size\n",
    "    model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "    model.config.max_length = 64\n",
    "    model.config.early_stopping = True\n",
    "    model.config.no_repeat_ngram_size = 3\n",
    "    model.config.length_penalty = 2.0\n",
    "    model.config.num_beams = 4\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    eval_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Intantiate the optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    # Instantiate the learning rate scheduler\n",
    "    lr_scheduler = OneCycleLR(optimizer=optimizer, max_lr=1e-5, epochs=num_epochs, steps_per_epoch=len(train_dataloader))\n",
    "\n",
    "    # Preparing everything for multi-gpu support\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    "    )\n",
    "    \n",
    "    # Getting the progress bar set up \n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    patience = 0\n",
    "    best_val_loss = float('inf')\n",
    "    last_val_loss = float('inf')\n",
    "    running_tloss = []\n",
    "    running_vloss = []\n",
    "    running_cer = []\n",
    "    # Now you train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        # Track metrics\n",
    "        train_loss = 0.0\n",
    "        running_loss = 0\n",
    "        model.train()\n",
    "        for i,batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Print out the loss 100 times per epoch just to confirm training is going well \n",
    "            if accelerator.is_main_process:\n",
    "                try:\n",
    "                    running_loss += loss.sum().item()\n",
    "                    if i % round(len(train_dataloader)/100) == round(len(train_dataloader)/100)-1:   \n",
    "                        accelerator.print('[Epoch: %d, Batches Processed: %d] loss: %.3f' %\n",
    "                              (epoch + 1, i + 1, running_loss /round(len(train_dataloader)/100)))\n",
    "                        running_loss = 0.0\n",
    "                except ZeroDivisionError: # if the trainloader  is small enough this isnt neccesary \n",
    "                    pass\n",
    "                    \n",
    "        train_loss = train_loss/len(train_dataloader)      \n",
    "        running_tloss.append(train_loss)\n",
    "\n",
    "        # Evaluation \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        valid_cer = 0\n",
    "        \n",
    "        for batch in eval_dataloader:\n",
    "            with torch.no_grad():\n",
    "\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "                a =accelerator.unwrap_model(model).generate(batch[\"pixel_values\"])\n",
    "                cer = compute_cer(pred_ids=a, label_ids=batch[\"labels\"],processor = processor)\n",
    "                valid_cer += cer \n",
    "        val_loss = val_loss/len(eval_dataloader)\n",
    "        valid_cer = valid_cer/len(eval_dataloader)\n",
    "        running_vloss.append(val_loss)\n",
    "        running_cer.append(valid_cer/len(eval_dataloader))\n",
    "        accelerator.log({\"train_loss\": train_loss, \"valid_loss\": val_loss,\"CER\": valid_cer}, step=epoch)\n",
    "        # Print metrics\n",
    "#         accelerator.print(f\"Epoch {epoch} validation loss: {val_loss}\")\n",
    "        # Save model if the validation loss is the best we've seen so far.\n",
    "        accelerator.print(f\"Epoch {epoch} train loss: {train_loss} CER: {valid_cer:.2f} Val Loss: {val_loss}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            patience = 0\n",
    "            best_val_loss = val_loss\n",
    "#             accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "#             if accelerator.is_main_process:\n",
    "#                 unwrapped_model.save_pretrained(os.path.join(model_directory,\"best_model\"))\n",
    "            accelerator.print(\"Saving model checkpoint to \",os.path.join(model_directory,model_name.split('/')[1]+'_best_model_CVITpoint1.pt'))\n",
    "            accelerator.wait_for_everyone()\n",
    "            torch.save(unwrapped_model.state_dict(), model_directory + model_name.split('/')[1]+'_best_model_CVITpoint1.pt')\n",
    "            \n",
    "#         elif val_loss >= last_val_loss:\n",
    "#             patience += 1\n",
    "#             if patience == 3:\n",
    "                \n",
    "#                 accelerator.print(\"Stopping training\")\n",
    "# #                 \n",
    "#                 return 2\n",
    "        last_val_loss = val_loss\n",
    "    accelerator.end_training()\n",
    "        # Use accelerator.print to print only on the main process.\n",
    "#     global_metrics['train_loss'] = running_tloss\n",
    "#     global_metrics['val_loss'] = running_vloss\n",
    "#     global_metrics['cer'] = running_cer\n",
    "# #     accelerator.wait_for_everyone()\n",
    "#     global_metrics['model'] = accelerator.unwrap_model(model)\n",
    "#     global_metrics['processor'] = processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89a24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlilloukas\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/space/wandb/run-20221204_192919-2k3v061v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/lilloukas/CVIT_TRAIN/runs/2k3v061v\" target=\"_blank\">decent-frost-17</a></strong> to <a href=\"https://wandb.ai/lilloukas/CVIT_TRAIN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/8971998.1.cds-gpu/ipykernel_56446/2810057373.py:60: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  progress_bar = tqdm(range(num_training_steps))\n",
      "/scratch/8971998.1.cds-gpu/ipykernel_56446/2810057373.py:60: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  progress_bar = tqdm(range(num_training_steps))\n",
      "/scratch/8971998.1.cds-gpu/ipykernel_56446/2810057373.py:60: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  progress_bar = tqdm(range(num_training_steps))\n",
      "/scratch/8971998.1.cds-gpu/ipykernel_56446/2810057373.py:60: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  progress_bar = tqdm(range(num_training_steps))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8ed01db35146f399346c7e9ee3bc02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc0a3c561f14855a6721e930e7446dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267e6ed6045a463db0e5be4a70facdb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9818544e0ee64d84ab4adc26b874d516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1, Batches Processed: 128] loss: 8.818\n",
      "[Epoch: 1, Batches Processed: 256] loss: 2.823\n",
      "[Epoch: 1, Batches Processed: 384] loss: 1.648\n",
      "[Epoch: 1, Batches Processed: 512] loss: 1.255\n",
      "[Epoch: 1, Batches Processed: 640] loss: 1.012\n",
      "[Epoch: 1, Batches Processed: 768] loss: 0.928\n",
      "[Epoch: 1, Batches Processed: 896] loss: 0.910\n",
      "[Epoch: 1, Batches Processed: 1024] loss: 0.804\n",
      "[Epoch: 1, Batches Processed: 1152] loss: 0.740\n",
      "[Epoch: 1, Batches Processed: 1280] loss: 0.690\n",
      "[Epoch: 1, Batches Processed: 1408] loss: 0.612\n",
      "[Epoch: 1, Batches Processed: 1536] loss: 0.613\n",
      "[Epoch: 1, Batches Processed: 1664] loss: 0.591\n",
      "[Epoch: 1, Batches Processed: 1792] loss: 0.547\n",
      "[Epoch: 1, Batches Processed: 1920] loss: 0.525\n",
      "[Epoch: 1, Batches Processed: 2048] loss: 0.511\n",
      "[Epoch: 1, Batches Processed: 2176] loss: 0.497\n",
      "[Epoch: 1, Batches Processed: 2304] loss: 0.469\n",
      "[Epoch: 1, Batches Processed: 2432] loss: 0.436\n",
      "[Epoch: 1, Batches Processed: 2560] loss: 0.466\n",
      "[Epoch: 1, Batches Processed: 2688] loss: 0.463\n",
      "[Epoch: 1, Batches Processed: 2816] loss: 0.436\n",
      "[Epoch: 1, Batches Processed: 2944] loss: 0.424\n",
      "[Epoch: 1, Batches Processed: 3072] loss: 0.416\n",
      "[Epoch: 1, Batches Processed: 3200] loss: 0.393\n",
      "[Epoch: 1, Batches Processed: 3328] loss: 0.405\n",
      "[Epoch: 1, Batches Processed: 3456] loss: 0.356\n",
      "[Epoch: 1, Batches Processed: 3584] loss: 0.375\n",
      "[Epoch: 1, Batches Processed: 3712] loss: 0.338\n",
      "[Epoch: 1, Batches Processed: 3840] loss: 0.358\n",
      "[Epoch: 1, Batches Processed: 3968] loss: 0.329\n",
      "[Epoch: 1, Batches Processed: 4096] loss: 0.350\n",
      "[Epoch: 1, Batches Processed: 4224] loss: 0.334\n",
      "[Epoch: 1, Batches Processed: 4352] loss: 0.307\n",
      "[Epoch: 1, Batches Processed: 4480] loss: 0.279\n",
      "[Epoch: 1, Batches Processed: 4608] loss: 0.298\n",
      "[Epoch: 1, Batches Processed: 4736] loss: 0.313\n",
      "[Epoch: 1, Batches Processed: 4864] loss: 0.277\n",
      "[Epoch: 1, Batches Processed: 4992] loss: 0.268\n",
      "[Epoch: 1, Batches Processed: 5120] loss: 0.261\n",
      "[Epoch: 1, Batches Processed: 5248] loss: 0.256\n",
      "[Epoch: 1, Batches Processed: 5376] loss: 0.279\n",
      "[Epoch: 1, Batches Processed: 5504] loss: 0.280\n",
      "[Epoch: 1, Batches Processed: 5632] loss: 0.276\n",
      "[Epoch: 1, Batches Processed: 5760] loss: 0.276\n",
      "[Epoch: 1, Batches Processed: 5888] loss: 0.270\n",
      "[Epoch: 1, Batches Processed: 6016] loss: 0.234\n",
      "[Epoch: 1, Batches Processed: 6144] loss: 0.247\n",
      "[Epoch: 1, Batches Processed: 6272] loss: 0.248\n",
      "[Epoch: 1, Batches Processed: 6400] loss: 0.241\n",
      "[Epoch: 1, Batches Processed: 6528] loss: 0.232\n",
      "[Epoch: 1, Batches Processed: 6656] loss: 0.247\n",
      "[Epoch: 1, Batches Processed: 6784] loss: 0.230\n",
      "[Epoch: 1, Batches Processed: 6912] loss: 0.247\n",
      "[Epoch: 1, Batches Processed: 7040] loss: 0.232\n",
      "[Epoch: 1, Batches Processed: 7168] loss: 0.207\n",
      "[Epoch: 1, Batches Processed: 7296] loss: 0.225\n",
      "[Epoch: 1, Batches Processed: 7424] loss: 0.230\n",
      "[Epoch: 1, Batches Processed: 7552] loss: 0.215\n",
      "[Epoch: 1, Batches Processed: 7680] loss: 0.243\n",
      "[Epoch: 1, Batches Processed: 7808] loss: 0.223\n",
      "[Epoch: 1, Batches Processed: 7936] loss: 0.218\n",
      "[Epoch: 1, Batches Processed: 8064] loss: 0.212\n",
      "[Epoch: 1, Batches Processed: 8192] loss: 0.205\n",
      "[Epoch: 1, Batches Processed: 8320] loss: 0.188\n",
      "[Epoch: 1, Batches Processed: 8448] loss: 0.206\n",
      "[Epoch: 1, Batches Processed: 8576] loss: 0.202\n",
      "[Epoch: 1, Batches Processed: 8704] loss: 0.190\n",
      "[Epoch: 1, Batches Processed: 8832] loss: 0.170\n",
      "[Epoch: 1, Batches Processed: 8960] loss: 0.187\n",
      "[Epoch: 1, Batches Processed: 9088] loss: 0.185\n",
      "[Epoch: 1, Batches Processed: 9216] loss: 0.199\n",
      "[Epoch: 1, Batches Processed: 9344] loss: 0.181\n",
      "[Epoch: 1, Batches Processed: 9472] loss: 0.224\n",
      "[Epoch: 1, Batches Processed: 9600] loss: 0.205\n",
      "[Epoch: 1, Batches Processed: 9728] loss: 0.180\n",
      "[Epoch: 1, Batches Processed: 9856] loss: 0.189\n",
      "[Epoch: 1, Batches Processed: 9984] loss: 0.179\n",
      "[Epoch: 1, Batches Processed: 10112] loss: 0.210\n",
      "[Epoch: 1, Batches Processed: 10240] loss: 0.199\n",
      "[Epoch: 1, Batches Processed: 10368] loss: 0.193\n",
      "[Epoch: 1, Batches Processed: 10496] loss: 0.171\n",
      "[Epoch: 1, Batches Processed: 10624] loss: 0.144\n",
      "[Epoch: 1, Batches Processed: 10752] loss: 0.149\n",
      "[Epoch: 1, Batches Processed: 10880] loss: 0.148\n",
      "[Epoch: 1, Batches Processed: 11008] loss: 0.151\n",
      "[Epoch: 1, Batches Processed: 11136] loss: 0.150\n",
      "[Epoch: 1, Batches Processed: 11264] loss: 0.164\n",
      "[Epoch: 1, Batches Processed: 11392] loss: 0.163\n",
      "[Epoch: 1, Batches Processed: 11520] loss: 0.134\n",
      "[Epoch: 1, Batches Processed: 11648] loss: 0.173\n",
      "[Epoch: 1, Batches Processed: 11776] loss: 0.146\n",
      "[Epoch: 1, Batches Processed: 11904] loss: 0.170\n",
      "[Epoch: 1, Batches Processed: 12032] loss: 0.149\n",
      "[Epoch: 1, Batches Processed: 12160] loss: 0.146\n",
      "[Epoch: 1, Batches Processed: 12288] loss: 0.149\n",
      "[Epoch: 1, Batches Processed: 12416] loss: 0.140\n",
      "[Epoch: 1, Batches Processed: 12544] loss: 0.136\n",
      "[Epoch: 1, Batches Processed: 12672] loss: 0.149\n",
      "[Epoch: 1, Batches Processed: 12800] loss: 0.130\n",
      "Epoch 0 train loss: 0.43774623893852765 CER: 0.01 Val Loss: 0.10204403689192201\n",
      "Saving model checkpoint to  /projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/trocr-base-printed_best_model_CVITpoint1.pt\n",
      "[Epoch: 2, Batches Processed: 128] loss: 0.098\n",
      "[Epoch: 2, Batches Processed: 256] loss: 0.106\n",
      "[Epoch: 2, Batches Processed: 384] loss: 0.151\n",
      "[Epoch: 2, Batches Processed: 512] loss: 0.111\n",
      "[Epoch: 2, Batches Processed: 640] loss: 0.090\n",
      "[Epoch: 2, Batches Processed: 768] loss: 0.096\n",
      "[Epoch: 2, Batches Processed: 896] loss: 0.089\n",
      "[Epoch: 2, Batches Processed: 1024] loss: 0.105\n",
      "[Epoch: 2, Batches Processed: 1152] loss: 0.096\n",
      "[Epoch: 2, Batches Processed: 1280] loss: 0.103\n",
      "[Epoch: 2, Batches Processed: 1408] loss: 0.099\n",
      "[Epoch: 2, Batches Processed: 1536] loss: 0.095\n",
      "[Epoch: 2, Batches Processed: 1664] loss: 0.100\n",
      "[Epoch: 2, Batches Processed: 1792] loss: 0.110\n",
      "[Epoch: 2, Batches Processed: 1920] loss: 0.123\n",
      "[Epoch: 2, Batches Processed: 2048] loss: 0.124\n",
      "[Epoch: 2, Batches Processed: 2176] loss: 0.091\n",
      "[Epoch: 2, Batches Processed: 2304] loss: 0.131\n",
      "[Epoch: 2, Batches Processed: 2432] loss: 0.119\n",
      "[Epoch: 2, Batches Processed: 2560] loss: 0.108\n",
      "[Epoch: 2, Batches Processed: 2688] loss: 0.100\n",
      "[Epoch: 2, Batches Processed: 2816] loss: 0.103\n",
      "[Epoch: 2, Batches Processed: 2944] loss: 0.079\n",
      "[Epoch: 2, Batches Processed: 3072] loss: 0.101\n",
      "[Epoch: 2, Batches Processed: 3200] loss: 0.096\n",
      "[Epoch: 2, Batches Processed: 3328] loss: 0.080\n",
      "[Epoch: 2, Batches Processed: 3456] loss: 0.088\n",
      "[Epoch: 2, Batches Processed: 3584] loss: 0.078\n",
      "[Epoch: 2, Batches Processed: 3712] loss: 0.090\n",
      "[Epoch: 2, Batches Processed: 3840] loss: 0.088\n",
      "[Epoch: 2, Batches Processed: 3968] loss: 0.105\n",
      "[Epoch: 2, Batches Processed: 4096] loss: 0.087\n",
      "[Epoch: 2, Batches Processed: 4224] loss: 0.091\n",
      "[Epoch: 2, Batches Processed: 4352] loss: 0.103\n",
      "[Epoch: 2, Batches Processed: 4480] loss: 0.080\n",
      "[Epoch: 2, Batches Processed: 4608] loss: 0.070\n",
      "[Epoch: 2, Batches Processed: 4736] loss: 0.078\n",
      "[Epoch: 2, Batches Processed: 4864] loss: 0.072\n",
      "[Epoch: 2, Batches Processed: 4992] loss: 0.081\n",
      "[Epoch: 2, Batches Processed: 5120] loss: 0.075\n",
      "[Epoch: 2, Batches Processed: 5248] loss: 0.072\n",
      "[Epoch: 2, Batches Processed: 5376] loss: 0.067\n",
      "[Epoch: 2, Batches Processed: 5504] loss: 0.090\n",
      "[Epoch: 2, Batches Processed: 5632] loss: 0.075\n",
      "[Epoch: 2, Batches Processed: 5760] loss: 0.085\n",
      "[Epoch: 2, Batches Processed: 5888] loss: 0.079\n",
      "[Epoch: 2, Batches Processed: 6016] loss: 0.084\n",
      "[Epoch: 2, Batches Processed: 6144] loss: 0.073\n",
      "[Epoch: 2, Batches Processed: 6272] loss: 0.062\n",
      "[Epoch: 2, Batches Processed: 6400] loss: 0.072\n",
      "[Epoch: 2, Batches Processed: 6528] loss: 0.081\n",
      "[Epoch: 2, Batches Processed: 6656] loss: 0.066\n",
      "[Epoch: 2, Batches Processed: 6784] loss: 0.073\n",
      "[Epoch: 2, Batches Processed: 6912] loss: 0.072\n",
      "[Epoch: 2, Batches Processed: 7040] loss: 0.066\n",
      "[Epoch: 2, Batches Processed: 7168] loss: 0.067\n",
      "[Epoch: 2, Batches Processed: 7296] loss: 0.067\n",
      "[Epoch: 2, Batches Processed: 7424] loss: 0.063\n",
      "[Epoch: 2, Batches Processed: 7552] loss: 0.055\n",
      "[Epoch: 2, Batches Processed: 7680] loss: 0.068\n",
      "[Epoch: 2, Batches Processed: 7808] loss: 0.050\n",
      "[Epoch: 2, Batches Processed: 7936] loss: 0.062\n",
      "[Epoch: 2, Batches Processed: 8064] loss: 0.068\n",
      "[Epoch: 2, Batches Processed: 8192] loss: 0.059\n",
      "[Epoch: 2, Batches Processed: 8320] loss: 0.048\n",
      "[Epoch: 2, Batches Processed: 8448] loss: 0.052\n",
      "[Epoch: 2, Batches Processed: 8576] loss: 0.045\n",
      "[Epoch: 2, Batches Processed: 8704] loss: 0.049\n",
      "[Epoch: 2, Batches Processed: 8832] loss: 0.043\n",
      "[Epoch: 2, Batches Processed: 8960] loss: 0.066\n",
      "[Epoch: 2, Batches Processed: 9088] loss: 0.060\n",
      "[Epoch: 2, Batches Processed: 9216] loss: 0.061\n",
      "[Epoch: 2, Batches Processed: 9344] loss: 0.050\n",
      "[Epoch: 2, Batches Processed: 9472] loss: 0.044\n",
      "[Epoch: 2, Batches Processed: 9600] loss: 0.057\n",
      "[Epoch: 2, Batches Processed: 9728] loss: 0.047\n",
      "[Epoch: 2, Batches Processed: 9856] loss: 0.052\n",
      "[Epoch: 2, Batches Processed: 9984] loss: 0.051\n",
      "[Epoch: 2, Batches Processed: 10112] loss: 0.057\n",
      "[Epoch: 2, Batches Processed: 10240] loss: 0.051\n",
      "[Epoch: 2, Batches Processed: 10368] loss: 0.046\n",
      "[Epoch: 2, Batches Processed: 10496] loss: 0.040\n",
      "[Epoch: 2, Batches Processed: 10624] loss: 0.061\n",
      "[Epoch: 2, Batches Processed: 10752] loss: 0.042\n",
      "[Epoch: 2, Batches Processed: 10880] loss: 0.045\n",
      "[Epoch: 2, Batches Processed: 11008] loss: 0.040\n",
      "[Epoch: 2, Batches Processed: 11136] loss: 0.042\n",
      "[Epoch: 2, Batches Processed: 11264] loss: 0.041\n",
      "[Epoch: 2, Batches Processed: 11392] loss: 0.042\n",
      "[Epoch: 2, Batches Processed: 11520] loss: 0.040\n",
      "[Epoch: 2, Batches Processed: 11648] loss: 0.043\n",
      "[Epoch: 2, Batches Processed: 11776] loss: 0.036\n",
      "[Epoch: 2, Batches Processed: 11904] loss: 0.042\n",
      "[Epoch: 2, Batches Processed: 12032] loss: 0.038\n",
      "[Epoch: 2, Batches Processed: 12160] loss: 0.030\n",
      "[Epoch: 2, Batches Processed: 12288] loss: 0.036\n",
      "[Epoch: 2, Batches Processed: 12416] loss: 0.038\n",
      "[Epoch: 2, Batches Processed: 12544] loss: 0.035\n",
      "[Epoch: 2, Batches Processed: 12672] loss: 0.034\n",
      "[Epoch: 2, Batches Processed: 12800] loss: 0.040\n",
      "Epoch 1 train loss: 0.07207205181621135 CER: 0.00 Val Loss: 0.03133037095359383\n",
      "Saving model checkpoint to  /projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/trocr-base-printed_best_model_CVITpoint1.pt\n",
      "[Epoch: 3, Batches Processed: 128] loss: 0.027\n",
      "[Epoch: 3, Batches Processed: 256] loss: 0.024\n",
      "[Epoch: 3, Batches Processed: 384] loss: 0.021\n",
      "[Epoch: 3, Batches Processed: 512] loss: 0.019\n",
      "[Epoch: 3, Batches Processed: 640] loss: 0.028\n",
      "[Epoch: 3, Batches Processed: 768] loss: 0.023\n",
      "[Epoch: 3, Batches Processed: 896] loss: 0.022\n",
      "[Epoch: 3, Batches Processed: 1024] loss: 0.024\n",
      "[Epoch: 3, Batches Processed: 1152] loss: 0.021\n",
      "[Epoch: 3, Batches Processed: 1280] loss: 0.021\n",
      "[Epoch: 3, Batches Processed: 1408] loss: 0.025\n",
      "[Epoch: 3, Batches Processed: 1536] loss: 0.019\n",
      "[Epoch: 3, Batches Processed: 1664] loss: 0.026\n",
      "[Epoch: 3, Batches Processed: 1792] loss: 0.021\n",
      "[Epoch: 3, Batches Processed: 1920] loss: 0.025\n",
      "[Epoch: 3, Batches Processed: 2048] loss: 0.021\n",
      "[Epoch: 3, Batches Processed: 2176] loss: 0.021\n",
      "[Epoch: 3, Batches Processed: 2304] loss: 0.025\n",
      "[Epoch: 3, Batches Processed: 2432] loss: 0.019\n",
      "[Epoch: 3, Batches Processed: 2560] loss: 0.016\n",
      "[Epoch: 3, Batches Processed: 2688] loss: 0.017\n",
      "[Epoch: 3, Batches Processed: 2816] loss: 0.016\n",
      "[Epoch: 3, Batches Processed: 2944] loss: 0.019\n",
      "[Epoch: 3, Batches Processed: 3072] loss: 0.023\n",
      "[Epoch: 3, Batches Processed: 3200] loss: 0.025\n",
      "[Epoch: 3, Batches Processed: 3328] loss: 0.018\n",
      "[Epoch: 3, Batches Processed: 3456] loss: 0.019\n",
      "[Epoch: 3, Batches Processed: 3584] loss: 0.019\n",
      "[Epoch: 3, Batches Processed: 3712] loss: 0.018\n",
      "[Epoch: 3, Batches Processed: 3840] loss: 0.019\n",
      "[Epoch: 3, Batches Processed: 3968] loss: 0.018\n",
      "[Epoch: 3, Batches Processed: 4096] loss: 0.020\n",
      "[Epoch: 3, Batches Processed: 4224] loss: 0.018\n",
      "[Epoch: 3, Batches Processed: 4352] loss: 0.016\n",
      "[Epoch: 3, Batches Processed: 4480] loss: 0.023\n",
      "[Epoch: 3, Batches Processed: 4608] loss: 0.018\n",
      "[Epoch: 3, Batches Processed: 4736] loss: 0.014\n",
      "[Epoch: 3, Batches Processed: 4864] loss: 0.017\n",
      "[Epoch: 3, Batches Processed: 4992] loss: 0.015\n",
      "[Epoch: 3, Batches Processed: 5120] loss: 0.015\n",
      "[Epoch: 3, Batches Processed: 5248] loss: 0.016\n",
      "[Epoch: 3, Batches Processed: 5376] loss: 0.015\n",
      "[Epoch: 3, Batches Processed: 5504] loss: 0.015\n",
      "[Epoch: 3, Batches Processed: 5632] loss: 0.014\n",
      "[Epoch: 3, Batches Processed: 5760] loss: 0.017\n",
      "[Epoch: 3, Batches Processed: 5888] loss: 0.014\n",
      "[Epoch: 3, Batches Processed: 6016] loss: 0.016\n",
      "[Epoch: 3, Batches Processed: 6144] loss: 0.016\n",
      "[Epoch: 3, Batches Processed: 6272] loss: 0.015\n",
      "[Epoch: 3, Batches Processed: 6400] loss: 0.017\n",
      "[Epoch: 3, Batches Processed: 6528] loss: 0.022\n",
      "[Epoch: 3, Batches Processed: 6656] loss: 0.012\n",
      "[Epoch: 3, Batches Processed: 6784] loss: 0.012\n",
      "[Epoch: 3, Batches Processed: 6912] loss: 0.013\n",
      "[Epoch: 3, Batches Processed: 7040] loss: 0.013\n",
      "[Epoch: 3, Batches Processed: 7168] loss: 0.018\n",
      "[Epoch: 3, Batches Processed: 7296] loss: 0.017\n",
      "[Epoch: 3, Batches Processed: 7424] loss: 0.015\n",
      "[Epoch: 3, Batches Processed: 7552] loss: 0.014\n",
      "[Epoch: 3, Batches Processed: 7680] loss: 0.013\n",
      "[Epoch: 3, Batches Processed: 7808] loss: 0.013\n",
      "[Epoch: 3, Batches Processed: 7936] loss: 0.013\n",
      "[Epoch: 3, Batches Processed: 8064] loss: 0.012\n",
      "[Epoch: 3, Batches Processed: 8192] loss: 0.012\n",
      "[Epoch: 3, Batches Processed: 8320] loss: 0.014\n",
      "[Epoch: 3, Batches Processed: 8448] loss: 0.016\n",
      "[Epoch: 3, Batches Processed: 8576] loss: 0.013\n"
     ]
    }
   ],
   "source": [
    "# global_metrics = {}\n",
    "df2 = df.sample(frac=.1)\n",
    "args = (df2,\"fp16\", 42, 8)\n",
    "notebook_launcher(training_loop, args, num_processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184bdea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c27ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
