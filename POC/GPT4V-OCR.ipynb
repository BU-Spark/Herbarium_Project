{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT4 Vision OCR implementation - Smriti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./lib/python3.10/site-packages (1.1.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./lib/python3.10/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: tqdm>4 in ./lib/python3.10/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./lib/python3.10/site-packages (from openai) (0.25.1)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in ./lib/python3.10/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in ./lib/python3.10/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./lib/python3.10/site-packages (from openai) (2.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in ./lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
      "Requirement already satisfied: idna>=2.8 in ./lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: httpcore in ./lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.1)\n",
      "Requirement already satisfied: certifi in ./lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in ./lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./lib/python3.10/site-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the API we make requests to the newly released GPT4-V model to recognize the text from the TROCR Evaluation set data and save the responses in a json format in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projectnb/sparkgrp/ml-herbarium-grp\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from collections import defaultdict\n",
    "import base64\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# os.chdir(\"ml-herbarium-grp\")\n",
    "print(os.getcwd())\n",
    "json_results = defaultdict()\n",
    "too_big = []\n",
    "# OpenAI API Key\n",
    "api_key = key\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Path to your image\n",
    "folder_path = \"ml-herbarium-data/TROCR_Training/goodfiles/\"\n",
    "image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith(('jpg', 'png'))]\n",
    "\n",
    "for img in image_files:\n",
    "    if os.stat(img).st_size > 19000000:\n",
    "        too_big.append(img)\n",
    "    else:\n",
    "    # Getting the base64 string\n",
    "        base64_image = encode_image(img)\n",
    "        \n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {api_key}\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": \"gpt-4-vision-preview\",\n",
    "            \"messages\": [\n",
    "              {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                  {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Extract all the text, both typed and handwritten from this image and display it in a JSON format\"\n",
    "                  },\n",
    "                  {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                      \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                  }\n",
    "                ]\n",
    "              }\n",
    "            ],\n",
    "            \"max_tokens\": 4096\n",
    "        }\n",
    "    \n",
    "        response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "        json_results[img] = response.json()\n",
    "    \n",
    "    # print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note : We observed that the image file size was limited to 20MB and so we were not able to pass the higher resolution images (162 of them) to the model. Future steps would involve compressing the larger files to be more size appropriate for GPT4-V. We currently obtain results for 64 images from the evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ml-herbarium-data/TROCR_Training/goodfiles/1802552799.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998322454.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/2595747531.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998413329.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1675940934.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998540182.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1990825865.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/2236176339.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1999330570.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998481102.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/2265485412.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/2573054178.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/3341257544.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1455960532.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998370401.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1317726996.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1999314245.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1929881359.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998550976.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998465329.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/3341248414.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998729261.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/3467354375.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998980522.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1929709919.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1931124118.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1455174725.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998882868.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998590947.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1702827727.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/437160969.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1999032409.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/2236597761.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/2595756978.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998543749.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1317746297.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998758107.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998686142.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1852143901.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1999283271.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/2858981761.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/2265566552.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/2859305213.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1928034398.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1928479020.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1999110359.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/2265588715.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998994775.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1702851818.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1929883118.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/2859205685.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1852124166.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1456276626.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998387136.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1999047345.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1998843424.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1928246346.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1930449245.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1927799942.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1999105588.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1999253468.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1928276301.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1929084979.jpg', 'ml-herbarium-data/TROCR_Training/goodfiles/1319864119.jpg'])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(json_results\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(json_results\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mjson_results\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m():\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m json_results[l]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "copyjson = json_results.copy()\n",
    "print(json_results.keys())\n",
    "for l in list(json_results.keys()):\n",
    "    if \"choices\" not in json_results[l].keys():\n",
    "        del json_results[l]\n",
    "    else:\n",
    "        json_results[l] = json_results[l]['choices'][0]['message']['content']\n",
    "\n",
    "print(len(json_results))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    }
   ],
   "source": [
    "print(len(too_big))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1802552799.jpg', '1998322454.jpg', '2595747531.jpg', '1998413329.jpg', '1675940934.jpg', '1998540182.jpg', '1990825865.jpg', '2236176339.jpg', '1999330570.jpg', '1998481102.jpg', '2265485412.jpg', '2573054178.jpg', '3341257544.jpg', '1455960532.jpg', '1998370401.jpg', '1317726996.jpg', '1999314245.jpg', '1929881359.jpg', '1998550976.jpg', '1998465329.jpg', '3341248414.jpg', '1998729261.jpg', '3467354375.jpg', '1998980522.jpg', '1929709919.jpg', '1931124118.jpg', '1455174725.jpg', '1998882868.jpg', '1998590947.jpg', '1702827727.jpg', '437160969.jpg', '1999032409.jpg', '2236597761.jpg', '2595756978.jpg', '1998543749.jpg', '1317746297.jpg', '1998758107.jpg', '1998686142.jpg', '1852143901.jpg', '1999283271.jpg', '2858981761.jpg', '2265566552.jpg', '2859305213.jpg', '1928034398.jpg', '1928479020.jpg', '1999110359.jpg', '2265588715.jpg', '1998994775.jpg', '1702851818.jpg', '1929883118.jpg', '2859205685.jpg', '1852124166.jpg', '1456276626.jpg', '1998387136.jpg', '1999047345.jpg', '1998843424.jpg', '1928246346.jpg', '1930449245.jpg', '1927799942.jpg', '1999105588.jpg', '1999253468.jpg', '1928276301.jpg', '1929084979.jpg', '1319864119.jpg'])\n"
     ]
    }
   ],
   "source": [
    "paths = list(json_results.keys())\n",
    "folder_path = \"ml-herbarium-data/TROCR_Training/goodfiles/\"\n",
    "img_names = [i.replace(folder_path, \"\") for i in paths]\n",
    "\n",
    "# print(img_names)\n",
    "final_dict = defaultdict()\n",
    "\n",
    "for img in img_names:\n",
    "    final_dict[img] = json_results[folder_path+img]\n",
    "\n",
    "print(final_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'message': {'role': 'assistant', 'content': '```json\\n{\\n  \"Image Number\": \"00427028\",\\n  \"Specimen ID\": \"1627083\",\\n  \"Herbarium\": \"UNITED STATES NATIONAL MUSEUM\",\\n  \"Flora\": \"Flora Hawaiiensis\",\\n  \"Collected by\": \"C. N. Forbes on Oahu\",\\n  \"Species Name\": \"Cheirodendron platyphyllum (Hook. & Arn.) Frodin\",\\n  \"Collection Date\": \"Apr. 26 - May 6 - 1911\",\\n  \"Accession Number\": \"No. 74318\",\\n  \"Barcode of the Bishop Museum Herbarium\": \"Image No. 00427028\"\\n}\\n```'}, 'finish_details': {'type': 'stop', 'stop': '<|fim_suffix|>'}, 'index': 0}]\n"
     ]
    }
   ],
   "source": [
    "#TEST CODE\n",
    "# lists = list(json_results.values())\n",
    "# print(lists[0]['choices'])\n",
    "# contents = [l['choices'][0]['message']['content'] for l in lists]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "# print(len(contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"Herbarium Label\": {\n",
      "    \"Scientific Name\": \"Cheirodendron trigynum (Gaud.) A. Heller var. helleri Sherff\",\n",
      "    \"Collection Information\": \"Flora Hawaiianensis. Collected by C. N. Forbes on Oahu.\",\n",
      "    \"Location\": \"Punaluu, Koolau Mts.\",\n",
      "    \"Elevation\": \"Apl. 1-20\" + \"May 6 - 1914\",\n",
      "    \"Collector Number\": \"XV: 5/18\",\n",
      "    \"Barcode\": \"00427028\"\n",
      "  },\n",
      "  \"Institution Label\": {\n",
      "    \"Institution\": \"UNITED STATES NATIONAL MUSEUM\",\n",
      "    \"Specimen Number\": \"1627083\"\n",
      "  },\n",
      "  \"Imaging Label\": {\n",
      "    \"Image Number\": \"Image No. 00427028\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "Please note that there could be slight inaccuracies in transcription due to the handwriting and the quality of the image. The elevation appears to be a range given with a mixture of dates (April 1-20, May 6, 1914) which is transcribed as given.\n"
     ]
    }
   ],
   "source": [
    "# print(contents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We save the contents of each image in the form of txt files since there are also comments from the model regarding most of the transcriptions that could be useful while evaluating results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projectnb/sparkgrp/ml-herbarium-grp\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "for i in final_dict:\n",
    "    f = open(\"fall2023/gpt4v-results/\"+i.replace(\"jpg\", \"txt\"), \"w\")\n",
    "    f.writelines(final_dict[i])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To compare with results from Azure Vision, we use a subset of specimen from GBIF (low resolution since Azure vision does not accept the size of specimen in the TROCR eval set), the code is exactly the same as before, only this time on low resolution images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projectnb/sparkgrp/ml-herbarium-grp/fall2023\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from collections import defaultdict\n",
    "import base64\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# os.chdir(\"ml-herbarium-grp\")\n",
    "print(os.getcwd())\n",
    "json_results = defaultdict()\n",
    "too_big = []\n",
    "# OpenAI API Key\n",
    "api_key = key\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Path to your image\n",
    "folder_path = \"LLM_Specimens/\"\n",
    "image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith(('jpg', 'png'))]\n",
    "\n",
    "for img in image_files:\n",
    "    if os.stat(img).st_size > 19000000:\n",
    "        too_big.append(img)\n",
    "    else:\n",
    "    # Getting the base64 string\n",
    "        base64_image = encode_image(img)\n",
    "        \n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {api_key}\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": \"gpt-4-vision-preview\",\n",
    "            \"messages\": [\n",
    "              {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                  {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Extract all the text, both typed and handwritten from this image and display it in a JSON format\"\n",
    "                  },\n",
    "                  {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                      \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                  }\n",
    "                ]\n",
    "              }\n",
    "            ],\n",
    "            \"max_tokens\": 4096\n",
    "        }\n",
    "    \n",
    "        response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "        json_results[img] = response.json()\n",
    "    \n",
    "    # print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['LLM_Specimens/Text_Sample_3.png', 'LLM_Specimens/Mixed_Sample_5.png', 'LLM_Specimens/Text_Sample_11.png', 'LLM_Specimens/Text_Sample_17.png', 'LLM_Specimens/Text_Sample_5.png', 'LLM_Specimens/Mixed_Sample_4.png', 'LLM_Specimens/Mixed_Sample_1.png', 'LLM_Specimens/Text_Sample_1.png', 'LLM_Specimens/Text_Sample_20.png', 'LLM_Specimens/Text_Sample_14.png', 'LLM_Specimens/Text_Sample_19.png', 'LLM_Specimens/Text_Sample_16.png', 'LLM_Specimens/Mixed_Sample_9.png', 'LLM_Specimens/Text_Sample_13.png', 'LLM_Specimens/Text_Sample_12.png', 'LLM_Specimens/Mixed_Sample_6.png', 'LLM_Specimens/Mixed_Sample_2.png', 'LLM_Specimens/Text_Sample_18.png', 'LLM_Specimens/Text_Sample_15.png', 'LLM_Specimens/Mixed_Sample_3.png', 'LLM_Specimens/Text_Sample_7.png', 'LLM_Specimens/Text_Sample_9.png', 'LLM_Specimens/Text_Sample_10.png', 'LLM_Specimens/Text_Sample_6.png', 'LLM_Specimens/Text_Sample_2.png', 'LLM_Specimens/Mixed_Sample_8.png', 'LLM_Specimens/Text_Sample_8.png', 'LLM_Specimens/Mixed_Sample_7.png', 'LLM_Specimens/Mixed_Sample_10.png', 'LLM_Specimens/Text_Sample_4.png'])\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "copyjson = json_results.copy()\n",
    "print(json_results.keys())\n",
    "for l in list(json_results.keys()):\n",
    "    if \"choices\" not in json_results[l].keys():\n",
    "        del json_results[l]\n",
    "    else:\n",
    "        json_results[l] = json_results[l]['choices'][0]['message']['content']\n",
    "\n",
    "print(len(json_results))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Text_Sample_3.png', 'Mixed_Sample_5.png', 'Text_Sample_11.png', 'Text_Sample_17.png', 'Text_Sample_5.png', 'Mixed_Sample_4.png', 'Mixed_Sample_1.png', 'Text_Sample_1.png', 'Text_Sample_20.png', 'Text_Sample_14.png', 'Text_Sample_19.png', 'Text_Sample_16.png', 'Mixed_Sample_9.png', 'Text_Sample_13.png', 'Text_Sample_12.png', 'Mixed_Sample_6.png', 'Mixed_Sample_2.png', 'Text_Sample_18.png', 'Text_Sample_15.png', 'Mixed_Sample_3.png', 'Text_Sample_7.png', 'Text_Sample_9.png', 'Text_Sample_10.png', 'Text_Sample_6.png', 'Text_Sample_2.png', 'Mixed_Sample_8.png', 'Text_Sample_8.png', 'Mixed_Sample_7.png', 'Mixed_Sample_10.png', 'Text_Sample_4.png'])\n"
     ]
    }
   ],
   "source": [
    "paths = list(json_results.keys())\n",
    "folder_path = \"LLM_Specimens/\"\n",
    "img_names = [i.replace(folder_path, \"\") for i in paths]\n",
    "\n",
    "# print(img_names)\n",
    "final_dict = defaultdict()\n",
    "\n",
    "for img in img_names:\n",
    "    final_dict[img] = json_results[folder_path+img]\n",
    "\n",
    "print(final_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projectnb/sparkgrp/ml-herbarium-grp/fall2023\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "for i in final_dict:\n",
    "    f = open(\"gpt4v-results-gbif/\"+i.replace(\"png\", \"txt\"), \"w\")\n",
    "    f.writelines(final_dict[i])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation from the results : \n",
    "\n",
    "1. **TROCR Evaluation set** : The labels for each specimen extracted are different according to GPT4-V. Some specimen have labels that are not present in the image itself but GPT4 tries to allocate an appropriate label for the text that it extracts in the json format. This is interesting to note and we will be working on creating uniform labels for every specimen after discussion with the Clients and Professor. There are still inaccuracies in the extracted handwritten texts but we will be able to confirm the accuracy measure only after the subject matter experts go through the results folder. There are also additional comments provided by the model to more accurately assess the text extraction with caution warnings that certain phrases it extracts are from handwritten samples and may not be the best match. Overall promising results found for all the specimen. Folder with extracted text is named **gpt4v-results**\n",
    "\n",
    "2. **GBIF Evaluation set** : This smaller specimen set was formed to compare results obtained by GPT4-V and Azure Vision. Azure vision cannot handle images of higher size and resolution so we manually extracted specimen (with both plant+label and only label) from the GBIF database to run through both the LLMs. Observations were not as good for GPT4-V, since it could not understand the request we gave it for most of the specimen and was only able to extract text for a few of them. We are experimenting with this a bit more, since it could be an effect of the lower quality images used in this case or an API request issue from their end. Updates on the Azure Vision part are present in the Azure OCR notebook. Folder with extracted text is named **gpt4v-results-gbif**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "l=[]\n",
    "for i in final_dict:\n",
    "    if final_dict[i] == \"I'm sorry, but I cannot assist with that request.\" or final_dict[i] == \"I'm sorry, but I can't assist with that request.\":\n",
    "        c+=1\n",
    "        l.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following files could not be processed by GPT4-V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 ['Text_Sample_17.png', 'Mixed_Sample_4.png', 'Mixed_Sample_1.png', 'Text_Sample_16.png', 'Mixed_Sample_9.png', 'Text_Sample_15.png', 'Mixed_Sample_3.png', 'Text_Sample_4.png']\n"
     ]
    }
   ],
   "source": [
    "print(c, l)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
