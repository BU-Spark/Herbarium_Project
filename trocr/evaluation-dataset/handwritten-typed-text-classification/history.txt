 1/1:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# !pip install craft-text-detector
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

# from string_grouper import match_strings, match_most_similar
# !pip install pycountry
import pycountry
import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results
 1/2:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/colejh/goodfilescraft'
# Location to save all output files
save_dir = '/projectnb/sparkgrp/colejh/saved_results2/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc
# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
ALL_TAXON_FILE = '/usr3/graduate/colejh/corpus_taxon.txt'
 1/3:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft'
# Location to save all output files
save_dir = '/projectnb/sparkgrp/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc
# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
ALL_TAXON_FILE = '/projectnb/sparkgrp/kabilanm/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
 1/4:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/projectnb/sparkgrp/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc
# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
ALL_TAXON_FILE = '/projectnb/sparkgrp/kabilanm/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
 1/5:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# !pip install craft-text-detector
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

# from string_grouper import match_strings, match_most_similar
# !pip install pycountry
import pycountry
import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results
 1/6:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/projectnb/sparkgrp/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc
# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon.txt'
 1/7:
# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft,export_extra = False, text_threshold = .7,link_threshold = .4, crop_type="poly",low_text = .3,cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
 1/8:
# Deleting empty folders, which occurs if some of the images get no segementation from CRAFT
root = output_dir_craft
folders = list(os.walk(root))[1:]
deleted = []
for folder in folders:
    if not folder[2]:
        deleted.append(folder)
        os.rmdir(folder[0])
        
# Setting up the Tr-OCR model and processor
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten") 
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten").to(device)

# Use all available gpu's
model_gpu= nn.DataParallel(model,list(range(torch.cuda.device_count()))).to(device)

# Dataloader for working with gpu's
trainset = datasets.ImageFolder(output_dir_craft, transform = processor)
testloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=False)

# For matching words to image
filenames = [s.replace('_crops', '') for s in list(trainset.class_to_idx)]

# For matching the image name with the label name
word_log_dic = {k: v for k,v in enumerate(filenames)}
# For matching the image name with the transriptions
words_identified = {k: [] for v,k in enumerate(filenames)}
 1/9:
# Save filenames
with open(save_dir+'filenames.txt', 'w') as fp:
    for item in filenames:
        # write each item on a new line
        fp.write("%s\n" % item)
# Save word_log_dic 
with open(save_dir+'word_log_dic.json', 'w') as fp:
    json.dump(word_log_dic, fp)
# Save words_identified
with open(save_dir+'words_identified.json', 'w') as fp:
    json.dump(words_identified, fp)
1/10:
# Save filenames
with open(save_dir+'filenames.txt', 'w') as fp:
    for item in filenames:
        # write each item on a new line
        fp.write("%s\n" % item)
# Save word_log_dic 
with open(save_dir+'word_log_dic.json', 'w') as fp:
    json.dump(word_log_dic, fp)
# Save words_identified
with open(save_dir+'words_identified.json', 'w') as fp:
    json.dump(words_identified, fp)
1/11:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/usr4/ds549/kabilanm/saved_results'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc
# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon.txt'
1/12:
# Save filenames
with open(save_dir+'filenames.txt', 'w') as fp:
    for item in filenames:
        # write each item on a new line
        fp.write("%s\n" % item)
# Save word_log_dic 
with open(save_dir+'word_log_dic.json', 'w') as fp:
    json.dump(word_log_dic, fp)
# Save words_identified
with open(save_dir+'words_identified.json', 'w') as fp:
    json.dump(words_identified, fp)
1/13:
#Storing the outputs
results,confidence,labels = trocr.evaluate_craft_seg(model,processor, words_identified,word_log_dic,testloader,device)
#Saving all the outputs in dataframe
df = pd.DataFrame(list(zip(results,confidence,labels)),columns = ['Results','Confidence','Labels'])
df.to_pickle(save_dir+'full_results.pkl')
1/14:
# First part of final csv with results, confidence level from tr-ocr, and label
combined_df = trocr.combine_by_label(df)

# Adding the image path and all bounding boxes 

df_dictionary = pd.DataFrame(boxes.items(), columns=['Image_Path', 'Bounding_Boxes'])
combined_df = pd.concat([combined_df, df_dictionary], axis=1, join='inner')
display(combined_df.head())
1/15:
#Save intermediate file
combined_df.to_pickle(save_dir+'/test.pkl')
1/16:
# Creating a new column which contains all bigrams from the transcription, with an associated index for each bigram
bigram_df = combined_df.copy()

bigram_df['Bigrams'] = bigram_df['Transcription'].str.join(' ').str.split(' ')

bigram_df['Bigrams'] = bigram_df['Bigrams'].apply(lambda lst: [lst[i:i+2] for i in range(len(lst) - 1)]).apply(lambda sublists: [' '.join(sublist) for sublist in sublists])
bigram_df['Bigram_idx'] = bigram_df.apply(matching.bigram_indices, axis=1)

# Associating all biagrams with their respective image
bigram_idx = []
for i in range(len(bigram_df)):
    for j in range(len(bigram_df.loc[i, 'Bigrams'])):
        bigram_idx.append((i))
bigram_idx = pd.Series(bigram_idx)

# Getting the bigrams as individual strings
results = pd.Series(bigram_df['Bigrams'].explode().reset_index(drop=True))
1/17:
species = pd.Series(list(pd.read_pickle(ALL_SPECIES_FILE)))
genus = pd.Series(list(pd.read_pickle(ALL_GENUS_FILE)))
taxon = pd.read_csv(ALL_TAXON_FILE,delimiter = "\t", names=["Taxon"]).squeeze()

# All countries and subdivisions for matching 
countries = []
for country in list(pycountry.countries):
    countries.append(country)


subdivisions_dict = {}
subdivisions = []
for subdivision in pycountry.subdivisions:
    subdivisions.append(subdivision.name)
    subdivisions_dict[subdivision.name] = pycountry.countries.get(alpha_2 = subdivision.country_code).name
1/18:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/usr4/ds549/kabilanm/saved_results'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc
# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
1/19:
species = pd.Series(list(pd.read_pickle(ALL_SPECIES_FILE)))
genus = pd.Series(list(pd.read_pickle(ALL_GENUS_FILE)))
taxon = pd.read_csv(ALL_TAXON_FILE,delimiter = "\t", names=["Taxon"]).squeeze()

# All countries and subdivisions for matching 
countries = []
for country in list(pycountry.countries):
    countries.append(country)


subdivisions_dict = {}
subdivisions = []
for subdivision in pycountry.subdivisions:
    subdivisions.append(subdivision.name)
    subdivisions_dict[subdivision.name] = pycountry.countries.get(alpha_2 = subdivision.country_code).name
1/20:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = matching.pooled_match(results,bigram_idx,minimum_similarity =.01,Taxon = taxon,Species = species,Genus = genus,Countries = countries,Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
1/21:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/usr4/ds549/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc
# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
ALL_TAXON_FILE = '/usr4/ds549/kabilanm/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
1/22:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/usr4/ds549/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc
# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
# ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
ALL_TAXON_FILE = '/usr4/ds549/kabilanm/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
1/23:
species = pd.Series(list(pd.read_pickle(ALL_SPECIES_FILE)))
genus = pd.Series(list(pd.read_pickle(ALL_GENUS_FILE)))
taxon = pd.read_csv(ALL_TAXON_FILE,delimiter = "\t", names=["Taxon"]).squeeze()

# All countries and subdivisions for matching 
countries = []
for country in list(pycountry.countries):
    countries.append(country)


subdivisions_dict = {}
subdivisions = []
for subdivision in pycountry.subdivisions:
    subdivisions.append(subdivision.name)
    subdivisions_dict[subdivision.name] = pycountry.countries.get(alpha_2 = subdivision.country_code).name
1/24:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = matching.pooled_match(results,bigram_idx,minimum_similarity =.01,Taxon = taxon,Species = species,Genus = genus,Countries = countries,Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
1/25: species.head
1/26: species.head()
1/27: genus.head()
1/28: taxon.head()
1/29: countries[0:5]
1/30:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = matching.pooled_match(results,bigram_idx,minimum_similarity =.01,Taxon = taxon,Species = species,Genus = genus,Countries = countries,Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
1/31:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = matching.pooled_match(results,bigram_idx,minimum_similarity =.01,Taxon = taxon,Species = species,Genus = genus,Countries = countries,Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
1/32:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = matching.pooled_match(results,bigram_idx,minimum_similarity =.01,Taxon = taxon,Species = species,Genus = genus,Countries = countries,Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
1/33:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# !pip install craft-text-detector
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

# from string_grouper import match_strings, match_most_similar
# !pip install pycountry
import pycountry
import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results
1/34:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# !pip install craft-text-detector
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

# from string_grouper import match_strings, match_most_similar
# !pip install pycountry
import pycountry
import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results
1/35:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = matching.pooled_match(results,bigram_idx,minimum_similarity =.01,Taxon = taxon,Species = species,Genus = genus,Countries = countries,Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
1/36:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = matching.pooled_match(results,bigram_idx,minimum_similarity =.01,Taxon = taxon,Species = species,Genus = genus,Countries = countries,Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
1/37:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = matching.pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
1/38:
species = pd.Series(list(pd.read_pickle(ALL_SPECIES_FILE)))
genus = pd.Series(list(pd.read_pickle(ALL_GENUS_FILE)))
taxon = pd.read_csv(ALL_TAXON_FILE,delimiter = "\t", names=["Taxon"]).squeeze()

# All countries and subdivisions for matching 
countries = []
for country in list(pycountry.countries):
    countries.append(country)


subdivisions_dict = {}
subdivisions = []
for subdivision in pycountry.subdivisions:
    subdivisions.append(subdivision.name)
    subdivisions_dict[subdivision.name] = pycountry.countries.get(alpha_2 = subdivision.country_code).name
1/39:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = matching.pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
1/40:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = matching.pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
1/41:
# Creating a new column which contains all bigrams from the transcription, with an associated index for each bigram
bigram_df = combined_df.copy()

bigram_df['Bigrams'] = bigram_df['Transcription'].str.join(' ').str.split(' ')

bigram_df['Bigrams'] = bigram_df['Bigrams'].apply(lambda lst: [lst[i:i+2] for i in range(len(lst) - 1)]).apply(lambda sublists: [' '.join(sublist) for sublist in sublists])
bigram_df['Bigram_idx'] = bigram_df.apply(matching.bigram_indices, axis=1)

# Associating all biagrams with their respective image
bigram_idx = []
for i in range(len(bigram_df)):
    for j in range(len(bigram_df.loc[i, 'Bigrams'])):
        bigram_idx.append((i))
bigram_idx = pd.Series(bigram_idx)

# Getting the bigrams as individual strings
results = pd.Series(bigram_df['Bigrams'].explode().reset_index(drop=True))
1/42:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = matching.pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
1/43: species.columns
1/44: species
1/45: species.head()
1/46: results.head()
1/47: bigram_idx.head()
1/48:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = matching.pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
1/49:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(corpus_list)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
1/50:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
1/51:
# save all_matches pickle
with open(save_dir + 'all_matches.pkl', 'wb') as f:
    pickle.dump(all_matches, f)
 2/1:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# !pip install craft-text-detector
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

# from string_grouper import match_strings, match_most_similar
# !pip install pycountry
import pycountry
import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results
 2/2:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/usr4/ds549/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc
# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
# ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
ALL_TAXON_FILE = '/usr4/ds549/kabilanm/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
 2/3:
# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft,export_extra = False, text_threshold = .7,link_threshold = .4, crop_type="poly",low_text = .3,cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
 2/4:
# Deleting empty folders, which occurs if some of the images get no segementation from CRAFT
root = output_dir_craft
folders = list(os.walk(root))[1:]
deleted = []
for folder in folders:
    if not folder[2]:
        deleted.append(folder)
        os.rmdir(folder[0])
        
# Setting up the Tr-OCR model and processor
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten") 
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten").to(device)

# Use all available gpu's
model_gpu= nn.DataParallel(model,list(range(torch.cuda.device_count()))).to(device)

# Dataloader for working with gpu's
trainset = datasets.ImageFolder(output_dir_craft, transform = processor)
testloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=False)

# For matching words to image
filenames = [s.replace('_crops', '') for s in list(trainset.class_to_idx)]

# For matching the image name with the label name
word_log_dic = {k: v for k,v in enumerate(filenames)}
# For matching the image name with the transriptions
words_identified = {k: [] for v,k in enumerate(filenames)}
 2/5:
# Save filenames
with open(save_dir+'filenames.txt', 'w') as fp:
    for item in filenames:
        # write each item on a new line
        fp.write("%s\n" % item)
# Save word_log_dic 
with open(save_dir+'word_log_dic.json', 'w') as fp:
    json.dump(word_log_dic, fp)
# Save words_identified
with open(save_dir+'words_identified.json', 'w') as fp:
    json.dump(words_identified, fp)
 2/6:
#Storing the outputs
results,confidence,labels = trocr.evaluate_craft_seg(model,processor, words_identified,word_log_dic,testloader,device)
#Saving all the outputs in dataframe
df = pd.DataFrame(list(zip(results,confidence,labels)),columns = ['Results','Confidence','Labels'])
df.to_pickle(save_dir+'full_results.pkl')
 2/7:
# First part of final csv with results, confidence level from tr-ocr, and label
combined_df = trocr.combine_by_label(df)

# Adding the image path and all bounding boxes 

df_dictionary = pd.DataFrame(boxes.items(), columns=['Image_Path', 'Bounding_Boxes'])
combined_df = pd.concat([combined_df, df_dictionary], axis=1, join='inner')
display(combined_df.head())
 2/8:
#Save intermediate file
combined_df.to_pickle(save_dir+'/test.pkl')
 2/9:
# Creating a new column which contains all bigrams from the transcription, with an associated index for each bigram
bigram_df = combined_df.copy()

bigram_df['Bigrams'] = bigram_df['Transcription'].str.join(' ').str.split(' ')

bigram_df['Bigrams'] = bigram_df['Bigrams'].apply(lambda lst: [lst[i:i+2] for i in range(len(lst) - 1)]).apply(lambda sublists: [' '.join(sublist) for sublist in sublists])
bigram_df['Bigram_idx'] = bigram_df.apply(matching.bigram_indices, axis=1)

# Associating all biagrams with their respective image
bigram_idx = []
for i in range(len(bigram_df)):
    for j in range(len(bigram_df.loc[i, 'Bigrams'])):
        bigram_idx.append((i))
bigram_idx = pd.Series(bigram_idx)

# Getting the bigrams as individual strings
results = pd.Series(bigram_df['Bigrams'].explode().reset_index(drop=True))
2/10:
species = pd.Series(list(pd.read_pickle(ALL_SPECIES_FILE)))
genus = pd.Series(list(pd.read_pickle(ALL_GENUS_FILE)))
taxon = pd.read_csv(ALL_TAXON_FILE,delimiter = "\t", names=["Taxon"]).squeeze()

# All countries and subdivisions for matching 
countries = []
for country in list(pycountry.countries):
    countries.append(country)


subdivisions_dict = {}
subdivisions = []
for subdivision in pycountry.subdivisions:
    subdivisions.append(subdivision.name)
    subdivisions_dict[subdivision.name] = pycountry.countries.get(alpha_2 = subdivision.country_code).name
2/11: countries[0:5]
2/12: species.head()
2/13: genus.head()
2/14: taxon.head()
2/15: results.head()
2/16: bigram_idx.head()
2/17:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(corpus_list)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/18:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/19:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(corpus_list)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/20:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/21:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(corpus_list)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/22:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/23:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(corpus_list)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/24:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/25:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(type(corpus_list))

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/26:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(type(corpus_list))

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/27:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/28:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(type(corpus_list))
    print(type(corpus_list[0]))

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/29:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(type(corpus_list))
    print(type(corpus_list[0]))

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/30:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/31:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(type(corpus_list))
    print(type(corpus_list[0]))
    
    for _ in range(len(corpus_list)):
        print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/32:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/33:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(type(corpus_list))
    print(type(corpus_list[0]))
    
    for _ in range(len(corpus_list)):
        if isinstance(_, pd.Series) print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/34:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/35:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(type(corpus_list))
    print(type(corpus_list[0]))
    
    for _ in range(len(corpus_list)):
        if isinstance(_, pd.Series): 
            print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/36:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(type(corpus_list))
    print(type(corpus_list[0]))
    
    for _ in range(len(corpus_list)):
        if isinstance(_, pd.Series): 
            print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/37:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/38:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(type(corpus_list))
    print(type(corpus_list[0]))
    
    for _ in range(len(corpus_list)):
#         print(type())
        if isinstance(_, pandas.core.series.Series): 
            print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/39:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/40:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(type(corpus_list))
    print(type(corpus_list[0]))
    
    for _ in range(len(corpus_list)):
#         print(type())
        if isinstance(_, pd.core.series.Series): 
            print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/41:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(type(corpus_list))
    print(type(corpus_list[0]))
    
    for _ in range(len(corpus_list)):
#         print(type())
        if isinstance(_, pd.core.series.Series): 
            print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/42:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/43:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(type(corpus_list))
    print(type(corpus_list[0]))
    
    for _ in range(len(corpus_list)):
#         print(type())
        if isinstance(_, pd.core.series.Series): 
            print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/44:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
       
    print(type(corpus_list))
    print(type(corpus_list[0]))
    
    for _ in range(len(corpus_list)):
        print(type(_))
        if isinstance(_, pd.core.series.Series): 
            print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/45:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/46:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    print(corpus_list[1].name)
    
#     for _ in range(len(corpus_list)):
#         print(type(_))
#         if isinstance(_, pd.core.series.Series): 
#             print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/47:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/48:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    print(corpus_list[1].name)
    
    for _ in range(len(corpus_list)):
        print(corpus_list, end="\n\n")
#         if isinstance(_, pd.core.series.Series): 
#             print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/49:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/50:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    print(corpus_list[1].name)
    
    for _ in range(len(corpus_list)):
        print(_, end="\n\n")
#         if isinstance(_, pd.core.series.Series): 
#             print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/51:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/52:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    print(corpus_list[1].name)
    
    for _ in range(corpus_list):
        print(_, end="\n\n")
#         if isinstance(_, pd.core.series.Series): 
#             print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/53:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/54:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    print(corpus_list[1].name)
    
    for _ in range(len(corpus_list)):
        print(corpus_list[_], end="\n\n")
#         if isinstance(_, pd.core.series.Series): 
#             print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/55:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/56:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    print(corpus_list[1].name)
    
    print(corpus_list[3])
    
#     for _ in range(len(corpus_list)):
#         print(corpus_list[_], end="\n\n")
#         if isinstance(_, pd.core.series.Series): 
#             print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/57:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/58:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    print(corpus_list[1].name)
    
    print(corpus_list[3][0])
    
#     for _ in range(len(corpus_list)):
#         print(corpus_list[_], end="\n\n")
#         if isinstance(_, pd.core.series.Series): 
#             print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
    
    pool.map(func,corpus_list)
   
    result_dic = {}
    for i,result in enumerate(pool.map(func,corpus_list)):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/59:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/60:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    print(corpus_list[1].name)
    
    print(corpus_list[3][0])
    
#     for _ in range(len(corpus_list)):
#         print(corpus_list[_], end="\n\n")
#         if isinstance(_, pd.core.series.Series): 
#             print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

#     func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
#     pool = multiprocessing.Pool()
    
#     pool.map(func,corpus_list)

    results = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)
    
    result_dic = {}
    for i,result in enumerate(results):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
    
    
#     result_dic = {}
#     for i,result in enumerate(pool.map(func,corpus_list)):
#         print('Matching against',corpus_name[i])
#         result.columns.values[1] = corpus_name[i]+'_Corpus'
#         result.columns.values[3] = "Predictions"
#         result = result.drop('left_index', axis=1)
#         result = highest_score_per_image(result,labels)
#         result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/61:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/62:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    print(corpus_list[1].name)
    
    print(corpus_list[3][0])
    
#     for _ in range(len(corpus_list)):
#         print(corpus_list[_], end="\n\n")
#         if isinstance(_, pd.core.series.Series): 
#             print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

#     func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
#     pool = multiprocessing.Pool()
    
#     pool.map(func,corpus_list)

    results = match_strings(corpus_list,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)
    
    result_dic = {}
    for i,result in enumerate(results):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
    
    
#     result_dic = {}
#     for i,result in enumerate(pool.map(func,corpus_list)):
#         print('Matching against',corpus_name[i])
#         result.columns.values[1] = corpus_name[i]+'_Corpus'
#         result.columns.values[3] = "Predictions"
#         result = result.drop('left_index', axis=1)
#         result = highest_score_per_image(result,labels)
#         result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/63:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/64:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    print(corpus_list[1].name)
    
    print(corpus_list[3][0])
    
#     for _ in range(len(corpus_list)):
#         print(corpus_list[_], end="\n\n")
#         if isinstance(_, pd.core.series.Series): 
#             print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

#     func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
#     pool = multiprocessing.Pool()
    
#     pool.map(func,corpus_list)

    results = match(corpus_list)
    
    result_dic = {}
    for i,result in enumerate(results):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
    
    
#     result_dic = {}
#     for i,result in enumerate(pool.map(func,corpus_list)):
#         print('Matching against',corpus_name[i])
#         result.columns.values[1] = corpus_name[i]+'_Corpus'
#         result.columns.values[3] = "Predictions"
#         result = result.drop('left_index', axis=1)
#         result = highest_score_per_image(result,labels)
#         result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/65:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/66:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    print(corpus_list[1].name)
    
    print(corpus_list[3][0])
    
#     for _ in range(len(corpus_list)):
#         print(corpus_list[_], end="\n\n")
#         if isinstance(_, pd.core.series.Series): 
#             print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

#     func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
#     pool = multiprocessing.Pool()
    
#     pool.map(func,corpus_list)

    results = match(corpus_list, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    
    result_dic = {}
    for i,result in enumerate(results):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
    
    
#     result_dic = {}
#     for i,result in enumerate(pool.map(func,corpus_list)):
#         print('Matching against',corpus_name[i])
#         result.columns.values[1] = corpus_name[i]+'_Corpus'
#         result.columns.values[3] = "Predictions"
#         result = result.drop('left_index', axis=1)
#         result = highest_score_per_image(result,labels)
#         result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/67:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/68:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    print(corpus_list[1].name)
    
    print(corpus_list[3][0])
    
#     for _ in range(len(corpus_list)):
#         print(corpus_list[_], end="\n\n")
#         if isinstance(_, pd.core.series.Series): 
#             print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

#     func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
#     pool = multiprocessing.Pool()
    
#     pool.map(func,corpus_list)

    df = pd.Series( (v for v in corpus_list) )

    results = match(main, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    
    result_dic = {}
    for i,result in enumerate(results):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
    
    
#     result_dic = {}
#     for i,result in enumerate(pool.map(func,corpus_list)):
#         print('Matching against',corpus_name[i])
#         result.columns.values[1] = corpus_name[i]+'_Corpus'
#         result.columns.values[3] = "Predictions"
#         result = result.drop('left_index', axis=1)
#         result = highest_score_per_image(result,labels)
#         result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/69:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/70:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    print(corpus_list[1].name)
    
    print(corpus_list[3][0])
    
#     for _ in range(len(corpus_list)):
#         print(corpus_list[_], end="\n\n")
#         if isinstance(_, pd.core.series.Series): 
#             print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

#     func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
#     pool = multiprocessing.Pool()
    
#     pool.map(func,corpus_list)

    main = pd.Series( (v for v in corpus_list) )

    results = match(main, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    
    result_dic = {}
    for i,result in enumerate(results):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
    
    
#     result_dic = {}
#     for i,result in enumerate(pool.map(func,corpus_list)):
#         print('Matching against',corpus_name[i])
#         result.columns.values[1] = corpus_name[i]+'_Corpus'
#         result.columns.values[3] = "Predictions"
#         result = result.drop('left_index', axis=1)
#         result = highest_score_per_image(result,labels)
#         result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/71:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/72:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    print(corpus_list[1].name)
    
    print(corpus_list[3][0])
    
#     for _ in range(len(corpus_list)):
#         print(corpus_list[_], end="\n\n")
#         if isinstance(_, pd.core.series.Series): 
#             print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

#     func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
#     pool = multiprocessing.Pool()
    
#     pool.map(func,corpus_list)

    main = pd.Series( (v for v in corpus_list) )
    
    print(type(main))

    results = match(main, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    
    result_dic = {}
    for i,result in enumerate(results):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
    
    
#     result_dic = {}
#     for i,result in enumerate(pool.map(func,corpus_list)):
#         print('Matching against',corpus_name[i])
#         result.columns.values[1] = corpus_name[i]+'_Corpus'
#         result.columns.values[3] = "Predictions"
#         result = result.drop('left_index', axis=1)
#         result = highest_score_per_image(result,labels)
#         result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/73:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/74:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    print(corpus_list[1].name)
    
    print(corpus_list[3][0])
    
#     for _ in range(len(corpus_list)):
#         print(corpus_list[_], end="\n\n")
#         if isinstance(_, pd.core.series.Series): 
#             print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

#     func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
#     pool = multiprocessing.Pool()
    
#     pool.map(func,corpus_list)

    main = pd.Series( (v for v in corpus_list) )
    
    print(type(main))
    print(main.head())

    results = match(main, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    
    result_dic = {}
    for i,result in enumerate(results):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
    
    
#     result_dic = {}
#     for i,result in enumerate(pool.map(func,corpus_list)):
#         print('Matching against',corpus_name[i])
#         result.columns.values[1] = corpus_name[i]+'_Corpus'
#         result.columns.values[3] = "Predictions"
#         result = result.drop('left_index', axis=1)
#         result = highest_score_per_image(result,labels)
#         result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/75:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
2/76:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# !pip install craft-text-detector
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

# from string_grouper import match_strings, match_most_similar
# !pip install pycountry
import pycountry
import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results
2/77:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/usr4/ds549/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc
# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
# ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
ALL_TAXON_FILE = '/usr4/ds549/kabilanm/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
2/78:
# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft,export_extra = False, text_threshold = .7,link_threshold = .4, crop_type="poly",low_text = .3,cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
2/79:
# Deleting empty folders, which occurs if some of the images get no segementation from CRAFT
root = output_dir_craft
folders = list(os.walk(root))[1:]
deleted = []
for folder in folders:
    if not folder[2]:
        deleted.append(folder)
        os.rmdir(folder[0])
        
# Setting up the Tr-OCR model and processor
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten") 
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten").to(device)

# Use all available gpu's
model_gpu= nn.DataParallel(model,list(range(torch.cuda.device_count()))).to(device)

# Dataloader for working with gpu's
trainset = datasets.ImageFolder(output_dir_craft, transform = processor)
testloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=False)

# For matching words to image
filenames = [s.replace('_crops', '') for s in list(trainset.class_to_idx)]

# For matching the image name with the label name
word_log_dic = {k: v for k,v in enumerate(filenames)}
# For matching the image name with the transriptions
words_identified = {k: [] for v,k in enumerate(filenames)}
2/80:
# Save filenames
with open(save_dir+'filenames.txt', 'w') as fp:
    for item in filenames:
        # write each item on a new line
        fp.write("%s\n" % item)
# Save word_log_dic 
with open(save_dir+'word_log_dic.json', 'w') as fp:
    json.dump(word_log_dic, fp)
# Save words_identified
with open(save_dir+'words_identified.json', 'w') as fp:
    json.dump(words_identified, fp)
2/81:
#Storing the outputs
results,confidence,labels = trocr.evaluate_craft_seg(model,processor, words_identified,word_log_dic,testloader,device)
#Saving all the outputs in dataframe
df = pd.DataFrame(list(zip(results,confidence,labels)),columns = ['Results','Confidence','Labels'])
df.to_pickle(save_dir+'full_results.pkl')
2/82:
# First part of final csv with results, confidence level from tr-ocr, and label
combined_df = trocr.combine_by_label(df)

# Adding the image path and all bounding boxes 

df_dictionary = pd.DataFrame(boxes.items(), columns=['Image_Path', 'Bounding_Boxes'])
combined_df = pd.concat([combined_df, df_dictionary], axis=1, join='inner')
display(combined_df.head())
2/83:
#Save intermediate file
combined_df.to_pickle(save_dir+'/test.pkl')
2/84:
# Creating a new column which contains all bigrams from the transcription, with an associated index for each bigram
bigram_df = combined_df.copy()

bigram_df['Bigrams'] = bigram_df['Transcription'].str.join(' ').str.split(' ')

bigram_df['Bigrams'] = bigram_df['Bigrams'].apply(lambda lst: [lst[i:i+2] for i in range(len(lst) - 1)]).apply(lambda sublists: [' '.join(sublist) for sublist in sublists])
bigram_df['Bigram_idx'] = bigram_df.apply(matching.bigram_indices, axis=1)

# Associating all biagrams with their respective image
bigram_idx = []
for i in range(len(bigram_df)):
    for j in range(len(bigram_df.loc[i, 'Bigrams'])):
        bigram_idx.append((i))
bigram_idx = pd.Series(bigram_idx)

# Getting the bigrams as individual strings
results = pd.Series(bigram_df['Bigrams'].explode().reset_index(drop=True))
2/85:
species = pd.Series(list(pd.read_pickle(ALL_SPECIES_FILE)))
genus = pd.Series(list(pd.read_pickle(ALL_GENUS_FILE)))
taxon = pd.read_csv(ALL_TAXON_FILE,delimiter = "\t", names=["Taxon"]).squeeze()

# All countries and subdivisions for matching 
countries = []
for country in list(pycountry.countries):
    countries.append(country)


subdivisions_dict = {}
subdivisions = []
for subdivision in pycountry.subdivisions:
    subdivisions.append(subdivision.name)
    subdivisions_dict[subdivision.name] = pycountry.countries.get(alpha_2 = subdivision.country_code).name
2/86: countries[0:5]
2/87: species.head()
2/88: genus.head()
2/89: taxon.head()
2/90: results.head()
2/91: bigram_idx.head()
2/92:
import multiprocessing
import pandas as pd
from functools import partial
from itertools import repeat
from string_grouper import match_strings

# All matching functions used in the project

def matches_above_x(df,x = .75):
    # Takes in a dataframe with similarity scores and returns a dataframe with only matches above a certain similarity
    return df.loc[df['similarity']>=x]

def match(main,comparison_file,minimum_similarity=.001):
    """
    Given a main file containing strings and a comparison file to match against the main file,
    this function returns a DataFrame of matches based on their cosine similarity.
    
    Parameters:
    main (list or pandas.Series): A list or pandas Series containing the strings to match against.
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the main file.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    
    Returns:
    pandas.DataFrame: A DataFrame containing the matches and their similarity to the strings in the main file.
    """

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)
        
    matches = match_strings(main,comparison_file,n_blocks = 'guess',min_similarity = minimum_similarity,max_n_matches = 1)

    return matches

def highest_score_per_image(df,labels):
    """
    Given a DataFrame with similarity scores and a list of labels, this function returns
    a DataFrame with only the highest score per image.
    
    Parameters:
    df (pandas.DataFrame): A DataFrame containing similarity scores, with columns 'right_index' and 'similarity'.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    
    Returns:
    pandas.DataFrame: A DataFrame containing the highest similarity scores for each unique 'labels' value.
    """

    # Getting the highest score for each individual image 
    exclude = ['Don Williams','Don William']
    index_to_labels = df.copy()
    for a in index_to_labels.right_index.unique():
        index_to_labels.loc[index_to_labels['right_index'] == a, 'right_index'] = labels[a]
    #drop all rows whose column value in Predictions is in exclude
    index_to_labels = index_to_labels[~index_to_labels['Predictions'].isin(exclude)]

    unique_labels = index_to_labels.loc[index_to_labels.groupby('right_index')['similarity'].idxmax()]


    return unique_labels

def pooled_match(comparison_file,labels, minimum_similarity = .001,**kwargs):
    """
    Given a comparison file containing strings and any number of additional files to match against the comparison file,
    this function returns a dictionary with keys the same name as the input files and values as the DataFrame with
    matching information.
    
    Parameters:
    comparison_file (list or pandas.Series): A list or pandas Series containing the strings to compare to the other input files.
    labels (list): A list of labels corresponding to the 'right_index' values in the DataFrame.
    minimum_similarity (float): The minimum similarity confidence level required for a match (default is 0.001).
    **kwargs: Any number of additional files containing strings to match against the comparison file, with the
              file names as keyword arguments and the strings as values.
    
    Returns:
    dict: A dictionary with keys the same name as the input files and values as the DataFrame with matching information.
    """
    print('Starting matching process...')
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    print(corpus_list[1].name)
    
    print(corpus_list[3][0])
    
#     for _ in range(len(corpus_list)):
#         print(corpus_list[_], end="\n\n")
#         if isinstance(_, pd.core.series.Series): 
#             print(_.name)

    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

#     func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
#     pool = multiprocessing.Pool()
    
#     pool.map(func,corpus_list)

    main = pd.Series( (v for v in corpus_list) )
    
    print(type(main))
    print(main.head())

    results = match(main, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    
    result_dic = {}
    for i,result in enumerate(results):
        print('Matching against',corpus_name[i])
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
    
    
#     result_dic = {}
#     for i,result in enumerate(pool.map(func,corpus_list)):
#         print('Matching against',corpus_name[i])
#         result.columns.values[1] = corpus_name[i]+'_Corpus'
#         result.columns.values[3] = "Predictions"
#         result = result.drop('left_index', axis=1)
#         result = highest_score_per_image(result,labels)
#         result_dic[corpus_name[i]] = result

   
    return result_dic

def pooled_match2(comparison_file,labels, minimum_similarity = .001,**kwargs):
    # Take in any number of files containing strings to match against and return a dictionary
    # with keys the same name as input and values as the dataframe with matching information
    # Same as pooled_match but uses a different method to multiprocess
    corpus_list = []
    corpus_name = []
    
    for k,v in kwargs.items():
        # Convert to series (string-grouper requires this type), will work if input is list, array, or series
        if not isinstance(v, pd.Series):
            v = pd.Series(v)
        corpus_list.append(v)
        corpus_name.append(k)
    
    if not isinstance(comparison_file, pd.Series):
        comparison_file = pd.Series(comparison_file)

    # func = partial(match, comparison_file = comparison_file,  minimum_similarity = minimum_similarity)
    pool = multiprocessing.Pool()
   
    result_dic = {}
    for i,result in enumerate(pool.starmap(match, zip(corpus_list, repeat(comparison_file),repeat(minimum_similarity)))):
        result.columns.values[1] = corpus_name[i]+'_Corpus'
        result.columns.values[3] = "Predictions"
        result = result.drop('left_index', axis=1)
        result = highest_score_per_image(result,labels)
        result_dic[corpus_name[i]] = result
   
    return result_dic


 # Function to add in the bigram index as a column on the dataframe
def bigram_indices(row):
    """
    Given a row of data with columns 'Transcription' and 'Bigrams', this function
    returns a list of tuples containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Parameters:
    row (pandas.Series): A row of data with columns 'Transcription' and 'Bigrams'.
    
    Returns:
    list: A list of tuples, each containing the indices of the words in the 'Transcription'
    column that correspond to the bigrams in the 'Bigrams' column.
    
    Example:
    row = pd.Series({'Transcription': ['Herbier Museum.', 'Paris Cryptogamie.', 'PC0693452', '0.'],
    'Bigrams': ['Herbier Museum.','Museum. Paris','Paris Cryptogamie.','Cryptogamie. PC0693452','PC0693452 0.']})
    bigram_indices(row)
    [(0, 0), (0, 1), (1, 1), (1, 2), (2, 3)]
    """
    list_of_transcriptions =  [list(x.split(' ')) for x in row['Transcription']]

    first_idx = None
    second_idx = None
    bigram_idx = []
    for word in row['Bigrams']:
        strings = word.split(' ')
        for i,x in enumerate(list_of_transcriptions):
            if strings[0] in x:
                first_idx = i
            if strings[1] in x:
                second_idx = i
            if first_idx != None and second_idx != None:
                bigram_idx.append((first_idx, second_idx))
                first_idx = None
                second_idx = None
                break
        
    return bigram_idx
2/93:
#running the matching against all files
minimum_similarity = .01 #arbitrary, set here to get every prediction, likely want to set this quite a bit higher
start = time.time()
all_matches = pooled_match(results, bigram_idx, minimum_similarity =.01, Taxon = taxon, Species = species, Genus = genus, Countries = countries, Subdivisions = subdivisions)
end = time.time()
print('Time to match all strings: ',end-start)
 3/1:
from PIL import Image
 
# creating a object
im = Image.open("input_images/https%3A%2F%2Ffm-digital-assets.fieldmuseum.org%2F1118%2F517%2FC0614459F.jpg")
 
im.show()
 3/2:
from PIL import Image
 
# creating a object
im = Image.open("./input_images/https%3A%2F%2Ffm-digital-assets.fieldmuseum.org%2F1118%2F517%2FC0614459F.jpg")
 
im.show()
 3/3: ! pwd
 3/4:
from PIL import Image

! cd /usr4/ds549/kabilanm/TextFuseNet
# creating a object
im = Image.open("./input_images/https%3A%2F%2Ffm-digital-assets.fieldmuseum.org%2F1118%2F517%2FC0614459F.jpg")
 
im.show()
 3/5:
! cd /usr4/ds549/kabilanm/TextFuseNet
! pwd
 4/1:
from PIL import Image

# creating a object
im = Image.open("./input_images/https%3A%2F%2Ffm-digital-assets.fieldmuseum.org%2F1118%2F517%2FC0614459F.jpg")
 
im.show()
 4/2:
from PIL import Image

# creating a object
im = Image.open("input_images/https%3A%2F%2Ffm-digital-assets.fieldmuseum.org%2F1118%2F517%2FC0614459F.jpg")
 
im.show()
 4/3:
from PIL import Image

# creating a object
im = Image.open("input_images/https%3A%2F%2Ffm-digital-assets.fieldmuseum.org%2F1118%2F517%2FC0614459F.jpg")
 
im.show()
 4/4:
from PIL import Image

# creating a object
im = Image.open("input_images/https%3A%2F%2Ffm-digital-assets.fieldmuseum.org%2F1118%2F517%2FC0614459F.jpg")
 
im.show()
 4/5:
from PIL import Image

# creating a object
im = Image.open("input_images/https%3A%2F%2Ffm-digital-assets.fieldmuseum.org%2F1118%2F517%2FC0614459F.jpg")
 
im.show()
 4/6: ! pwd
 4/7: ! pwd
 4/8: ! ls
 4/9:
from PIL import Image

# creating a object
im = Image.open("TextFuseNet/input_images/https%3A%2F%2Ffm-digital-assets.fieldmuseum.org%2F1118%2F517%2FC0614459F.jpg")
 
im.show()
4/10:
from PIL import Image

# creating a object
im = Image.open("TextFuseNet/input_images/C0614459F.jpg")
 
im.show()
4/11:
from PIL import Image

# creating a object
im = Image.open("TextFuseNet/input_images/C0614459F.jpg")
 
im.show()
 5/1: ! pip install -r requirements.txt
 5/2: ! pip install -r requirements.txt
 5/3: ! pip install -r requirements.txt
 5/4: ! pip install -r requirements.txt
 5/5: ! pip install -r requirements.txt
 5/6:
# mindspore==1.5.0
# mindspore_ascend==1.3.0

! python --version
 6/1: ! pip install -r requirements.txt
 6/2:
# mindspore==1.5.0
# mindspore_ascend==1.3.0

! python --version
 6/3: ! pip install -r requirements.txt
 6/4:
# mindspore==1.5.0
# mindspore_ascend==1.3.0

! python --version
 6/5: ! pip install gdown
 6/6: ! pip install gdown
 6/7: ! gdown https://drive.google.com/file/d/1bC68CzsSVTusZVvOkk7imSZSbgD1MqK2/view?usp=sharing
 6/8: ! gdown https://drive.google.com/uc?id=1bC68CzsSVTusZVvOkk7imSZSbgD1MqK2
 8/2:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# !pip install craft-text-detector
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

# from string_grouper import match_strings, match_most_similar
# !pip install pycountry
import pycountry
import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results
 9/1:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# !pip install craft-text-detector
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

# from string_grouper import match_strings, match_most_similar
# !pip install pycountry
import pycountry
import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results
 9/2:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/usr4/ds549/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc
# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
# ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
ALL_TAXON_FILE = '/usr4/ds549/kabilanm/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
 9/3:
# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft,export_extra = False, text_threshold = .7,link_threshold = .4, crop_type="poly",low_text = .3,cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
 9/4:
# Deleting empty folders, which occurs if some of the images get no segementation from CRAFT
root = output_dir_craft
folders = list(os.walk(root))[1:]
deleted = []
for folder in folders:
    if not folder[2]:
        deleted.append(folder)
        os.rmdir(folder[0])
        
# Setting up the Tr-OCR model and processor
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten") 
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten").to(device)

# Use all available gpu's
model_gpu= nn.DataParallel(model,list(range(torch.cuda.device_count()))).to(device)

# Dataloader for working with gpu's
trainset = datasets.ImageFolder(output_dir_craft, transform = processor)
testloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=False)

# For matching words to image
filenames = [s.replace('_crops', '') for s in list(trainset.class_to_idx)]

# For matching the image name with the label name
word_log_dic = {k: v for k,v in enumerate(filenames)}
# For matching the image name with the transriptions
words_identified = {k: [] for v,k in enumerate(filenames)}
 9/5:
# Save filenames
with open(save_dir+'filenames.txt', 'w') as fp:
    for item in filenames:
        # write each item on a new line
        fp.write("%s\n" % item)
# Save word_log_dic 
with open(save_dir+'word_log_dic.json', 'w') as fp:
    json.dump(word_log_dic, fp)
# Save words_identified
with open(save_dir+'words_identified.json', 'w') as fp:
    json.dump(words_identified, fp)
 9/6:
#Storing the outputs
results,confidence,labels = trocr.evaluate_craft_seg(model,processor, words_identified,word_log_dic,testloader,device)
#Saving all the outputs in dataframe
df = pd.DataFrame(list(zip(results,confidence,labels)),columns = ['Results','Confidence','Labels'])
df.to_pickle(save_dir+'full_results.pkl')
 9/7:
# First part of final csv with results, confidence level from tr-ocr, and label
combined_df = trocr.combine_by_label(df)

# Adding the image path and all bounding boxes 

df_dictionary = pd.DataFrame(boxes.items(), columns=['Image_Path', 'Bounding_Boxes'])
combined_df = pd.concat([combined_df, df_dictionary], axis=1, join='inner')
display(combined_df.head())
 9/8:
#Save intermediate file
combined_df.to_pickle(save_dir+'/test.pkl')
 9/9:
# Creating a new column which contains all bigrams from the transcription, with an associated index for each bigram
bigram_df = combined_df.copy()

bigram_df['Bigrams'] = bigram_df['Transcription'].str.join(' ').str.split(' ')

bigram_df['Bigrams'] = bigram_df['Bigrams'].apply(lambda lst: [lst[i:i+2] for i in range(len(lst) - 1)]).apply(lambda sublists: [' '.join(sublist) for sublist in sublists])
bigram_df['Bigram_idx'] = bigram_df.apply(matching.bigram_indices, axis=1)

# Associating all biagrams with their respective image
bigram_idx = []
for i in range(len(bigram_df)):
    for j in range(len(bigram_df.loc[i, 'Bigrams'])):
        bigram_idx.append((i))
bigram_idx = pd.Series(bigram_idx)

# Getting the bigrams as individual strings
results = pd.Series(bigram_df['Bigrams'].explode().reset_index(drop=True))
9/10:
species = pd.Series(list(pd.read_pickle(ALL_SPECIES_FILE)))
genus = pd.Series(list(pd.read_pickle(ALL_GENUS_FILE)))
taxon = pd.read_csv(ALL_TAXON_FILE,delimiter = "\t", names=["Taxon"]).squeeze()

# All countries and subdivisions for matching 
countries = []
for country in list(pycountry.countries):
    countries.append(country)


subdivisions_dict = {}
subdivisions = []
for subdivision in pycountry.subdivisions:
    subdivisions.append(subdivision.name)
    subdivisions_dict[subdivision.name] = pycountry.countries.get(alpha_2 = subdivision.country_code).name
9/11: import taxonerd
9/12: import taxonerd
9/13:
from taxonerd import TaxoNERD
taxonerd = TaxoNERD(prefer_gpu=False)
nlp = taxonerd.load(model="en_core_eco_md", exclude=[], linker="taxref", threshold=0.7)
nlp.pipe_names
9/14:
from taxonerd import TaxoNERD
taxonerd = TaxoNERD(prefer_gpu=False)
nlp = taxonerd.load(model="en_core_eco_biobert", exclude=[], linker="taxref", threshold=0.7)
nlp.pipe_names
9/15:
from taxonerd import TaxoNERD
taxonerd = TaxoNERD(prefer_gpu=True)
nlp = taxonerd.load(model="en_core_eco_biobert")
doc = nlp("Brown bears (Ursus arctos), which are widely distributed throughout the northern hemisphere, are recognised as opportunistic omnivore")
print(doc.ents)
print([tok.lemma_ for tok in doc])
['Brown', 'bear', '(', 'ursus', 'arcto', ')', ',', 'which', 'be', 'widely', 'distribute', 'throughout', 'the', 'northern', 'hemisphere', ',', 'be', 'recognise', 'as', 'opportunistic', 'omnivore']
9/16:
from taxonerd import TaxoNERD
taxonerd = TaxoNERD(prefer_gpu=True)
nlp = taxonerd.load(model="en_core_eco_biobert")
doc = nlp("Brown bears (Ursus arctos), which are widely distributed throughout the northern hemisphere, are recognised as opportunistic omnivore")
print(doc.ents)
print([tok.lemma_ for tok in doc])
['Brown', 'bear', '(', 'ursus', 'arcto', ')', ',', 'which', 'be', 'widely', 'distribute', 'throughout', 'the', 'northern', 'hemisphere', ',', 'be', 'recognise', 'as', 'opportunistic', 'omnivore']
9/17: ! nvcc --version
9/18: ! module load cuda
9/19:
from taxonerd import TaxoNERD
taxonerd = TaxoNERD(prefer_gpu=True)
nlp = taxonerd.load(model="en_core_eco_biobert")
doc = nlp("Brown bears (Ursus arctos), which are widely distributed throughout the northern hemisphere, are recognised as opportunistic omnivore")
print(doc.ents)
print([tok.lemma_ for tok in doc])
['Brown', 'bear', '(', 'ursus', 'arcto', ')', ',', 'which', 'be', 'widely', 'distribute', 'throughout', 'the', 'northern', 'hemisphere', ',', 'be', 'recognise', 'as', 'opportunistic', 'omnivore']
9/20:
! module load cuda
! nvcc --version
9/21:
from taxonerd import TaxoNERD
taxonerd = TaxoNERD(prefer_gpu=True)
nlp = taxonerd.load(model="en_core_eco_biobert")
doc = nlp("Brown bears (Ursus arctos), which are widely distributed throughout the northern hemisphere, are recognised as opportunistic omnivore")
print(doc.ents)
print([tok.lemma_ for tok in doc])
['Brown', 'bear', '(', 'ursus', 'arcto', ')', ',', 'which', 'be', 'widely', 'distribute', 'throughout', 'the', 'northern', 'hemisphere', ',', 'be', 'recognise', 'as', 'opportunistic', 'omnivore']
9/22:
from taxonerd import TaxoNERD
taxonerd = TaxoNERD(prefer_gpu=False)
nlp = taxonerd.load(model="en_core_eco_biobert")
doc = nlp("Brown bears (Ursus arctos), which are widely distributed throughout the northern hemisphere, are recognised as opportunistic omnivore")
print(doc.ents)
print([tok.lemma_ for tok in doc])
['Brown', 'bear', '(', 'ursus', 'arcto', ')', ',', 'which', 'be', 'widely', 'distribute', 'throughout', 'the', 'northern', 'hemisphere', ',', 'be', 'recognise', 'as', 'opportunistic', 'omnivore']
9/23: combined_df["Transcription"]
9/24: combined_df["Transcription"][0]
9/25:
from taxonerd import TaxoNERD
taxonerd = TaxoNERD(prefer_gpu=False)
nlp = taxonerd.load(model="en_core_eco_biobert")
doc = nlp("Brown bears (Ursus arctos), which are widely distributed throughout the northern hemisphere, are recognised as opportunistic omnivore")
print(doc.ents)
print([tok.lemma_ for tok in doc])
9/26:
from taxonerd import TaxoNERD
taxonerd = TaxoNERD(prefer_gpu=False)
nlp = taxonerd.load(model="en_core_eco_biobert")
doc = nlp(["Museum d'Histoire naturelle de Paris",
 "Herbier d'Antoine Laurent de Jussieu",
 "Donne au Museum par les enfants d'Adrien de JUSSIE en",
 'Catal. no 3160 4A.',
 '8',
 '0 0',
 '0',
 'figurdia.',
 '8',
 'Ferraria pavonica himself kept',
 'Herbier Museum Paris',
 '900672666',
 'PR. Justin G. W. Du Company 1874'])
print(doc.ents)
print([tok.lemma_ for tok in doc])
9/27:
taxonerd = TaxoNERD(prefer_gpu=False)
nlp = taxonerd.load(model="en_core_eco_biobert")

sample_input = ["Museum d'Histoire naturelle de Paris",
 "Herbier d'Antoine Laurent de Jussieu",
 "Donne au Museum par les enfants d'Adrien de JUSSIE en",
 'Catal. no 3160 4A.',
 '8',
 '0 0',
 '0',
 'figurdia.',
 '8',
 'Ferraria pavonica himself kept',
 'Herbier Museum Paris',
 '900672666',
 'PR. Justin G. W. Du Company 1874']

for item in sample_input:
    doc = nlp(item)
    print(doc.ents)
# print([tok.lemma_ for tok in doc])
9/28:
taxonerd = TaxoNERD(prefer_gpu=False)
nlp = taxonerd.load(model="en_core_eco_biobert", linker="gbif_backbone")

sample_input = ["Museum d'Histoire naturelle de Paris",
 "Herbier d'Antoine Laurent de Jussieu",
 "Donne au Museum par les enfants d'Adrien de JUSSIE en",
 'Catal. no 3160 4A.',
 '8',
 '0 0',
 '0',
 'figurdia.',
 '8',
 'Ferraria pavonica himself kept',
 'Herbier Museum Paris',
 '900672666',
 'PR. Justin G. W. Du Company 1874']

for item in sample_input:
    doc = nlp(item)
    print(doc.ents)
# print([tok.lemma_ for tok in doc])
9/29:
taxonerd = TaxoNERD(prefer_gpu=False)
nlp = taxonerd.load(model="en_core_eco_biobert", linker="taxref")

sample_input = ["Museum d'Histoire naturelle de Paris",
 "Herbier d'Antoine Laurent de Jussieu",
 "Donne au Museum par les enfants d'Adrien de JUSSIE en",
 'Catal. no 3160 4A.',
 '8',
 '0 0',
 '0',
 'figurdia.',
 '8',
 'Ferraria pavonica himself kept',
 'Herbier Museum Paris',
 '900672666',
 'PR. Justin G. W. Du Company 1874']

for item in sample_input:
    doc = nlp(item)
    print(doc.ents)
# print([tok.lemma_ for tok in doc])
10/1:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# !pip install craft-text-detector
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

# from string_grouper import match_strings, match_most_similar
# !pip install pycountry
import pycountry
import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results
11/1:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# !pip install craft-text-detector
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

# from string_grouper import match_strings, match_most_similar
# !pip install pycountry
import pycountry
import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results
11/2:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# !pip install craft-text-detector
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

# from string_grouper import match_strings, match_most_similar
# !pip install pycountry
import pycountry
import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results
12/1:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# !pip install craft-text-detector
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

# from string_grouper import match_strings, match_most_similar
# !pip install pycountry
import pycountry
import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results
13/1:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# !pip install craft-text-detector
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

# from string_grouper import match_strings, match_most_similar
# !pip install pycountry
import pycountry
import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results
13/2:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/usr4/ds549/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc

# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
# ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
ALL_TAXON_FILE = '/usr4/ds549/kabilanm/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
13/3:
# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft,export_extra = False, text_threshold = .7,link_threshold = .4, crop_type="poly",low_text = .3,cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
13/4:
# Deleting empty folders, which occurs if some of the images get no segementation from CRAFT
root = output_dir_craft
folders = list(os.walk(root))[1:]
deleted = []
for folder in folders:
    if not folder[2]:
        deleted.append(folder)
        os.rmdir(folder[0])
        
# Setting up the Tr-OCR model and processor
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten") 
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten").to(device)

# Use all available gpu's
model_gpu= nn.DataParallel(model,list(range(torch.cuda.device_count()))).to(device)

# Dataloader for working with gpu's
trainset = datasets.ImageFolder(output_dir_craft, transform = processor)
testloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=False)

# For matching words to image
filenames = [s.replace('_crops', '') for s in list(trainset.class_to_idx)]

# For matching the image name with the label name
word_log_dic = {k: v for k,v in enumerate(filenames)}
# For matching the image name with the transriptions
words_identified = {k: [] for v,k in enumerate(filenames)}
13/5:
# Save filenames
with open(save_dir+'filenames.txt', 'w') as fp:
    for item in filenames:
        # write each item on a new line
        fp.write("%s\n" % item)
# Save word_log_dic 
with open(save_dir+'word_log_dic.json', 'w') as fp:
    json.dump(word_log_dic, fp)
# Save words_identified
with open(save_dir+'words_identified.json', 'w') as fp:
    json.dump(words_identified, fp)
13/6:
# Save filenames
with open(save_dir+'filenames.txt', 'w') as fp:
    for item in filenames:
        # write each item on a new line
        fp.write("%s\n" % item)
# Save word_log_dic 
with open(save_dir+'word_log_dic.json', 'w') as fp:
    json.dump(word_log_dic, fp)
# Save words_identified
with open(save_dir+'words_identified.json', 'w') as fp:
    json.dump(words_identified, fp)
13/7:
#Storing the outputs
results,confidence,labels = trocr.evaluate_craft_seg(model,processor, words_identified,word_log_dic,testloader,device)
#Saving all the outputs in dataframe
df = pd.DataFrame(list(zip(results,confidence,labels)),columns = ['Results','Confidence','Labels'])
df.to_pickle(save_dir+'full_results.pkl')
13/8:
# First part of final csv with results, confidence level from tr-ocr, and label
combined_df = trocr.combine_by_label(df)

# Adding the image path and all bounding boxes 

df_dictionary = pd.DataFrame(boxes.items(), columns=['Image_Path', 'Bounding_Boxes'])
combined_df = pd.concat([combined_df, df_dictionary], axis=1, join='inner')
display(combined_df.head())
13/9:
#Save intermediate file
combined_df.to_pickle(save_dir+'/test.pkl')
13/10: combined_df.shape()
13/11: combined_df.shape
13/12:
# Creating a new column which contains all bigrams from the transcription, with an associated index for each bigram
bigram_df = combined_df.copy()

bigram_df['Bigrams'] = bigram_df['Transcription'].str.join(' ').str.split(' ')

bigram_df['Bigrams'] = bigram_df['Bigrams'].apply(lambda lst: [lst[i:i+2] for i in range(len(lst) - 1)]).apply(lambda sublists: [' '.join(sublist) for sublist in sublists])
bigram_df['Bigram_idx'] = bigram_df.apply(matching.bigram_indices, axis=1)

# Associating all biagrams with their respective image
bigram_idx = []
for i in range(len(bigram_df)):
    for j in range(len(bigram_df.loc[i, 'Bigrams'])):
        bigram_idx.append((i))
bigram_idx = pd.Series(bigram_idx)

# Getting the bigrams as individual strings
results = pd.Series(bigram_df['Bigrams'].explode().reset_index(drop=True))
13/13:
from taxonerd import TaxoNERD
taxonerd = TaxoNERD(prefer_gpu=False)
nlp = taxonerd.load(model="en_core_eco_biobert", exclude=[], linker="taxref", threshold=0.9)
nlp.pipe_names
13/14:
for row in combined_df:
    print(row)
13/15:
for row in combined_df.iterrows():
    print(row)
13/16:
for row in combined_df.iterrows():
    print(row["Transcription"])
13/17:
for index, row in combined_df.iterrows():
    print(row["Transcription"])
13/18:
# input = ["Museum d'Histoire naturelle de Paris",
#  "Herbier d'Antoine Laurent de Jussieu",
#  "Donne au Museum par les enfants d'Adrien de JUSSIE en",
#  'Catal. no 3160 4A.',
#  '8',
#  '0 0',
#  '0',
#  'figurdia.',
#  '8',
#  'Ferraria pavonica himself kept',
#  'Herbier Museum Paris',
#  '900672666',
#  'PR. Justin G. W. Du Company 1874', 
#  '0-',
#  '100,',
#  'ed state.',
#  '1627083',
#  'United States national museum',
#  'Flora Hawaiiansis',
#  'Collected by C. N. Forbes on Oahn.',
#  'Clermontia persicifolia Gand',
#  'Maboulain Thugsky Popular',
#  'open 26-th. Taylor edge',
#  'Image no.',
#  'soil # TO. Device PACS. " STOP MUSICAL REPORT',
#  '0427028']
13/19:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0.9
) 

for index, row in combined_df.iterrows():
    input = row["Transcription"]
    
    for item in sample_input:
        doc = taxonerd.find_in_text(item)
        try:
            print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
    #         print(doc.entity)
            count+=1
        except:
            pass
13/20:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0.9
) 

for index, row in combined_df.iterrows():
    input = row["Transcription"]
    
    for item in input:
        doc = taxonerd.find_in_text(item)
        try:
            print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
    #         print(doc.entity)
            count+=1
        except:
            pass
13/21: print(count)
13/22:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0.9
) 

for index, row in combined_df.iterrows():
    input_text = row["Transcription"]
    
    for item in input_text:
        doc = taxonerd.find_in_text(item)
        try:
            output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
            print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
            count+=1
        except:
            pass
13/23: print(output)
13/24:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# Get confidence 

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0.9
) 

for index, row in combined_df.iterrows():
    input_text = row["Transcription"]
    
    for item in input_text:
        doc = taxonerd.find_in_text(item)
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
        try:
            print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
            count+=1
        except:
            pass
13/25:
output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
print(output)
13/26:
print(doc)
output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
print(output)
13/27:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# Get confidence 

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0.9
) 

for index, row in combined_df.iterrows():
    input_text = row["Transcription"]
    
    for item in input_text:
        doc = taxonerd.find_in_text(item)
        try:
            print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
            output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
            count+=1
        except:
            pass
13/28: print(count)
13/29: print(output)
13/30:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# !pip install craft-text-detector
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

# from string_grouper import match_strings, match_most_similar
# !pip install pycountry
import pycountry
import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results
13/31:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/usr4/ds549/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc

# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
# ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
ALL_TAXON_FILE = '/usr4/ds549/kabilanm/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
13/32:
# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft,export_extra = False, text_threshold = .7,link_threshold = .4, crop_type="poly",low_text = .3,cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
13/33:
# Deleting empty folders, which occurs if some of the images get no segementation from CRAFT
root = output_dir_craft
folders = list(os.walk(root))[1:]
deleted = []
for folder in folders:
    if not folder[2]:
        deleted.append(folder)
        os.rmdir(folder[0])
        
# Setting up the Tr-OCR model and processor
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten") 
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten").to(device)

# Use all available gpu's
model_gpu= nn.DataParallel(model,list(range(torch.cuda.device_count()))).to(device)

# Dataloader for working with gpu's
trainset = datasets.ImageFolder(output_dir_craft, transform = processor)
testloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=False)

# For matching words to image
filenames = [s.replace('_crops', '') for s in list(trainset.class_to_idx)]

# For matching the image name with the label name
word_log_dic = {k: v for k,v in enumerate(filenames)}
# For matching the image name with the transriptions
words_identified = {k: [] for v,k in enumerate(filenames)}
13/34:
# Save filenames
with open(save_dir+'filenames.txt', 'w') as fp:
    for item in filenames:
        # write each item on a new line
        fp.write("%s\n" % item)
# Save word_log_dic 
with open(save_dir+'word_log_dic.json', 'w') as fp:
    json.dump(word_log_dic, fp)
# Save words_identified
with open(save_dir+'words_identified.json', 'w') as fp:
    json.dump(words_identified, fp)
13/35:
#Storing the outputs
results,confidence,labels = trocr.evaluate_craft_seg(model,processor, words_identified,word_log_dic,testloader,device)
#Saving all the outputs in dataframe
df = pd.DataFrame(list(zip(results,confidence,labels)),columns = ['Results','Confidence','Labels'])
df.to_pickle(save_dir+'full_results.pkl')
13/36:
# First part of final csv with results, confidence level from tr-ocr, and label
combined_df = trocr.combine_by_label(df)

# Adding the image path and all bounding boxes 

df_dictionary = pd.DataFrame(boxes.items(), columns=['Image_Path', 'Bounding_Boxes'])
combined_df = pd.concat([combined_df, df_dictionary], axis=1, join='inner')
display(combined_df.head())
13/37: combined_df.shape
13/38:
#Save intermediate file
combined_df.to_pickle(save_dir+'/test.pkl')
13/39:
# Creating a new column which contains all bigrams from the transcription, with an associated index for each bigram
bigram_df = combined_df.copy()

bigram_df['Bigrams'] = bigram_df['Transcription'].str.join(' ').str.split(' ')

bigram_df['Bigrams'] = bigram_df['Bigrams'].apply(lambda lst: [lst[i:i+2] for i in range(len(lst) - 1)]).apply(lambda sublists: [' '.join(sublist) for sublist in sublists])
bigram_df['Bigram_idx'] = bigram_df.apply(matching.bigram_indices, axis=1)

# Associating all biagrams with their respective image
bigram_idx = []
for i in range(len(bigram_df)):
    for j in range(len(bigram_df.loc[i, 'Bigrams'])):
        bigram_idx.append((i))
bigram_idx = pd.Series(bigram_idx)

# Getting the bigrams as individual strings
results = pd.Series(bigram_df['Bigrams'].explode().reset_index(drop=True))
13/40:
from taxonerd import TaxoNERD
taxonerd = TaxoNERD(prefer_gpu=False)
nlp = taxonerd.load(model="en_core_eco_biobert", exclude=[], linker="taxref", threshold=0.9)
nlp.pipe_names
13/41:
taxonerd = TaxoNERD(prefer_gpu=True)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# Get confidence for each interval
# Use bigrams

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0.9
) 

for index, row in combined_df.iterrows():
    input_text = row["Transcription"]
    
    for item in input_text:
        doc = taxonerd.find_in_text(item)
        try:
            print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
            count+=1
            output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
        except:
            pass
13/42:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# Get confidence for each interval
# Use bigrams

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0.9
) 

for index, row in combined_df.iterrows():
    input_text = row["Transcription"]
    
    for item in input_text:
        doc = taxonerd.find_in_text(item)
        try:
            print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
            count+=1
            output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
        except:
            pass
13/43: print(count)
13/44: print(output)
14/1:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# !pip install craft-text-detector
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

# from string_grouper import match_strings, match_most_similar
# !pip install pycountry
import pycountry
import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results
14/2:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/usr4/ds549/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc

# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
# ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
ALL_TAXON_FILE = '/usr4/ds549/kabilanm/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
14/3:
# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft,export_extra = False, text_threshold = .7,link_threshold = .4, crop_type="poly",low_text = .3,cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
14/4:
# Deleting empty folders, which occurs if some of the images get no segementation from CRAFT
root = output_dir_craft
folders = list(os.walk(root))[1:]
deleted = []
for folder in folders:
    if not folder[2]:
        deleted.append(folder)
        os.rmdir(folder[0])
        
# Setting up the Tr-OCR model and processor
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten") 
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten").to(device)

# Use all available gpu's
model_gpu= nn.DataParallel(model,list(range(torch.cuda.device_count()))).to(device)

# Dataloader for working with gpu's
trainset = datasets.ImageFolder(output_dir_craft, transform = processor)
testloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=False)

# For matching words to image
filenames = [s.replace('_crops', '') for s in list(trainset.class_to_idx)]

# For matching the image name with the label name
word_log_dic = {k: v for k,v in enumerate(filenames)}
# For matching the image name with the transriptions
words_identified = {k: [] for v,k in enumerate(filenames)}
14/5:
# Save filenames
with open(save_dir+'filenames.txt', 'w') as fp:
    for item in filenames:
        # write each item on a new line
        fp.write("%s\n" % item)
# Save word_log_dic 
with open(save_dir+'word_log_dic.json', 'w') as fp:
    json.dump(word_log_dic, fp)
# Save words_identified
with open(save_dir+'words_identified.json', 'w') as fp:
    json.dump(words_identified, fp)
14/6:
#Storing the outputs
results,confidence,labels = trocr.evaluate_craft_seg(model,processor, words_identified,word_log_dic,testloader,device)
#Saving all the outputs in dataframe
df = pd.DataFrame(list(zip(results,confidence,labels)),columns = ['Results','Confidence','Labels'])
df.to_pickle(save_dir+'full_results.pkl')
14/7:
# First part of final csv with results, confidence level from tr-ocr, and label
combined_df = trocr.combine_by_label(df)

# Adding the image path and all bounding boxes 

df_dictionary = pd.DataFrame(boxes.items(), columns=['Image_Path', 'Bounding_Boxes'])
combined_df = pd.concat([combined_df, df_dictionary], axis=1, join='inner')
display(combined_df.head())
14/8: combined_df.shape
14/9:
#Save intermediate file
combined_df.to_pickle(save_dir+'/test.pkl')
14/10:
# Creating a new column which contains all bigrams from the transcription, with an associated index for each bigram
bigram_df = combined_df.copy()

bigram_df['Bigrams'] = bigram_df['Transcription'].str.join(' ').str.split(' ')

bigram_df['Bigrams'] = bigram_df['Bigrams'].apply(lambda lst: [lst[i:i+2] for i in range(len(lst) - 1)]).apply(lambda sublists: [' '.join(sublist) for sublist in sublists])
bigram_df['Bigram_idx'] = bigram_df.apply(matching.bigram_indices, axis=1)

# Associating all biagrams with their respective image
bigram_idx = []
for i in range(len(bigram_df)):
    for j in range(len(bigram_df.loc[i, 'Bigrams'])):
        bigram_idx.append((i))
bigram_idx = pd.Series(bigram_idx)

# Getting the bigrams as individual strings
results = pd.Series(bigram_df['Bigrams'].explode().reset_index(drop=True))
14/11:
from taxonerd import TaxoNERD
taxonerd = TaxoNERD(prefer_gpu=False)
nlp = taxonerd.load(model="en_core_eco_biobert", exclude=[], linker="taxref", threshold=0.9)
nlp.pipe_names
14/12:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# Get confidence for each interval
# Use bigrams

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0.9
) 

for index, row in combined_df.iterrows():
    input_text = row["Transcription"]
    
    for item in input_text:
        doc = taxonerd.find_in_text(item)
        try:
            print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
            count+=1
            output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
        except:
            pass
14/13: print(count)
14/14: print(output)
14/15:
# input = ["Museum d'Histoire naturelle de Paris",
#  "Herbier d'Antoine Laurent de Jussieu",
#  "Donne au Museum par les enfants d'Adrien de JUSSIE en",
#  'Catal. no 3160 4A.',
#  '8',
#  '0 0',
#  '0',
#  'figurdia.',
#  '8',
#  'Ferraria pavonica himself kept',
#  'Herbier Museum Paris',
#  '900672666',
#  'PR. Justin G. W. Du Company 1874', 
#  '0-',
#  '100,',
#  'ed state.',
#  '1627083',
#  'United States national museum',
#  'Flora Hawaiiansis',
#  'Collected by C. N. Forbes on Oahn.',
#  'Clermontia persicifolia Gand',
#  'Maboulain Thugsky Popular',
#  'open 26-th. Taylor edge',
#  'Image no.',
#  'soil # TO. Device PACS. " STOP MUSICAL REPORT',
#  '0427028']
14/16:
species = pd.Series(list(pd.read_pickle(ALL_SPECIES_FILE)))
genus = pd.Series(list(pd.read_pickle(ALL_GENUS_FILE)))
taxon = pd.read_csv(ALL_TAXON_FILE,delimiter = "\t", names=["Taxon"]).squeeze()

# All countries and subdivisions for matching 
countries = []
for country in list(pycountry.countries):
    countries.append(country)


subdivisions_dict = {}
subdivisions = []
for subdivision in pycountry.subdivisions:
    subdivisions.append(subdivision.name)
    subdivisions_dict[subdivision.name] = pycountry.countries.get(alpha_2 = subdivision.country_code).name
14/17:
# Reading in the ground truth values

gt_t = workdir2+'/taxon_gt.txt'
Taxon_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_t) }

gt_g = workdir2+'/geography_gt.txt'
Geography_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_g) }

gt_c = workdir2+'/collector_gt.txt'
Collector_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_c) }

comparison_file = {"Taxon":Taxon_truth,"Countries":Geography_truth,"Collector":Collector_truth}
14/18: combined_df[['Image_Path']].head
14/19: combined_df[['Image_Path']].head(1)
14/20: print(combined_df[['Image_Path']])
14/21: print(combined_df[['Image_Path']].head(1))
14/22: string = combined_df[['Image_Path']].iloc[1]
14/23:
string = combined_df[['Image_Path']].iloc[1]
print(String)
14/24:
string = combined_df[['Image_Path']].iloc[1]
print(tring)
14/25:
string = combined_df[['Image_Path']].iloc[1]
print(string)
14/26:
string = combined_df[['Image_Path']].loc[1]
print(string)
14/27:
string = combined_df[['Image_Path']].loc[1].str()
print(string)
14/28:
string = combined_df[['Image_Path']].loc[1].str
print(string)
14/29:
string = str(combined_df[['Image_Path']].loc[1])
print(string)
14/30:
string = str(combined_df['Image_Path'].loc[1])
print(string)
14/31:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0.9
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    
#     for item in input_text:
    doc = taxonerd.find_in_text(input_text)
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        count+=1
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
    except:
        pass
14/32: print(count)
14/33: print(output)
14/34:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0.8
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    
#     for item in input_text:
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        count+=1
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
    except:
        pass
14/35: print(count)
14/36:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0.70
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    
#     for item in input_text:
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        count+=1
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
    except:
        pass
14/37: print(count)
14/38:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0.60
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    
#     for item in input_text:
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        count+=1
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
    except:
        pass
14/39: print(count)
14/40:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0.50
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    
#     for item in input_text:
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        count+=1
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
    except:
        pass
14/41: print(count)
14/42:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
#     threshold=0.50
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    
#     for item in input_text:
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        count+=1
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
    except:
        pass
14/43: print(count)
14/44:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    
#     for item in input_text:
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        count+=1
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
    except:
        pass
14/45: print(count)
14/46: combined_output[["Taxon_Output"]].head()
14/47: combined_df[["Taxon_Output"]].head()
14/48:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    
#     for item in input_text:
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        combined_df["Taxon_Output"] = doc.entity[0][0][1]
        count+=1
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
    except:
        pass
14/49: combined_df[["Taxon_Output"]].head()
14/50: combined_df[["Taxon_Output"]].tail()
14/51:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        row["Taxon_Output"] = doc.entity[0][0][1]
        count+=1
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
    except:
        pass
14/52: combined_df[["Taxon_Output"]].head()
14/53:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        row["Taxon_Output"] = doc.entity[0][0][1]
        count+=1
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
    except:
        pass
14/54: print(count)
14/55: combined_df[["Taxon_Output"]].head()
14/56:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        combined_df[index]["Taxon_Output"] = doc.entity[0][0][1]
        count+=1
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
    except:
        pass
14/57: print(count)
14/58: combined_df[["Taxon_Output"]].head()
14/59:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []
taxon_output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# [DONE] Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        taxon_output.append(doc.entity[0][0][1])
        count+=1
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
    except:
        taxon_output.append("")
14/60:
print(count)
print(taxon_output)
14/61:
print(count)
print(taxon_output)
print(index)
14/62:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []
taxon_output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# [DONE] Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        taxon_output.append(doc.entity[0][0][1])
        count+=1
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
    except:
#         taxon_output.append("")
        pass
14/63:
print(count)
print(taxon_output)
print(index)
14/64:
print(count)
print(len(taxon_output))
print(index)
14/65:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []
taxon_output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# [DONE] Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    doc = taxonerd.find_in_text(input_text)
    
#     try:
    print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
    taxon_output.append(doc.entity[0][0][1])
    count+=1
    output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
#     except:
# #         taxon_output.append("")
#         pass
14/66:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []
taxon_output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# [DONE] Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    doc = taxonerd.find_in_text(input_text)
    
#     try:
    print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
    taxon_output.append(doc.entity[0][0][1])
    count+=1
#     output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
#     except:
# #         taxon_output.append("")
#         pass
14/67:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []
taxon_output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# [DONE] Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    doc = taxonerd.find_in_text(input_text)
    
#     try:
    print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
    taxon_output.append(doc.entity[0][0][1])
    count+=1
#     output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
#     except:
# #         taxon_output.append("")
#         pass
14/68:
count = 0
for filename in tqdm(sorted(os.listdir(workdir))):
    count +=1

print(count)
15/1:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# !pip install craft-text-detector
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

# from string_grouper import match_strings, match_most_similar
# !pip install pycountry
import pycountry
import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results
15/2:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/usr4/ds549/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc

# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
# ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
ALL_TAXON_FILE = '/usr4/ds549/kabilanm/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
15/3:
count = 0
for filename in tqdm(sorted(os.listdir(workdir))):
    count +=1

print(count)
15/4:
# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft,export_extra = False, text_threshold = .7,link_threshold = .4, crop_type="poly",low_text = .3,cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
    
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
15/5:
# Deleting empty folders, which occurs if some of the images get no segementation from CRAFT
root = output_dir_craft
folders = list(os.walk(root))[1:]
deleted = []
for folder in folders:
    if not folder[2]:
        deleted.append(folder)
        os.rmdir(folder[0])
        
# Setting up the Tr-OCR model and processor
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten") 
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten").to(device)

# Use all available gpu's
model_gpu= nn.DataParallel(model,list(range(torch.cuda.device_count()))).to(device)

# Dataloader for working with gpu's
trainset = datasets.ImageFolder(output_dir_craft, transform = processor)
testloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=False)

# For matching words to image
filenames = [s.replace('_crops', '') for s in list(trainset.class_to_idx)]

# For matching the image name with the label name
word_log_dic = {k: v for k,v in enumerate(filenames)}
# For matching the image name with the transriptions
words_identified = {k: [] for v,k in enumerate(filenames)}
15/6:
# Save filenames
with open(save_dir+'filenames.txt', 'w') as fp:
    for item in filenames:
        # write each item on a new line
        fp.write("%s\n" % item)
# Save word_log_dic 
with open(save_dir+'word_log_dic.json', 'w') as fp:
    json.dump(word_log_dic, fp)
# Save words_identified
with open(save_dir+'words_identified.json', 'w') as fp:
    json.dump(words_identified, fp)
15/7:
#Storing the outputs
results,confidence,labels = trocr.evaluate_craft_seg(model,processor, words_identified,word_log_dic,testloader,device)
#Saving all the outputs in dataframe
df = pd.DataFrame(list(zip(results,confidence,labels)),columns = ['Results','Confidence','Labels'])
df.to_pickle(save_dir+'full_results.pkl')
15/8:
# First part of final csv with results, confidence level from tr-ocr, and label
combined_df = trocr.combine_by_label(df)

# Adding the image path and all bounding boxes 

df_dictionary = pd.DataFrame(boxes.items(), columns=['Image_Path', 'Bounding_Boxes'])
combined_df = pd.concat([combined_df, df_dictionary], axis=1, join='inner')
display(combined_df.head())
15/9: combined_df.shape
15/10:
string = str(combined_df['Image_Path'].loc[1])
print(string)
15/11:
#Save intermediate file
combined_df.to_pickle(save_dir+'/test.pkl')
15/12:
# Creating a new column which contains all bigrams from the transcription, with an associated index for each bigram
bigram_df = combined_df.copy()

bigram_df['Bigrams'] = bigram_df['Transcription'].str.join(' ').str.split(' ')

bigram_df['Bigrams'] = bigram_df['Bigrams'].apply(lambda lst: [lst[i:i+2] for i in range(len(lst) - 1)]).apply(lambda sublists: [' '.join(sublist) for sublist in sublists])
bigram_df['Bigram_idx'] = bigram_df.apply(matching.bigram_indices, axis=1)

# Associating all biagrams with their respective image
bigram_idx = []
for i in range(len(bigram_df)):
    for j in range(len(bigram_df.loc[i, 'Bigrams'])):
        bigram_idx.append((i))
bigram_idx = pd.Series(bigram_idx)

# Getting the bigrams as individual strings
results = pd.Series(bigram_df['Bigrams'].explode().reset_index(drop=True))
15/13:
taxonerd = TaxoNERD(prefer_gpu=False)

count = 0
output = []
taxon_output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# [DONE] Get confidence for each interval
# Use bigrams

# construct one single sentence out of all words
# use BERT for person and location
# same text input to both models

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="gbif_backbone", 
    threshold=0
) 

for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    doc = taxonerd.find_in_text(input_text)
    
#     try:
    print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
    taxon_output.append(doc.entity[0][0][1])
    count+=1
#     output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
#     except:
# #         taxon_output.append("")
#         pass
15/14:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results

from taxonerd import TaxoNERD
15/15:
taxonerd = TaxoNERD(prefer_gpu=False) # set to "true" if GPU is accessible

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="gbif_backbone", 
    threshold=0
)

count = 0
output = []
taxon_output = []

# test "gbif_backbone" linker -> more species here
# Clermontia persicifolia -> couldn't find this species in TAXREF
# [DONE] Get confidence for each interval

# use BERT for person and location
# same text input to both models



for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    doc = taxonerd.find_in_text(input_text)
    
#     try:
    print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
    taxon_output.append(doc.entity[0][0][1])
    count+=1
#     output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
#     except:
# #         taxon_output.append("")
#         pass
15/16:
taxonerd = TaxoNERD(prefer_gpu=False) # set to "true" if GPU is accessible

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="gbif_backbone", 
    threshold=0
)
15/17:
taxonerd = TaxoNERD(prefer_gpu=False) # set to "true" if GPU is accessible

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="taxref", 
    threshold=0
)
15/18:
count = 0
output = []
taxon_output = []

# test "gbif_backbone" linker -> more species here
# use BERT for person and location


for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        taxon_output.append(doc.entity[0][0][1])
        count+=1
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
    except:
#         taxon_output.append("")
        pass
15/19:
taxonerd = TaxoNERD(prefer_gpu=False) # set to "true" if GPU is accessible

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="ncbi_taxonomy", 
    threshold=0
)
15/20:
count = 0
output = []
taxon_output = []

# test "gbif_backbone" linker -> more species here
# use BERT for person and location


for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp)
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        taxon_output.append(doc.entity[0][0][1])
        count+=1
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
    except:
#         taxon_output.append("")
        pass
15/21:
print(count)
print(len(taxon_output))
print(index)
15/22: print(taxon_output)
15/23:
count = 0
output = []
taxon_output = []

# test "gbif_backbone" linker -> more species here
# use BERT for person and location

# predict taxons for text detected from each image
for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp) # construct a single string out of all the detected text
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        taxon_output.append(doc.entity[0][0][1])
        count+=1
        output.append(tuple(doc.entity[0][0][1], doc.entity[0][0][2]))
    except:
        taxon_output.append("")
        pass
15/24:
print(count)
print(len(taxon_output))
15/25: print(output)
15/26: print(output)
15/27: print(taxon_output)
15/28:
count = 0
output = []
taxon_output = []

# test "gbif_backbone" linker -> more species here
# use BERT for person and location

# predict taxons for text detected from each image
for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp) # construct a single string out of all the detected text
    doc = taxonerd.find_in_text(input_text)
    
#     try:
    print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
    count+=1
    taxon_output.append(str(doc.entity[0][0][1]))
#     except:
#         taxon_output.append("")
#         pass
15/29:
count = 0
output = []
taxon_output = []

# test "gbif_backbone" linker -> more species here
# use BERT for person and location

# predict taxons for text detected from each image
for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp) # construct a single string out of all the detected text
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        count+=1
        taxon_output.append(str(doc.entity[0][0][1]))
    except AttributeError:
        taxon_output.append("")
        pass
15/30:
print(count)
print(len(taxon_output))
15/31: print(taxon_output)
15/32:
count = 0
output = []
taxon_output = []

# test "gbif_backbone" linker -> more species here
# use BERT for person and location

# predict taxons for text detected from each image
for index, row in combined_df.iterrows():
    temp = row["Transcription"]
    
    input_text = " ".join(temp) # construct a single string out of all the detected text
    doc = taxonerd.find_in_text(input_text)
    
    try:
        print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
        count+=1
        taxon_output.append(str(doc.entity[0][0][1]))
    except AttributeError:
        taxon_output.append("")
        pass
15/33:
print(count)
print(len(taxon_output))
15/34: print(taxon_output)
15/35:
# Reading in the ground truth values

gt_t = workdir2+'/taxon_gt.txt'
Taxon_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_t) }

gt_g = workdir2+'/geography_gt.txt'
Geography_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_g) }

gt_c = workdir2+'/collector_gt.txt'
Collector_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_c) }

comparison_file = {"Taxon":Taxon_truth,"Countries":Geography_truth,"Collector":Collector_truth}
15/36: print(Taxon_truth)
15/37: print(taxon_output[:10])
15/38:
from sklearn.metrics.pairwise import cosine_similarity

# print(cosine_similarity(df, df))
15/39:
combined_df[["Taxon_Output"]] = taxon_output

combined_df[["Taxon_Output"]].head()
15/40:
combined_df[["Taxon_Output"]].shape

combined_df[["Taxon_Output"]] = taxon_output

combined_df[["Taxon_Output"]].head()
15/41:
combined_df.shape

combined_df[["Taxon_Output"]] = taxon_output

combined_df[["Taxon_Output"]].head()
15/42:
combined_df.shape

# combined_df[["Taxon_Output"]] = taxon_output

# combined_df[["Taxon_Output"]].head()
15/43:
combined_df["Taxon_Output"] = taxon_output

combined_df[["Taxon_Output"]].head()
15/44:
combined_df["Taxon_Output"] = taxon_output

combined_df[["Image_Path", "Taxon_Output"]].head()
15/45:
from sklearn.metrics.pairwise import cosine_similarity

# print(cosine_similarity(df, df))

combined_df["Image_Path"]
15/46:
from sklearn.metrics.pairwise import cosine_similarity

# print(cosine_similarity(df, df))

for index, row in combined_df.iterrows():
    img_path = row["Image_Path"].split("/")
    print(img_path)
    
#     combined_df["Image_Path"].
15/47:
from sklearn.metrics.pairwise import cosine_similarity

# print(cosine_similarity(df, df))

for index, row in combined_df.iterrows():
    img_path = row["Image_Path"].split("/")[-1]
    print(img_path)
    
#     combined_df["Image_Path"].
15/48:
from sklearn.metrics.pairwise import cosine_similarity

# print(cosine_similarity(df, df))

for index, row in combined_df.iterrows():
    img_name = row["Image_Path"].split("/")[-1][-4]
    print(img_name)
    
#     combined_df["Image_Path"].
15/49:
from sklearn.metrics.pairwise import cosine_similarity

# print(cosine_similarity(df, df))

for index, row in combined_df.iterrows():
    img_name = row["Image_Path"].split("/")[-1][:-4]
    print(img_name)
    
#     combined_df["Image_Path"].
15/50:
from sklearn.metrics.pairwise import cosine_similarity

# print(cosine_similarity(df, df))

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    print(img_name)
    
    taxon_predicted = row["Taxon_Output"]
    
    taxon_gt = Taxon_truth[img_name]
    
    print(taxon_gt, taxon_predicted)
    
#     combined_df["Image_Path"].
15/51:
from sklearn.metrics.pairwise import cosine_similarity

# print(cosine_similarity(df, df))

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
#     print(img_name)
    
    taxon_predicted = row["Taxon_Output"]
    
    taxon_gt = Taxon_truth[img_name]
    
    print(taxon_gt, taxon_predicted)
    
#     combined_df["Image_Path"].
15/52:
from sklearn.metrics.pairwise import cosine_similarity

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    print(taxon_gt, taxon_predicted)
    
    # compute cosine similarity between the predicted taxon and ground truth
    print(cosine_similarity(taxon_gt, taxon_predicted))
    
#     combined_df["Image_Path"].
15/53:
def word2vec(word):
    from collections import Counter
    from math import sqrt

    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the "length" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))

    # return a tuple
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]
15/54:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    print(taxon_gt, taxon_predicted)
    
    # compute cosine similarity between the predicted taxon and ground truth
    print(cosdis(word2vec(taxon_gt), word2vec(taxon_predicted)))
    
#     combined_df["Image_Path"].
15/55:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    print(taxon_gt, taxon_predicted)
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        print(cosdis(word2vec(taxon_gt), word2vec(taxon_predicted)))
    except ZeroDivisionError:
        print("0")
    
#     combined_df["Image_Path"].
15/56:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

cosine_sim = []

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        print(taxon_gt, taxon_predicted, sim)
        
    except ZeroDivisionError:
        print("0")
15/57:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

cosine_sim = []

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        print(taxon_gt, taxon_predicted, sim)
        
    except ZeroDivisionError:
        print(taxon_gt, taxon_predicted,"0")
15/58:
sim_threshold = np.arange(0, 1, 0.1)

print(sim_threshold)
15/59:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

cosine_sim = []

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        print(taxon_gt, taxon_predicted, sim)
        
    except ZeroDivisionError:
        print(taxon_gt, taxon_predicted,"0")
15/60:
sim_threshold = np.arange(0, 1, 0.1)

accuracy_at_different_thresh = []

for threshold in sim_threshold:
    acc_count = (cosine_sim > threshold).sum()
    accuracy_at_different_thresh.append()
15/61:
sim_threshold = np.arange(0, 1, 0.1)

accuracy_at_different_thresh = []

for threshold in sim_threshold:
    acc_count = (cosine_sim > threshold).sum()
    accuracy_at_different_thresh.append(acc_count/combined_df.shape[0])
15/62:
sim_threshold = np.arange(0, 1, 0.1)

accuracy_at_different_thresh = []

for threshold in sim_threshold:
    acc_count = (cosine_sim > threshold).sum()
    accuracy_at_different_thresh.append(acc_count/combined_df.shape[0])

print(accuracy_at_different_thresh)
15/63:
sim_threshold = np.arange(0, 1, 0.1)

accuracy_at_different_thresh = []

for threshold in sim_threshold:
    acc_count = (cosine_sim > threshold).sum()
    accuracy_at_different_thresh.append(tuple(threshold, acc_count/combined_df.shape[0]))

print(accuracy_at_different_thresh)
15/64:
sim_threshold = np.arange(0, 1, 0.1)

accuracy_at_different_thresh = []

for threshold in sim_threshold:
    acc_count = (cosine_sim > threshold).sum()
    accuracy_at_different_thresh.append((threshold, acc_count/combined_df.shape[0]))

print(accuracy_at_different_thresh)
15/65:
# generate list of similarity thresholds
sim_threshold = np.arange(0.1, 1, 0.1)

accuracy_at_different_thresh = []

#  compute prediction accuracy at each similarity threshold
for threshold in sim_threshold:
    acc_count = (cosine_sim > threshold).sum()
    accuracy_at_different_thresh.append((threshold, acc_count/combined_df.shape[0]))

print(accuracy_at_different_thresh)
15/66:
# generate list of similarity thresholds
sim_threshold = np.arange(0.1, 1, 0.1)

accuracy_at_different_thresh = []

# compute similarity scores
cosine_sim = []

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        print(taxon_gt, taxon_predicted, sim)
        
    except ZeroDivisionError:
        print(taxon_gt, taxon_predicted,"0")

# append similarity scores to the dataframe
combined_df["Cosine_Similarity"] = cosine_sim
        

    
#  compute prediction accuracy at each similarity threshold
for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > threshold).sum()
    accuracy_at_different_thresh.append((threshold, acc_count/combined_df.shape[0]))

print(accuracy_at_different_thresh)
15/67:
# generate list of similarity thresholds
sim_threshold = np.arange(0.1, 1, 0.1)

accuracy_at_different_thresh = []

# compute similarity scores
cosine_sim = []

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        print(taxon_gt, taxon_predicted, sim)
        
    except ZeroDivisionError:
        print(taxon_gt, taxon_predicted,"0")

# append similarity scores to the dataframe
combined_df["Cosine_Similarity"] = cosine_sim

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        print(taxon_gt, taxon_predicted, sim)
        
    except ZeroDivisionError:
        cosine_sim.append(0)
        print(taxon_gt, taxon_predicted,"0")
        

    
#  compute prediction accuracy at each similarity threshold
for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > threshold).sum()
    accuracy_at_different_thresh.append((threshold, acc_count/combined_df.shape[0]))

print(accuracy_at_different_thresh)
15/68:
# generate list of similarity thresholds
sim_threshold = np.arange(0.1, 1, 0.1)

accuracy_at_different_thresh = []

# compute similarity scores
cosine_sim = []

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        print(taxon_gt, taxon_predicted, sim)
        
    except ZeroDivisionError:
        cosine_sim.append(0)
        print(taxon_gt, taxon_predicted,"0")

# append similarity scores to the dataframe
combined_df["Cosine_Similarity"] = cosine_sim


        

    
#  compute prediction accuracy at each similarity threshold
for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > threshold).sum()
    accuracy_at_different_thresh.append((threshold, acc_count/combined_df.shape[0]))

print(accuracy_at_different_thresh)
15/69:
# generate list of similarity thresholds
sim_threshold = np.arange(0.1, 1, 0.1)

accuracy_at_different_thresh = []

# compute similarity scores
cosine_sim = []

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        # print(taxon_gt, taxon_predicted, sim)
        
    except ZeroDivisionError:
        cosine_sim.append(0)
        # print(taxon_gt, taxon_predicted,"0")

# append similarity scores to the dataframe
combined_df["Cosine_Similarity"] = cosine_sim


        

    
#  compute prediction accuracy at each similarity threshold
for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > threshold).sum()
    accuracy_at_different_thresh.append((threshold, acc_count/combined_df.shape[0]))

print(accuracy_at_different_thresh)
15/70:
# generate list of similarity thresholds
sim_threshold = np.arange(0.1, 1, 0.1)

accuracy_at_different_thresh = []

# compute similarity scores
cosine_sim = []

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        # print(taxon_gt, taxon_predicted, sim)
        
    except ZeroDivisionError:
        cosine_sim.append(0)
        # print(taxon_gt, taxon_predicted,"0")

# append similarity scores to the dataframe
combined_df["Cosine_Similarity"] = cosine_sim


#  compute prediction accuracy at each similarity threshold
for threshold in sim_threshold:
    
    # pick subset of files with the new 

    

    acc_count = (combined_df["Cosine_Similarity"] > threshold).sum()
    accuracy_at_different_thresh.append((threshold, acc_count/combined_df.shape[0]))
    
    

print(accuracy_at_different_thresh)
15/71:
# generate list of similarity thresholds
sim_threshold = np.arange(0.1, 1, 0.1)

accuracy_at_different_thresh = []

# compute similarity scores
cosine_sim = []

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        # print(taxon_gt, taxon_predicted, sim)
        
    except ZeroDivisionError:
        cosine_sim.append(0)
        # print(taxon_gt, taxon_predicted,"0")

# append similarity scores to the dataframe
combined_df["Cosine_Similarity"] = cosine_sim

final_taxon_prediction = pd.DataFrame()
#  compute prediction accuracy at each similarity threshold
for threshold in sim_threshold:
    
    # pick subset of files with the new 

    

    acc_count = (combined_df["Cosine_Similarity"] > threshold).sum()
    accuracy_at_different_thresh.append((threshold, acc_count/combined_df.shape[0]))
    
    

print(accuracy_at_different_thresh)
15/72:
Taxon_truth_sample = {k: Taxon_truth[k] for k in list(Taxon_truth)[:10]}
print(Taxon_truth)
15/73:
Taxon_truth_sample = {k: Taxon_truth[k] for k in list(Taxon_truth)[:10]}
print(Taxon_truth_sample)
15/74:
# generate list of similarity thresholds
sim_threshold = np.arange(0.1, 1, 0.1)

accuracy_at_different_thresh = []

# compute similarity scores
cosine_sim = []

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        # print(taxon_gt, taxon_predicted, sim)
        
    except ZeroDivisionError:
        cosine_sim.append(0)
        # print(taxon_gt, taxon_predicted,"0")

# append similarity scores to the dataframe
combined_df["Cosine_Similarity"] = cosine_sim

final_taxon_prediction = pd.DataFrame(columns=["Threshold", "Taxons_Predicted", "Taxon_Accuracy_Predicted"])

#  compute prediction accuracy at each similarity threshold
for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > threshold).sum()
    acc_val = acc_count/combined_df.shape[0]
#     accuracy_at_different_thresh.append((threshold, acc_val))
    
    
    temp = [threshold, acc_count, acc_val] = 
    df.loc[len(final_taxon_prediction)] = temp
    
    

final_taxon_prediction
15/75:
# generate list of similarity thresholds
sim_threshold = np.arange(0.1, 1, 0.1)

accuracy_at_different_thresh = []

# compute similarity scores
cosine_sim = []

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        # print(taxon_gt, taxon_predicted, sim)
        
    except ZeroDivisionError:
        cosine_sim.append(0)
        # print(taxon_gt, taxon_predicted,"0")

# append similarity scores to the dataframe
combined_df["Cosine_Similarity"] = cosine_sim

final_taxon_prediction = pd.DataFrame(columns=["Threshold", "Taxons_Predicted", "Taxon_Accuracy_Predicted"])

#  compute prediction accuracy at each similarity threshold
for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > threshold).sum()
    acc_val = acc_count/combined_df.shape[0]
#     accuracy_at_different_thresh.append((threshold, acc_val))
    
    
    temp = [threshold, acc_count, acc_val]
    df.loc[len(final_taxon_prediction)] = temp
    
    

final_taxon_prediction
15/76:
# generate list of similarity thresholds
sim_threshold = np.arange(0.1, 1, 0.1)

accuracy_at_different_thresh = []

# compute similarity scores
cosine_sim = []

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        # print(taxon_gt, taxon_predicted, sim)
        
    except ZeroDivisionError:
        cosine_sim.append(0)
        # print(taxon_gt, taxon_predicted,"0")

# append similarity scores to the dataframe
combined_df["Cosine_Similarity"] = cosine_sim

final_taxon_prediction = pd.DataFrame(columns=["Threshold", "Taxons_Predicted", "Taxon_Accuracy_Predicted"])

#  compute prediction accuracy at each similarity threshold
for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > threshold).sum()
    acc_val = acc_count/combined_df.shape[0]
#     accuracy_at_different_thresh.append((threshold, acc_val))
    
    
    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp
    
    

final_taxon_prediction
15/77:
# generate list of similarity thresholds
sim_threshold = np.arange(0.1, 1, 0.1)

accuracy_at_different_thresh = []

# compute similarity scores
cosine_sim = []

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        # print(taxon_gt, taxon_predicted, sim)
        
    except ZeroDivisionError:
        cosine_sim.append(0)
        # print(taxon_gt, taxon_predicted,"0")

# append similarity scores to the dataframe
combined_df["Cosine_Similarity"] = cosine_sim

final_taxon_prediction = pd.DataFrame(columns=["Threshold", "Taxons_Predicted"])

#  compute prediction accuracy at each similarity threshold
for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > threshold).sum()
    acc_val = acc_count/combined_df.shape[0]    
    
    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp
    
final_taxon_prediction
15/78:
# generate list of similarity thresholds
sim_threshold = np.arange(0.1, 1, 0.1)

accuracy_at_different_thresh = []

# compute similarity scores
cosine_sim = []

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        # print(taxon_gt, taxon_predicted, sim)
        
    except ZeroDivisionError:
        cosine_sim.append(0)
        # print(taxon_gt, taxon_predicted,"0")

# append similarity scores to the dataframe
combined_df["Cosine_Similarity"] = cosine_sim

final_taxon_prediction = pd.DataFrame(columns=["Threshold", "Taxons_Predicted", "Taxon_Accuracy_Predicted"])

#  compute prediction accuracy at each similarity threshold
for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > threshold).sum()
    acc_val = acc_count/combined_df.shape[0]    
    
    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp
    
final_taxon_prediction
15/79:
# generate list of similarity thresholds
sim_threshold = np.arange(0.1, 1, 0.1)

accuracy_at_different_thresh = []

# compute similarity scores
cosine_sim = []

for index, row in combined_df.iterrows():
    
    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]
    
    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        # print(taxon_gt, taxon_predicted, sim)
        
    except ZeroDivisionError:
        cosine_sim.append(0)
        # print(taxon_gt, taxon_predicted,"0")

# append similarity scores to the dataframe
combined_df["Cosine_Similarity"] = cosine_sim

final_taxon_prediction = pd.DataFrame(columns=["Threshold", "Taxons_Predicted"])

#  compute prediction accuracy at each similarity threshold
for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > threshold).sum()
    acc_val = acc_count/combined_df.shape[0]    
    
    temp = [threshold, acc_count]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp
    
final_taxon_prediction
15/80: display(combined_df[["Image_Path", "Taxon_Output"]].head())
15/81:
# utility functions for finding cosine similarity

def word2vec(word):
    from collections import Counter
    from math import sqrt

    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the "length" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))

    # return a tuple
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]
15/82:
count = 0
output = []
taxon_output = []

# generate list of similarity thresholds
confidence_threshold = np.arange(0.1, 1, 0.1)

# test "gbif_backbone" linker -> more species here
# use BERT for person and location

for threshold in confidence_threshold:
    
    # predict taxons for text detected from each image
    for index, row in combined_df.iterrows():
        temp = row["Transcription"]

        # construct a single string out of all the detected text
        input_text = " ".join(temp) 
        doc = taxonerd.find_in_text(input_text)

        try:
#             print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
            count+=1

            # append linked taxon entity
            taxon_output.append(str(doc.entity[0][0][1]))

        except AttributeError:
            # append empty strings when no entity is detected
            taxon_output.append("")
        
        combined_df["Taxon_Output"] = taxon_output
        display(combined_df[["Image_Path", "Taxon_Output"]].head())
        
        # generate list of similarity thresholds
        sim_threshold = [0.9]

        # compute similarity scores
        cosine_sim = []

        for index, row in combined_df.iterrows():

            # extract image name from the dataframe
            img_name = row["Image_Path"].split("/")[-1][:-4]
            taxon_predicted = row["Taxon_Output"]
            taxon_gt = Taxon_truth[img_name]

            # compute cosine similarity between the predicted taxon and ground truth
            try:
                sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
                cosine_sim.append(sim)
                # print(taxon_gt, taxon_predicted, sim)

            except ZeroDivisionError:
                cosine_sim.append(0)
                # print(taxon_gt, taxon_predicted,"0")

        # append similarity scores to the dataframe
        combined_df["Cosine_Similarity"] = cosine_sim

        final_taxon_prediction = pd.DataFrame(columns=["Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

        #  compute prediction accuracy at each similarity threshold
#         for threshold in sim_threshold:
        acc_count = (combined_df["Cosine_Similarity"] > threshold).sum()
        acc_val = acc_count/combined_df.shape[0]    

        temp = [threshold, acc_count, acc_val]
        final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

        display(final_taxon_prediction)
15/83:
count = 0
output = []
taxon_output = []

# generate list of similarity thresholds
confidence_threshold = np.arange(0.1, 1, 0.1)

# test "gbif_backbone" linker -> more species here
# use BERT for person and location

for threshold in confidence_threshold:
    
    # predict taxons for text detected from each image
    for index, row in combined_df.iterrows():
        temp = row["Transcription"]

        # construct a single string out of all the detected text
        input_text = " ".join(temp) 
        doc = taxonerd.find_in_text(input_text)

        try:
#             print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
            count+=1

            # append linked taxon entity
            taxon_output.append(str(doc.entity[0][0][1]))

        except AttributeError:
            # append empty strings when no entity is detected
            taxon_output.append("")
        
        combined_df["Taxon_Output"] = taxon_output
        display(combined_df[["Image_Path", "Taxon_Output"]].head())
        
    # generate list of similarity thresholds
    sim_threshold = [0.9]

    # compute similarity scores
    cosine_sim = []

    for index, row in combined_df.iterrows():

        # extract image name from the dataframe
        img_name = row["Image_Path"].split("/")[-1][:-4]
        taxon_predicted = row["Taxon_Output"]
        taxon_gt = Taxon_truth[img_name]

        # compute cosine similarity between the predicted taxon and ground truth
        try:
            sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
            cosine_sim.append(sim)
            # print(taxon_gt, taxon_predicted, sim)

        except ZeroDivisionError:
            cosine_sim.append(0)
            # print(taxon_gt, taxon_predicted,"0")

    # append similarity scores to the dataframe
    combined_df["Cosine_Similarity"] = cosine_sim

    final_taxon_prediction = pd.DataFrame(columns=["Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

    #  compute prediction accuracy at each similarity threshold
#         for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > threshold).sum()
    acc_val = acc_count/combined_df.shape[0]    

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

    display(final_taxon_prediction)
15/84:
count = 0
output = []
taxon_output = []

# generate list of similarity thresholds
confidence_threshold = np.arange(0.1, 1, 0.1)

# test "gbif_backbone" linker -> more species here
# use BERT for person and location

for threshold in confidence_threshold:
    
    nlp = taxonerd.load(
        model="en_core_eco_biobert", 
        linker="ncbi_taxonomy", 
        threshold=threshold
    )
    
    # predict taxons for text detected from each image
    for index, row in combined_df.iterrows():
        temp = row["Transcription"]

        # construct a single string out of all the detected text
        input_text = " ".join(temp) 
        doc = taxonerd.find_in_text(input_text)

        try:
#             print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
            count+=1

            # append linked taxon entity
            taxon_output.append(str(doc.entity[0][0][1]))

        except AttributeError:
            # append empty strings when no entity is detected
            taxon_output.append("")
        
    combined_df["Taxon_Output"] = taxon_output
    display(combined_df[["Image_Path", "Taxon_Output"]].head())
        
    # generate list of similarity thresholds
    sim_threshold = [0.9]

    # compute similarity scores
    cosine_sim = []

    for index, row in combined_df.iterrows():

        # extract image name from the dataframe
        img_name = row["Image_Path"].split("/")[-1][:-4]
        taxon_predicted = row["Taxon_Output"]
        taxon_gt = Taxon_truth[img_name]

        # compute cosine similarity between the predicted taxon and ground truth
        try:
            sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
            cosine_sim.append(sim)
            # print(taxon_gt, taxon_predicted, sim)

        except ZeroDivisionError:
            cosine_sim.append(0)
            # print(taxon_gt, taxon_predicted,"0")

    # append similarity scores to the dataframe
    combined_df["Cosine_Similarity"] = cosine_sim

    final_taxon_prediction = pd.DataFrame(columns=["Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

    #  compute prediction accuracy at each similarity threshold
#         for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > threshold).sum()
    acc_val = acc_count/combined_df.shape[0]    

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

    display(final_taxon_prediction)
15/85: taxonerd = TaxoNERD(prefer_gpu=False) # set to "true" if GPU is accessible
15/86:
count = 0
output = []
taxon_output = []

# generate list of similarity thresholds
confidence_threshold = np.arange(0.1, 1, 0.1)

# test "gbif_backbone" linker -> more species here
# use BERT for person and location

for threshold in confidence_threshold:
    
    nlp = taxonerd.load(
        model="en_core_eco_biobert", 
        linker="ncbi_taxonomy", 
        threshold=threshold
    )
    
    # predict taxons for text detected from each image
    for index, row in combined_df.iterrows():
        temp = row["Transcription"]

        # construct a single string out of all the detected text
        input_text = " ".join(temp) 
        doc = taxonerd.find_in_text(input_text)

        try:
#             print("Linked name: ", doc.entity[0][0][1], " -> Confidence: ", doc.entity[0][0][2])
            count+=1

            # append linked taxon entity
            taxon_output.append(str(doc.entity[0][0][1]))

        except AttributeError:
            # append empty strings when no entity is detected
            taxon_output.append("")
        
    combined_df["Taxon_Output"] = taxon_output
#     display(combined_df[["Image_Path", "Taxon_Output"]].head())
        
    # generate list of similarity thresholds
    sim_threshold = [0.9]

    # compute similarity scores
    cosine_sim = []

    for index, row in combined_df.iterrows():

        # extract image name from the dataframe
        img_name = row["Image_Path"].split("/")[-1][:-4]
        taxon_predicted = row["Taxon_Output"]
        taxon_gt = Taxon_truth[img_name]

        # compute cosine similarity between the predicted taxon and ground truth
        try:
            sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
            cosine_sim.append(sim)
            # print(taxon_gt, taxon_predicted, sim)

        except ZeroDivisionError:
            cosine_sim.append(0)
            # print(taxon_gt, taxon_predicted,"0")

    # append similarity scores to the dataframe
    combined_df["Cosine_Similarity"] = cosine_sim

    final_taxon_prediction = pd.DataFrame(columns=["Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

    #  compute prediction accuracy at each similarity threshold
#         for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > 0.9).sum()
    acc_val = acc_count/combined_df.shape[0]    

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

    display(final_taxon_prediction)
15/87:
count = 0
output = []
taxon_output = []

# generate list of similarity thresholds
confidence_threshold = np.arange(0.1, 1, 0.1)

# test "gbif_backbone" linker -> more species here
# use BERT for person and location

for threshold in confidence_threshold:
    
    nlp = taxonerd.load(
        model="en_core_eco_biobert", 
        linker="ncbi_taxonomy", 
        threshold=threshold
    )
    
    # predict taxons for text detected from each image
    for index, row in combined_df.iterrows():
        temp = row["Transcription"]

        # construct a single string out of all the detected text
        input_text = " ".join(temp) 
        doc = taxonerd.find_in_text(input_text)

        try:
            # append linked taxon entity
            taxon_output.append(str(doc.entity[0][0][1]))

        except AttributeError:
            # append empty strings when no entity is detected
            taxon_output.append("")
        
    combined_df["Taxon_Output"] = taxon_output
        
    # generate list of similarity thresholds
    sim_threshold = [0.9]

    # compute similarity scores
    cosine_sim = []

    for index, row in combined_df.iterrows():

        # extract image name from the dataframe
        img_name = row["Image_Path"].split("/")[-1][:-4]
        taxon_predicted = row["Taxon_Output"]
        taxon_gt = Taxon_truth[img_name]

        # compute cosine similarity between the predicted taxon and ground truth
        try:
            sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
            cosine_sim.append(sim)
            # print(taxon_gt, taxon_predicted, sim)

        except ZeroDivisionError:
            cosine_sim.append(0)
            # print(taxon_gt, taxon_predicted,"0")

    # append similarity scores to the dataframe
    combined_df["Cosine_Similarity"] = cosine_sim

    final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

    #  compute prediction accuracy at each similarity threshold
        # for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > 0.9).sum()
    acc_val = acc_count/combined_df.shape[0]    

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

    display(final_taxon_prediction)
15/88:
count = 0
output = []
taxon_output = []

# generate list of similarity thresholds
confidence_threshold = np.arange(0.1, 1, 0.1)

# test "gbif_backbone" linker -> more species here
# use BERT for person and location information

for threshold in confidence_threshold:
    
    nlp = taxonerd.load(
        model="en_core_eco_biobert", 
        linker="ncbi_taxonomy", 
        threshold=threshold
    )
    
    # predict taxons for text detected from each image
    for index, row in combined_df.iterrows():
        temp = row["Transcription"]

        # construct a single string out of all the detected text
        input_text = " ".join(temp) 
        doc = taxonerd.find_in_text(input_text)

        try:
            # append linked taxon entity
            taxon_output.append(str(doc.entity[0][0][1]))

        except AttributeError:
            # append empty strings when no entity is detected
            taxon_output.append("")
        
    combined_df["Taxon_Output"] = taxon_output
        
    # generate list of similarity thresholds
    sim_threshold = [0.9]

    # compute similarity scores
    cosine_sim = []

    for index, row in combined_df.iterrows():

        # extract image name from the dataframe
        img_name = row["Image_Path"].split("/")[-1][:-4]
        taxon_predicted = row["Taxon_Output"]
        taxon_gt = Taxon_truth[img_name]

        # compute cosine similarity between the predicted taxon and ground truth
        try:
            sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
            cosine_sim.append(sim)
            # print(taxon_gt, taxon_predicted, sim)

        except ZeroDivisionError:
            cosine_sim.append(0)
            # print(taxon_gt, taxon_predicted,"0")

    # append similarity scores to the dataframe
    combined_df["Cosine_Similarity"] = cosine_sim

    final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

    #  compute prediction accuracy at each similarity threshold
        # for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > 0.9).sum()
    acc_val = acc_count/combined_df.shape[0]    

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
15/89:
# generate list of similarity thresholds
confidence_threshold = np.arange(0.1, 1, 0.1)

# test "gbif_backbone" linker -> more species here
# use BERT for person and location information

for threshold in confidence_threshold:
    taxon_output = []
    
    nlp = taxonerd.load(
        model="en_core_eco_biobert", 
        linker="ncbi_taxonomy", 
        threshold=threshold
    )
    
    # predict taxons for text detected from each image
    for index, row in combined_df.iterrows():
        temp = row["Transcription"]

        # construct a single string out of all the detected text
        input_text = " ".join(temp) 
        doc = taxonerd.find_in_text(input_text)

        try:
            # append linked taxon entity
            taxon_output.append(str(doc.entity[0][0][1]))

        except AttributeError:
            # append empty strings when no entity is detected
            taxon_output.append("")
        
    combined_df["Taxon_Output"] = taxon_output
        
    # generate list of similarity thresholds
    sim_threshold = [0.9]

    # compute similarity scores
    cosine_sim = []

    for index, row in combined_df.iterrows():

        # extract image name from the dataframe
        img_name = row["Image_Path"].split("/")[-1][:-4]
        taxon_predicted = row["Taxon_Output"]
        taxon_gt = Taxon_truth[img_name]

        # compute cosine similarity between the predicted taxon and ground truth
        try:
            sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
            cosine_sim.append(sim)
            # print(taxon_gt, taxon_predicted, sim)

        except ZeroDivisionError:
            cosine_sim.append(0)
            # print(taxon_gt, taxon_predicted,"0")

    # append similarity scores to the dataframe
    combined_df["Cosine_Similarity"] = cosine_sim

    final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

    #  compute prediction accuracy at each similarity threshold
        # for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > 0.9).sum()
    acc_val = acc_count/combined_df.shape[0]    

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/1:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
import transformers
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import requests 
import torch
import os, random
from PIL import Image,ImageFilter
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets
from torch.utils.data import Dataset
from tqdm import tqdm
import pandas as pd

import numpy as np
import imghdr
import pickle
from pathlib import Path
import cv2
import torch.nn.functional as F
import multiprocessing
from functools import partial
import json

import matplotlib.pyplot as plt
import warnings
import time

import trocr
import matching
import predictions
import results

from taxonerd import TaxoNERD
16/2:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/usr4/ds549/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc

# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
# ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
ALL_TAXON_FILE = '/usr4/ds549/kabilanm/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
16/3:
count = 0
for filename in tqdm(sorted(os.listdir(workdir))):
    count +=1

print(count)
16/4:
# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft,export_extra = False, text_threshold = .7,link_threshold = .4, crop_type="poly",low_text = .3,cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
    
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
16/5:
# Deleting empty folders, which occurs if some of the images get no segementation from CRAFT
root = output_dir_craft
folders = list(os.walk(root))[1:]
deleted = []
for folder in folders:
    if not folder[2]:
        deleted.append(folder)
        os.rmdir(folder[0])
        
# Setting up the Tr-OCR model and processor
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten") 
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten").to(device)

# Use all available gpu's
model_gpu= nn.DataParallel(model,list(range(torch.cuda.device_count()))).to(device)

# Dataloader for working with gpu's
trainset = datasets.ImageFolder(output_dir_craft, transform = processor)
testloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=False)

# For matching words to image
filenames = [s.replace('_crops', '') for s in list(trainset.class_to_idx)]

# For matching the image name with the label name
word_log_dic = {k: v for k,v in enumerate(filenames)}
# For matching the image name with the transriptions
words_identified = {k: [] for v,k in enumerate(filenames)}
16/6:
# Save filenames
with open(save_dir+'filenames.txt', 'w') as fp:
    for item in filenames:
        # write each item on a new line
        fp.write("%s\n" % item)
# Save word_log_dic 
with open(save_dir+'word_log_dic.json', 'w') as fp:
    json.dump(word_log_dic, fp)
# Save words_identified
with open(save_dir+'words_identified.json', 'w') as fp:
    json.dump(words_identified, fp)
16/7:
#Storing the outputs
results,confidence,labels = trocr.evaluate_craft_seg(model,processor, words_identified,word_log_dic,testloader,device)
#Saving all the outputs in dataframe
df = pd.DataFrame(list(zip(results,confidence,labels)),columns = ['Results','Confidence','Labels'])
df.to_pickle(save_dir+'full_results.pkl')
16/8:
# First part of final csv with results, confidence level from tr-ocr, and label
combined_df = trocr.combine_by_label(df)

# Adding the image path and all bounding boxes 

df_dictionary = pd.DataFrame(boxes.items(), columns=['Image_Path', 'Bounding_Boxes'])
combined_df = pd.concat([combined_df, df_dictionary], axis=1, join='inner')
display(combined_df.head())
16/9:
#Save intermediate file
combined_df.to_pickle(save_dir+'/test.pkl')
16/10:
# Reading in the ground truth values

gt_t = workdir2+'/taxon_gt.txt'
Taxon_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_t) }

gt_g = workdir2+'/geography_gt.txt'
Geography_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_g) }

gt_c = workdir2+'/collector_gt.txt'
Collector_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_c) }

comparison_file = {"Taxon":Taxon_truth,"Countries":Geography_truth,"Collector":Collector_truth}
16/11:
Taxon_truth_sample = {k: Taxon_truth[k] for k in list(Taxon_truth)[:10]}

# view subset of the taxon truth
print(Taxon_truth_sample)
16/12: taxonerd = TaxoNERD(prefer_gpu=False) # set to "true" if GPU is accessible
16/13:
# utility functions for finding cosine similarity

def word2vec(word):
    from collections import Counter
    from math import sqrt

    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the "length" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))

    # return a tuple
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]
16/14:
# generate list of similarity thresholds
confidence_threshold = np.arange(0.1, 1, 0.1)
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

# test "gbif_backbone" linker -> more species here
# use BERT for person and location information

for threshold in confidence_threshold:
    taxon_output = []
    
    nlp = taxonerd.load(
        model="en_core_eco_biobert", 
        linker="ncbi_taxonomy", 
        threshold=threshold
    )
    
    # predict taxons for text detected from each image
    for index, row in combined_df.iterrows():
        temp = row["Transcription"]

        # construct a single string out of all the detected text
        input_text = " ".join(temp) 
        doc = taxonerd.find_in_text(input_text)

        try:
            # append linked taxon entity
            taxon_output.append(str(doc.entity[0][0][1]))

        except AttributeError:
            # append empty strings when no entity is detected
            taxon_output.append("")
        
    combined_df["Taxon_Output"] = taxon_output
        
    # generate list of similarity thresholds
    sim_threshold = [0.9]

    # compute similarity scores
    cosine_sim = []

    for index, row in combined_df.iterrows():

        # extract image name from the dataframe
        img_name = row["Image_Path"].split("/")[-1][:-4]
        taxon_predicted = row["Taxon_Output"]
        taxon_gt = Taxon_truth[img_name]

        # compute cosine similarity between the predicted taxon and ground truth
        try:
            sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
            cosine_sim.append(sim)
            # print(taxon_gt, taxon_predicted, sim)

        except ZeroDivisionError:
            cosine_sim.append(0)
            # print(taxon_gt, taxon_predicted,"0")

    # append similarity scores to the dataframe
    combined_df["Cosine_Similarity"] = cosine_sim

    #  compute prediction accuracy at each similarity threshold
        # for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > 0.9).sum()
    acc_val = acc_count/combined_df.shape[0]    

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/15:
# generate list of similarity thresholds
confidence_threshold = np.arange(0, 1, 0.1)
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

# test "gbif_backbone" linker -> more species here
# use BERT for person and location information

for threshold in confidence_threshold:
    taxon_output = []
    
    nlp = taxonerd.load(
        model="en_core_eco_biobert", 
        linker="ncbi_taxonomy", 
        threshold=threshold
    )
    
    # predict taxons for text detected from each image
    for index, row in combined_df.iterrows():
        temp = row["Transcription"]

        # construct a single string out of all the detected text
        input_text = " ".join(temp) 
        doc = taxonerd.find_in_text(input_text)

        try:
            # append linked taxon entity
            taxon_output.append(str(doc.entity[0][0][1]))

        except AttributeError:
            # append empty strings when no entity is detected
            taxon_output.append("")
        
    combined_df["Taxon_Output"] = taxon_output
        
    # generate list of similarity thresholds
    sim_threshold = [0.9]

    # compute similarity scores
    cosine_sim = []

    for index, row in combined_df.iterrows():

        # extract image name from the dataframe
        img_name = row["Image_Path"].split("/")[-1][:-4]
        taxon_predicted = row["Taxon_Output"]
        taxon_gt = Taxon_truth[img_name]

        # compute cosine similarity between the predicted taxon and ground truth
        try:
            sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
            cosine_sim.append(sim)
            # print(taxon_gt, taxon_predicted, sim)

        except ZeroDivisionError:
            cosine_sim.append(0)
            # print(taxon_gt, taxon_predicted,"0")

    # append similarity scores to the dataframe
    combined_df["Cosine_Similarity"] = cosine_sim

    #  compute prediction accuracy at each similarity threshold
        # for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > 0.5).sum()
    acc_val = acc_count/combined_df.shape[0]    

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/16: combined_df["Taxon_Output"]
16/17: combined_df[["Taxon_Output"]]
16/18: combined_df
16/19: combined_df[["Taxon_Output"]].str.match('')
16/20: combined_df[combined_df["Taxon_Output"]].str.match('')
16/21: print(taxon_output)
16/22: combined_df[combined_df["Taxon_Output"].str.match('')]
16/23: len(combined_df[combined_df["Taxon_Output"].str.match('')])
16/24: len(combined_df[combined_df["Taxon_Output"].str.match("")])
16/25: len(combined_df["Taxon_Output"].str.match(""))
16/26: len(combined_df[["Taxon_Output"]].str.match(""))
16/27: len(combined_df["Taxon_Output"].str.match(""))
16/28: print(taxon_output)
16/29: combined_df["Taxon_Output"]
16/30: len(combined_df["Taxon_Output"].obj.match(""))
16/31: print(taxon_output)
16/32: sum(x for x in taxon_output if x == "")
16/33: len([x for x in taxon_output if x == ""])
16/34: len([x for x in taxon_output if x == ""])
16/35:
# generate list of similarity thresholds
confidence_threshold = np.arange(0, 1, 0.1)
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

# test "gbif_backbone" linker -> more species here
# use BERT for person and location information

for threshold in confidence_threshold:
    taxon_output = []
    
    nlp = taxonerd.load(
        model="en_core_eco_biobert", 
        linker="ncbi_taxonomy", 
        threshold=threshold
    )
    
    # predict taxons for text detected from each image
    for index, row in combined_df.iterrows():
        temp = row["Transcription"]

        # construct a single string out of all the detected text
        input_text = " ".join(temp) 
        doc = taxonerd.find_in_text(input_text)

        try:
            # append linked taxon entity
            taxon_output.append(str(doc.entity[0][0][1]))

        except AttributeError:
            # append empty strings when no entity is detected
            taxon_output.append("")
        
    combined_df["Taxon_Output"] = taxon_output
        
    # generate list of similarity thresholds
    # sim_threshold = [0.9]

    # compute similarity scores
    cosine_sim = []

    for index, row in combined_df.iterrows():

        # extract image name from the dataframe
        img_name = row["Image_Path"].split("/")[-1][:-4]
        taxon_predicted = row["Taxon_Output"]
        taxon_gt = Taxon_truth[img_name]

        # compute cosine similarity between the predicted taxon and ground truth
        try:
            sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
            cosine_sim.append(sim)
            # print(taxon_gt, taxon_predicted, sim)

        except ZeroDivisionError:
            cosine_sim.append(0)
            # print(taxon_gt, taxon_predicted,"0")

    # append similarity scores to the dataframe
    combined_df["Cosine_Similarity"] = cosine_sim

    #  compute prediction accuracy at each similarity threshold
        # for threshold in sim_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > 0.8).sum()
    
    num_empty_strings = len([x for x in taxon_output if x == ""])
    acc_val = acc_count/(combined_df.shape[0] - num_empty_strings)  

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/36:
# generate list of similarity thresholds
confidence_threshold = np.arange(0, 1, 0.1)
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

# test "gbif_backbone" linker -> more species here
# use BERT for person and location information

# for threshold in confidence_threshold:


taxon_output = []

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="ncbi_taxonomy", 
    threshold=0
)

# predict taxons for text detected from each image
for index, row in combined_df.iterrows():
    temp = row["Transcription"]

    # construct a single string out of all the detected text
    input_text = " ".join(temp) 
    doc = taxonerd.find_in_text(input_text)

    try:
        # append linked taxon entity
        taxon_output.append(str(doc.entity[0][0][1]))

    except AttributeError:
        # append empty strings when no entity is detected
        taxon_output.append("")

combined_df["Taxon_Output"] = taxon_output
16/37:
# generate list of similarity thresholds
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

# test "gbif_backbone" linker -> more species here
# use BERT for person and location information

taxon_output = []
confidence_output = []

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="ncbi_taxonomy", 
    threshold=0
)

# predict taxons for text detected from each image
for index, row in combined_df.iterrows()[:10]:
    temp = row["Transcription"]

    # construct a single string out of all the detected text
    input_text = " ".join(temp) 
    doc = taxonerd.find_in_text(input_text)

    try:
        # append linked taxon entity
        taxon_output.append(str(doc.entity[0][0][1]))
        confidence_output.append(float(doc.entity[0][0][2]))

    except AttributeError:
        # append empty strings when no entity is detected
        taxon_output.append("")
        confidence_output.append(float(0))

combined_df["Taxon_Output"] = taxon_output
16/38:
# generate list of similarity thresholds
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

# test "gbif_backbone" linker -> more species here
# use BERT for person and location information

taxon_output = []
confidence_output = []

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="ncbi_taxonomy", 
    threshold=0
)
16/39:
# predict taxons for text detected from each image
for index, row in combined_df.iterrows()[:10]:
    temp = row["Transcription"]

    # construct a single string out of all the detected text
    input_text = " ".join(temp) 
    doc = taxonerd.find_in_text(input_text)

    try:
        # append linked taxon entity
        taxon_output.append(str(doc.entity[0][0][1]))
        confidence_output.append(float(doc.entity[0][0][2]))

    except AttributeError:
        # append empty strings when no entity is detected
        taxon_output.append("")
        confidence_output.append(float(0))

combined_df["Taxon_Output"] = taxon_output
16/40:
count = 0

# predict taxons for text detected from each image
for index, row in combined_df.iterrows():
    count += 1
    if count == 10: break
        
    temp = row["Transcription"]

    # construct a single string out of all the detected text
    input_text = " ".join(temp) 
    doc = taxonerd.find_in_text(input_text)

    try:
        # append linked taxon entity
        taxon_output.append(str(doc.entity[0][0][1]))
        confidence_output.append(float(doc.entity[0][0][2]))

    except AttributeError:
        # append empty strings when no entity is detected
        taxon_output.append("")
        confidence_output.append(float(0))

combined_df["Taxon_Output"] = taxon_output
16/41:
combined_df["Taxon_Output"] = taxon_output + [0]*240
combined_df.head()
16/42:
combined_df["Taxon_Output"] = taxon_output + [0]*240
combined_df["Confidence_Output"] = confidence_output + [""]*240
combined_df.head()
16/43:
# predict taxons for text detected from each image
for index, row in combined_df.iterrows():
    temp = row["Transcription"]

    # construct a single string out of all the detected text
    input_text = " ".join(temp) 
    doc = taxonerd.find_in_text(input_text)

    try:
        # append linked taxon entity
        taxon_output.append(str(doc.entity[0][0][1]))
        confidence_output.append(float(doc.entity[0][0][2]))

    except AttributeError:
        # append empty strings when no entity is detected
        taxon_output.append("")
        confidence_output.append(float(0))

combined_df["Taxon_Output"] = taxon_output
combined_df["Confidence_Output"] = confidence_output
16/44:
combined_df["Taxon_Output"] = taxon_output[9:]
combined_df["Confidence_Output"] = confidence_output[9:]
16/45: combined_df.head()
16/46:
confidence_output = confidence_output[9:]
taxon_output = taxon_output[9:]
16/47:
combined_df["Taxon_Output"] = taxon_output
combined_df["Confidence_Output"] = confidence_output
16/48: combined_df.head()
16/49:
# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# generate list of similarity thresholds
# sim_threshold = [0.9]

# array to store computed similarity scores
cosine_sim = []

for index, row in combined_df.iterrows():

    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]

    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        # print(taxon_gt, taxon_predicted, sim)

    except ZeroDivisionError:
        cosine_sim.append(0)
        # print(taxon_gt, taxon_predicted,"0")

# append similarity scores to the dataframe
combined_df["Cosine_Similarity"] = cosine_sim
16/50: combined_df[(combined_df["Cosine_Similarity"] > 0.8) & (combined_df["Confidence_Output"] > 0.8)]
16/51: len(combined_df[(combined_df["Cosine_Similarity"] > 0.8) & (combined_df["Confidence_Output"] > 0.8)])
16/52:
#  compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > 0.8).sum()

    num_empty_strings = len(combined_df[(combined_df["Cosine_Similarity"] > 0.8) & 
                                        (combined_df["Confidence_Output"] > threshold)])
    acc_val = acc_count/(combined_df.shape[0] - num_empty_strings)  

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

    display(final_taxon_prediction)
16/53:
#  compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > 0.8).sum()

    num_empty_strings = len(combined_df[(combined_df["Cosine_Similarity"] > 0.8) & 
                                        (combined_df["Confidence_Output"] > threshold)])
    acc_val = acc_count/(combined_df.shape[0] - num_empty_strings)  

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/54:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > 0.8).sum()

    num_empty_strings = len(combined_df[(combined_df["Cosine_Similarity"] > 0.8) & 
                                        (combined_df["Confidence_Output"] > threshold)])
    acc_val = acc_count/(combined_df.shape[0] - num_empty_strings)  

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/55:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > 0.8).sum()

    num_empty_strings = len(combined_df[(combined_df["Cosine_Similarity"] > 0.8) & 
                                        (combined_df["Confidence_Output"] > threshold)])
    acc_val = acc_count/(combined_df.shape[0] - num_empty_strings)  

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/56:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > 0.8).sum()

    num_empty_strings = len(combined_df[(combined_df["Cosine_Similarity"] > 0.9) & 
                                        (combined_df["Confidence_Output"] > threshold)])
    acc_val = acc_count/(combined_df.shape[0] - num_empty_strings)  

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/57:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    acc_count = (combined_df["Cosine_Similarity"] > 0.8).sum()

    num_empty_strings = len(combined_df[(combined_df["Cosine_Similarity"] > 0.5) & 
                                        (combined_df["Confidence_Output"] > threshold)])
    acc_val = acc_count/(combined_df.shape[0] - num_empty_strings)  

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/58:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.8

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] >= threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] >= sim_threshold).sum()

    num_empty_strings = len(temp_df)
    acc_val = acc_count/(combined_df.shape[0] - num_empty_strings)  

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/59:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.5

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] >= threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] >= sim_threshold).sum()

    num_empty_strings = len(temp_df)
    acc_val = acc_count/(combined_df.shape[0] - num_empty_strings)  

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/60:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.5

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    num_empty_strings = len(temp_df)
    acc_val = acc_count/(combined_df.shape[0] - num_empty_strings)  

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/61:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.5

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/62:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.9

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/63:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.8

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/64:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.6

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/65:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.65

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/66:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 1

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/67:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/68:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.9

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/69:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.75

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/70:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.78

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/71:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.8

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/72:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.8

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/73:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.8

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for conf_threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > conf_threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/74:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.9

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for conf_threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > conf_threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/75:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.9

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for conf_threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > conf_threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [conf_threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
16/76:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.8

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for conf_threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > conf_threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [conf_threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
17/1:
#Imports and installs
# import transformers
# from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
# # import requests 
# import torch
# import os
# from PIL import Image,ImageFilter
# from torch.utils.data import DataLoader
# import torch
# import torch.nn as nn
# import torch.optim as optim
# import torchvision
# from torchvision import datasets
# from torch.utils.data import Dataset
# from tqdm import tqdm
# import pandas as pd

# import numpy as np
# import imghdr
# import pickle
# from pathlib import Path
# import cv2
# import torch.nn.functional as F
# import multiprocessing
# from functools import partial
# import json

# import matplotlib.pyplot as plt
# import warnings
# import time

# import trocr

# from taxonerd import TaxoNERD

#Imports and installs
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
# import requests 
import torch
import os
from PIL import Image
import torch
import torch.nn as nn
from torchvision import datasets
from tqdm import tqdm
import pandas as pd

import numpy as np
import json

import warnings

import trocr

from taxonerd import TaxoNERD
17/2:
#Imports and installs
# import transformers
# from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
# # import requests 
# import torch
# import os
# from PIL import Image,ImageFilter
# from torch.utils.data import DataLoader
# import torch
# import torch.nn as nn
# import torch.optim as optim
# import torchvision
# from torchvision import datasets
# from torch.utils.data import Dataset
# from tqdm import tqdm
# import pandas as pd

# import numpy as np
# import imghdr
# import pickle
# from pathlib import Path
# import cv2
# import torch.nn.functional as F
# import multiprocessing
# from functools import partial
# import json

# import matplotlib.pyplot as plt
# import warnings
# import time

# import trocr

# from taxonerd import TaxoNERD

#Imports and installs
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
# import requests 
import torch
import os
from PIL import Image
import torch
import torch.nn as nn
from torchvision import datasets
from tqdm import tqdm
import pandas as pd

import numpy as np
import json

import warnings

import trocr

from taxonerd import TaxoNERD
17/3:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/usr4/ds549/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc

# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
# ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
ALL_TAXON_FILE = '/usr4/ds549/kabilanm/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
17/4:
# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft,export_extra = False, text_threshold = .7,link_threshold = .4, crop_type="poly",low_text = .3,cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
    
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
18/1:
#Imports and installs
# import transformers
# from transformers import TrOCRProcessor, VisionEncoderDecoderModel
# from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
# # import requests 
# import torch
# import os
# from PIL import Image,ImageFilter
# from torch.utils.data import DataLoader
# import torch
# import torch.nn as nn
# import torch.optim as optim
# import torchvision
# from torchvision import datasets
# from torch.utils.data import Dataset
# from tqdm import tqdm
# import pandas as pd

# import numpy as np
# import imghdr
# import pickle
# from pathlib import Path
# import cv2
# import torch.nn.functional as F
# import multiprocessing
# from functools import partial
# import json

# import matplotlib.pyplot as plt
# import warnings
# import time

# import trocr

# from taxonerd import TaxoNERD

#Imports and installs
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
# import requests 
import torch
import os
from PIL import Image
import torch
import torch.nn as nn
from torchvision import datasets
from tqdm import tqdm
import pandas as pd

import numpy as np
import json

import warnings

import trocr

from taxonerd import TaxoNERD
18/2:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/usr4/ds549/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc

# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
# ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
ALL_TAXON_FILE = '/usr4/ds549/kabilanm/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
18/3:
# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft,export_extra = False, text_threshold = .7,link_threshold = .4, crop_type="poly",low_text = .3,cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
    
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
18/4:
# Deleting empty folders, which occurs if some of the images get no segementation from CRAFT
root = output_dir_craft
folders = list(os.walk(root))[1:]
deleted = []
for folder in folders:
    if not folder[2]:
        deleted.append(folder)
        os.rmdir(folder[0])
        
# Setting up the Tr-OCR model and processor
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten") 
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten").to(device)

# Use all available gpu's
model_gpu= nn.DataParallel(model,list(range(torch.cuda.device_count()))).to(device)

# Dataloader for working with gpu's
trainset = datasets.ImageFolder(output_dir_craft, transform = processor)
testloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=False)

# For matching words to image
filenames = [s.replace('_crops', '') for s in list(trainset.class_to_idx)]

# For matching the image name with the label name
word_log_dic = {k: v for k,v in enumerate(filenames)}
# For matching the image name with the transriptions
words_identified = {k: [] for v,k in enumerate(filenames)}
18/5:
# Save filenames
with open(save_dir+'filenames.txt', 'w') as fp:
    for item in filenames:
        # write each item on a new line
        fp.write("%s\n" % item)
# Save word_log_dic 
with open(save_dir+'word_log_dic.json', 'w') as fp:
    json.dump(word_log_dic, fp)
# Save words_identified
with open(save_dir+'words_identified.json', 'w') as fp:
    json.dump(words_identified, fp)
18/6:
#Storing the outputs
results,confidence,labels = trocr.evaluate_craft_seg(model,processor, words_identified,word_log_dic,testloader,device)
#Saving all the outputs in dataframe
df = pd.DataFrame(list(zip(results,confidence,labels)),columns = ['Results','Confidence','Labels'])
df.to_pickle(save_dir+'full_results.pkl')
18/7:
# First part of final csv with results, confidence level from tr-ocr, and label
combined_df = trocr.combine_by_label(df)

# Adding the image path and all bounding boxes 

df_dictionary = pd.DataFrame(boxes.items(), columns=['Image_Path', 'Bounding_Boxes'])
combined_df = pd.concat([combined_df, df_dictionary], axis=1, join='inner')
display(combined_df.head())
18/8:
#Save intermediate file
combined_df.to_pickle(save_dir+'/test.pkl')
18/9:
# Reading in the ground truth values

gt_t = workdir2+'/taxon_gt.txt'
Taxon_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_t) }

gt_g = workdir2+'/geography_gt.txt'
Geography_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_g) }

gt_c = workdir2+'/collector_gt.txt'
Collector_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_c) }

comparison_file = {"Taxon":Taxon_truth,"Countries":Geography_truth,"Collector":Collector_truth}
18/10:
Taxon_truth_sample = {k: Taxon_truth[k] for k in list(Taxon_truth)[:10]}

# view subset of the taxon truth
print(Taxon_truth_sample)
18/11: taxonerd = TaxoNERD(prefer_gpu=False) # set to "true" if GPU is accessible
18/12:
# utility functions for finding cosine similarity

def word2vec(word):
    from collections import Counter
    from math import sqrt

    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the "length" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))

    # return a tuple
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]
18/13:
# test "gbif_backbone" linker -> more species here
# use BERT for person and location information

taxon_output = []
confidence_output = []

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="ncbi_taxonomy", 
    threshold=0
)
18/14:
# predict taxons for text detected from each image
for index, row in combined_df.iterrows():
    temp = row["Transcription"]

    # construct a single string out of all the detected text
    input_text = " ".join(temp) 
    doc = taxonerd.find_in_text(input_text)

    try:
        # append linked taxon entity and confidence
        taxon_output.append(str(doc.entity[0][0][1]))
        confidence_output.append(float(doc.entity[0][0][2]))

    except AttributeError:
        # append empty strings when no entity is detected
        taxon_output.append("")
        confidence_output.append(float(0))
18/15:
# append predicted taxon and confidence scores to the dataframe
combined_df["Taxon_Output"] = taxon_output
combined_df["Confidence_Output"] = confidence_output
18/16: combined_df.head()
18/17:
# array to store computed similarity scores
cosine_sim = []

for index, row in combined_df.iterrows():

    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]

    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        # print(taxon_gt, taxon_predicted, sim)

    except ZeroDivisionError:
        cosine_sim.append(0)
        # print(taxon_gt, taxon_predicted,"0")

# append similarity scores to the dataframe
combined_df["Cosine_Similarity"] = cosine_sim
18/18:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.8

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for conf_threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > conf_threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [conf_threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
19/1:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import torch
import os
from PIL import Image
import torch
import torch.nn as nn
import torchvision
from torchvision import datasets
from tqdm import tqdm
import pandas as pd

import numpy as np
import json

import warnings

import trocr

from taxonerd import TaxoNERD
19/2:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/usr4/ds549/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc

# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
# ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
ALL_TAXON_FILE = '/usr4/ds549/kabilanm/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
19/3:
# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft,export_extra = False, text_threshold = .7,link_threshold = .4, crop_type="poly",low_text = .3,cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
    
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
19/4:
# Deleting empty folders, which occurs if some of the images get no segementation from CRAFT
root = output_dir_craft
folders = list(os.walk(root))[1:]
deleted = []
for folder in folders:
    if not folder[2]:
        deleted.append(folder)
        os.rmdir(folder[0])
        
# Setting up the Tr-OCR model and processor
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten") 
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten").to(device)

# Use all available gpu's
model_gpu= nn.DataParallel(model,list(range(torch.cuda.device_count()))).to(device)

# Dataloader for working with gpu's
trainset = datasets.ImageFolder(output_dir_craft, transform = processor)
testloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=False)

# For matching words to image
filenames = [s.replace('_crops', '') for s in list(trainset.class_to_idx)]

# For matching the image name with the label name
word_log_dic = {k: v for k,v in enumerate(filenames)}
# For matching the image name with the transriptions
words_identified = {k: [] for v,k in enumerate(filenames)}
19/5:
# Save filenames
with open(save_dir+'filenames.txt', 'w') as fp:
    for item in filenames:
        # write each item on a new line
        fp.write("%s\n" % item)
# Save word_log_dic 
with open(save_dir+'word_log_dic.json', 'w') as fp:
    json.dump(word_log_dic, fp)
# Save words_identified
with open(save_dir+'words_identified.json', 'w') as fp:
    json.dump(words_identified, fp)
19/6:
#Storing the outputs
results,confidence,labels = trocr.evaluate_craft_seg(model,processor, words_identified,word_log_dic,testloader,device)
#Saving all the outputs in dataframe
df = pd.DataFrame(list(zip(results,confidence,labels)),columns = ['Results','Confidence','Labels'])
df.to_pickle(save_dir+'full_results.pkl')
19/7:
# First part of final csv with results, confidence level from tr-ocr, and label
combined_df = trocr.combine_by_label(df)

# Adding the image path and all bounding boxes 

df_dictionary = pd.DataFrame(boxes.items(), columns=['Image_Path', 'Bounding_Boxes'])
combined_df = pd.concat([combined_df, df_dictionary], axis=1, join='inner')
display(combined_df.head())
19/8:
#Save intermediate file
combined_df.to_pickle(save_dir+'/test.pkl')
19/9:
# Reading in the ground truth values

gt_t = workdir2+'/taxon_gt.txt'
Taxon_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_t) }

gt_g = workdir2+'/geography_gt.txt'
Geography_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_g) }

gt_c = workdir2+'/collector_gt.txt'
Collector_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_c) }

comparison_file = {"Taxon":Taxon_truth,"Countries":Geography_truth,"Collector":Collector_truth}
19/10:
Taxon_truth_sample = {k: Taxon_truth[k] for k in list(Taxon_truth)[:10]}

# view subset of the taxon truth
print(Taxon_truth_sample)
19/11: taxonerd = TaxoNERD(prefer_gpu=False) # set to "true" if GPU is accessible
19/12:
# utility functions for finding cosine similarity

def word2vec(word):
    from collections import Counter
    from math import sqrt

    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the "length" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))

    # return a tuple
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]
19/13:
# test "gbif_backbone" linker -> more species here
# use BERT for person and location information

taxon_output = []
confidence_output = []

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="ncbi_taxonomy", 
    threshold=0
)
19/14:
# test "gbif_backbone" linker -> more species here
# use BERT for person and location information

taxon_output = []
confidence_output = []

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="ncbi_taxonomy", 
    threshold=0
)
19/15:
# predict taxons for text detected from each image
for index, row in combined_df.iterrows():
    temp = row["Transcription"]

    # construct a single string out of all the detected text
    input_text = " ".join(temp) 
    doc = taxonerd.find_in_text(input_text)

    try:
        # append linked taxon entity and confidence
        taxon_output.append(str(doc.entity[0][0][1]))
        confidence_output.append(float(doc.entity[0][0][2]))

    except AttributeError:
        # append empty strings when no entity is detected
        taxon_output.append("")
        confidence_output.append(float(0))
19/16: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/17: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/18: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/19:
# utility functions for finding cosine similarity

def word2vec(word):
    from collections import Counter
    from math import sqrt

    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the "length" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))

    # return a tuple
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]
19/20: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/21: ! nvcc --version
19/22: ! nvidia-smi
19/23: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/24: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/25: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/26: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/27: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/28: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/29: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/30: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/31: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/32: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/33: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/34: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/35: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/36: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/37: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/38: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/39: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/40: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/41: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/42: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/43: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/44: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/45: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/46: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/47: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/48: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/49: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/50: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/51: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/52: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/53: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/54: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/55: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/56: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/57: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/58: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/59: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/60: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/61: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/62: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/63: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/64: ! pip install cupy
19/65: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/66: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/67: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/68: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/69: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/70: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/71: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/72: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/73: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/74: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/75: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/76: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/77: taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/78:
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/79:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/80:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/81: ! nvcc
19/82: ! /usr/local/cuda/bin/nvcc --version
19/83: ! nvidia-smi
19/84: ! nvcc --version
19/85:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/86:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/87:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/88:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/89:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/90:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/91:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/92:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/93:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/94:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/95:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/96:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/97:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/98:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/99:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/100:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/101:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/102:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/103:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/104:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/105:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/106: ! module load cuda
19/107:
! module load cuda
! nvcc --version
19/108:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/109:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/110:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/111:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/112:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/113:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/114:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/115:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/116:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/117:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/118:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/119:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/120:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/121:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/122:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/123:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/124:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/125:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/126:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/127:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/128:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/129:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/130:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/131:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/132:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/133:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/134:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/135:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/136:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/137:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/138:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/139:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/140:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/141:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/142:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/143:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/144:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/145:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/146:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/147:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/148:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/149:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/150:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/151:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/152:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/153:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/154:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
19/155:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
20/1:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
20/2:
! module load cuda/11.3
! nvcc --version
21/1: ! nvidia-smi
21/2: ! nvcc --version
21/3:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
21/4:
import spacy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
21/5:
import spacy
import cupy
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
21/6:
import spacy
import cupy
print(cupy.__version__)
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
21/7:
import spacy
import cupy
print(cupy.version)
spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
22/1: ! nvcc --version
22/2:
import spacy
import cupy

spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
22/3:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import torch
import os
from PIL import Image
import torch
import torch.nn as nn
import torchvision
from torchvision import datasets
from tqdm import tqdm
import pandas as pd

import numpy as np
import json

import warnings

import trocr

from taxonerd import TaxoNERD
22/4:
import spacy
import cupy

spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
22/5:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import torch
import os
from PIL import Image
import torch
import torch.nn as nn
import torchvision
from torchvision import datasets
from tqdm import tqdm
import pandas as pd

import numpy as np
import json

import warnings

import trocr

from taxonerd import TaxoNERD
22/6:
# Suppressing all the huggingface warnings
SUPPRESS = True
if SUPPRESS:
    from transformers.utils import logging
    logging.set_verbosity(40)
# Turning off this warning, isn't relevant for this application
warnings.filterwarnings("ignore", "(Possibly )?corrupt EXIF data", UserWarning)

# Location of images
workdir = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/' # update this to the desired directory on scc
# Location of the segmentations
output_dir_craft = '/projectnb/sparkgrp/kabilanm/goodfilescraft/'
# Location to save all output files
save_dir = '/usr4/ds549/kabilanm/saved_results/'
# For ground truth labels 
workdir2 = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/scraped-data/drago_testdata/gt_labels' # update this to the desired directory on scc

# Corpus files
ALL_SPECIES_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_species.pkl'
ALL_GENUS_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/corpus_taxon/output/possible_genus.pkl'
# ALL_TAXON_FILE = '/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-new/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
ALL_TAXON_FILE = '/usr4/ds549/kabilanm/ml-herbarium/corpus/corpus_taxon/corpus_taxon.txt'
22/7:
# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft,export_extra = False, text_threshold = .7,link_threshold = .4, crop_type="poly",low_text = .3,cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
    
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
22/8:
# Deleting empty folders, which occurs if some of the images get no segementation from CRAFT
root = output_dir_craft
folders = list(os.walk(root))[1:]
deleted = []
for folder in folders:
    if not folder[2]:
        deleted.append(folder)
        os.rmdir(folder[0])
        
# Setting up the Tr-OCR model and processor
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten") 
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten").to(device)

# Use all available gpu's
model_gpu= nn.DataParallel(model,list(range(torch.cuda.device_count()))).to(device)

# Dataloader for working with gpu's
trainset = datasets.ImageFolder(output_dir_craft, transform = processor)
testloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=False)

# For matching words to image
filenames = [s.replace('_crops', '') for s in list(trainset.class_to_idx)]

# For matching the image name with the label name
word_log_dic = {k: v for k,v in enumerate(filenames)}
# For matching the image name with the transriptions
words_identified = {k: [] for v,k in enumerate(filenames)}
22/9:
# Save filenames
with open(save_dir+'filenames.txt', 'w') as fp:
    for item in filenames:
        # write each item on a new line
        fp.write("%s\n" % item)
# Save word_log_dic 
with open(save_dir+'word_log_dic.json', 'w') as fp:
    json.dump(word_log_dic, fp)
# Save words_identified
with open(save_dir+'words_identified.json', 'w') as fp:
    json.dump(words_identified, fp)
22/10:
#Storing the outputs
results,confidence,labels = trocr.evaluate_craft_seg(model,processor, words_identified,word_log_dic,testloader,device)
#Saving all the outputs in dataframe
df = pd.DataFrame(list(zip(results,confidence,labels)),columns = ['Results','Confidence','Labels'])
df.to_pickle(save_dir+'full_results.pkl')
22/11:
# First part of final csv with results, confidence level from tr-ocr, and label
combined_df = trocr.combine_by_label(df)

# Adding the image path and all bounding boxes 

df_dictionary = pd.DataFrame(boxes.items(), columns=['Image_Path', 'Bounding_Boxes'])
combined_df = pd.concat([combined_df, df_dictionary], axis=1, join='inner')
display(combined_df.head())
22/12:
#Save intermediate file
combined_df.to_pickle(save_dir+'/test.pkl')
22/13:
# Reading in the ground truth values

gt_t = workdir2+'/taxon_gt.txt'
Taxon_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_t) }

gt_g = workdir2+'/geography_gt.txt'
Geography_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_g) }

gt_c = workdir2+'/collector_gt.txt'
Collector_truth = { line.split(":")[0] : line.split(": ")[1].strip() for line in open(gt_c) }

comparison_file = {"Taxon":Taxon_truth,"Countries":Geography_truth,"Collector":Collector_truth}
22/14:
Taxon_truth_sample = {k: Taxon_truth[k] for k in list(Taxon_truth)[:10]}

# view subset of the taxon truth
print(Taxon_truth_sample)
22/15: ! nvidia-smi
22/16: ! nvcc --version
22/17:
import spacy
import cupy

spacy.require_gpu()
taxonerd = TaxoNERD(prefer_gpu=True) # set to "true" if GPU is accessible
22/18:
# utility functions for finding cosine similarity

def word2vec(word):
    from collections import Counter
    from math import sqrt

    # count the characters in word
    cw = Counter(word)
    # precomputes a set of the different characters
    sw = set(cw)
    # precomputes the "length" of the word vector
    lw = sqrt(sum(c*c for c in cw.values()))

    # return a tuple
    return cw, sw, lw

def cosdis(v1, v2):
    # which characters are common to the two words?
    common = v1[1].intersection(v2[1])
    # by definition of cosine distance we have
    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]
22/19:
# test "gbif_backbone" linker -> more species here
# use BERT for person and location information

taxon_output = []
confidence_output = []

nlp = taxonerd.load(
    model="en_core_eco_biobert", 
    linker="ncbi_taxonomy", 
    threshold=0
)
22/20:
# predict taxons for text detected from each image
for index, row in combined_df.iterrows():
    temp = row["Transcription"]

    # construct a single string out of all the detected text
    input_text = " ".join(temp) 
    doc = taxonerd.find_in_text(input_text)

    try:
        # append linked taxon entity and confidence
        taxon_output.append(str(doc.entity[0][0][1]))
        confidence_output.append(float(doc.entity[0][0][2]))

    except AttributeError:
        # append empty strings when no entity is detected
        taxon_output.append("")
        confidence_output.append(float(0))
22/21:
# append predicted taxon and confidence scores to the dataframe
combined_df["Taxon_Output"] = taxon_output
combined_df["Confidence_Output"] = confidence_output
22/22: combined_df.head()
22/23:
# array to store computed similarity scores
cosine_sim = []

for index, row in combined_df.iterrows():

    # extract image name from the dataframe
    img_name = row["Image_Path"].split("/")[-1][:-4]
    taxon_predicted = row["Taxon_Output"]
    taxon_gt = Taxon_truth[img_name]

    # compute cosine similarity between the predicted taxon and ground truth
    try:
        sim = cosdis(word2vec(taxon_gt), word2vec(taxon_predicted))
        cosine_sim.append(sim)
        # print(taxon_gt, taxon_predicted, sim)

    except ZeroDivisionError:
        cosine_sim.append(0)
        # print(taxon_gt, taxon_predicted,"0")

# append similarity scores to the dataframe
combined_df["Cosine_Similarity"] = cosine_sim
22/24:
final_taxon_prediction = pd.DataFrame(columns=["Confidence_Threshold", "Taxons_Predicted", "Taxons_Accuracy_Predicted"])
temp_df = pd.DataFrame()

# generate list of similarity thresholds
# sim_threshold = [0.9]
sim_threshold = 0.8

# generate list of confidence thresholds
confidence_threshold = np.arange(0, 1, 0.1)

# compute prediction accuracy at each confidence threshold
for conf_threshold in confidence_threshold:
    
    temp_df = combined_df[(combined_df["Confidence_Output"] > conf_threshold)]
    
    acc_count = (temp_df["Cosine_Similarity"] > sim_threshold).sum()

    acc_val = acc_count/len(temp_df)

    temp = [conf_threshold, acc_count, acc_val]
    final_taxon_prediction.loc[len(final_taxon_prediction)] = temp

display(final_taxon_prediction)
22/25:
# predict taxons for text detected from each image
for index, row in combined_df.iterrows():
    temp = row["Transcription"]

    # construct a single string out of all the detected text
    input_text = " ".join(temp) 
    doc = taxonerd.find_in_text(input_text)

    try:
        # append linked taxon entity and confidence
        taxon_output.append(str(doc.entity[0][0][1]))
        confidence_output.append(float(doc.entity[0][0][2]))

    except AttributeError:
        # append empty strings when no entity is detected
        taxon_output.append("")
        confidence_output.append(float(0))
22/26:
# predict taxons for text detected from each image
for index, row in tqdm(combined_df.iterrows()):
    temp = row["Transcription"]

    # construct a single string out of all the detected text
    input_text = " ".join(temp) 
    doc = taxonerd.find_in_text(input_text)

    try:
        # append linked taxon entity and confidence
        taxon_output.append(str(doc.entity[0][0][1]))
        confidence_output.append(float(doc.entity[0][0][2]))

    except AttributeError:
        # append empty strings when no entity is detected
        taxon_output.append("")
        confidence_output.append(float(0))
22/27:
# predict taxons for text detected from each image
for index, row in tqdm(combined_df.iterrows(), , total=combined_df.shape[0]):
    temp = row["Transcription"]

    # construct a single string out of all the detected text
    input_text = " ".join(temp) 
    doc = taxonerd.find_in_text(input_text)

    try:
        # append linked taxon entity and confidence
        taxon_output.append(str(doc.entity[0][0][1]))
        confidence_output.append(float(doc.entity[0][0][2]))

    except AttributeError:
        # append empty strings when no entity is detected
        taxon_output.append("")
        confidence_output.append(float(0))
22/28:
# predict taxons for text detected from each image
for index, row in tqdm(combined_df.iterrows(), total=combined_df.shape[0]):
    temp = row["Transcription"]

    # construct a single string out of all the detected text
    input_text = " ".join(temp) 
    doc = taxonerd.find_in_text(input_text)

    try:
        # append linked taxon entity and confidence
        taxon_output.append(str(doc.entity[0][0][1]))
        confidence_output.append(float(doc.entity[0][0][2]))

    except AttributeError:
        # append empty strings when no entity is detected
        taxon_output.append("")
        confidence_output.append(float(0))
23/1:
import os
import xml.etree.ElementTree as ET
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
23/2:
class IAMDataset(Dataset):
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.samples = self._load_samples()

    def _load_samples(self):
        samples = []
        xml_dir = os.path.join(self.data_dir, 'xml')
        for file_name in os.listdir(xml_dir):
            if file_name.endswith('.xml'):
                xml_path = os.path.join(xml_dir, file_name)
                image_path = os.path.join(self.data_dir, 'img', file_name[:-4] + '.png')
                samples.append((xml_path, image_path))
        return samples

    def _parse_xml(self, xml_path):
        tree = ET.parse(xml_path)
        root = tree.getroot()
        transcription = ''
        for line in root.findall('.//line'):
            transcription += ' '.join([word.text for word in line.findall('.//word')]) + '\n'
        return transcription.strip()

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        xml_path, image_path = self.samples[idx]
        transcription = self._parse_xml(xml_path)
        image = Image.open(image_path).convert('L')
        return image, transcription

# Example usage:
data_dir = '/path/to/iam_dataset'
dataset = IAMDataset(data_dir)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)

for images, transcriptions in dataloader:
    # Your training/validation loop
    pass
23/3:
import ipywidgets as widgets
from IPython.display import display

# List of image file paths
image_paths = ['/path/to/image1.jpg', '/path/to/image2.jpg', '/path/to/image3.jpg']

# Create widgets
image_widget = widgets.Image(layout=widgets.Layout(width='500px', height='500px'))
prev_button = widgets.Button(description='Previous')
next_button = widgets.Button(description='Next')

# Define button click event handlers
current_index = 0

def on_prev_button_clicked(b):
    global current_index
    current_index = max(0, current_index - 1)
    display_image()

def on_next_button_clicked(b):
    global current_index
    current_index = min(current_index + 1, len(image_paths) - 1)
    display_image()

def display_image():
    image_path = image_paths[current_index]
    with open(image_path, 'rb') as f:
        image_data = f.read()
    image_widget.value = image_data

# Attach event handlers to buttons
prev_button.on_click(on_prev_button_clicked)
next_button.on_click(on_next_button_clicked)

# Display widgets
display(widgets.HBox([prev_button, next_button]))
display(image_widget)
23/4:
! wget -P data/ https://fki.tic.heia-fr.ch/DBs/iamDB/data/lines.tgz
! wget -P data/ https://fki.tic.heia-fr.ch/DBs/iamDB/data/sentences.tgz
! wget -P data/ https://fki.tic.heia-fr.ch/DBs/iamDB/data/words.tgz
! wget -P data/ https://fki.tic.heia-fr.ch/DBs/iamDB/data/xml.tgz
23/5:
class IAMDataset(Dataset):
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.samples = self._load_samples()

    def _load_samples(self):
        samples = []
        xml_dir = os.path.join(self.data_dir, 'xml')
        for file_name in os.listdir(xml_dir):
            if file_name.endswith('.xml'):
                xml_path = os.path.join(xml_dir, file_name)
                image_path = os.path.join(self.data_dir, 'img', file_name[:-4] + '.png')
                samples.append((xml_path, image_path))
        return samples

    def _parse_xml(self, xml_path):
        tree = ET.parse(xml_path)
        root = tree.getroot()
        transcription = ''
        for line in root.findall('.//line'):
            transcription += ' '.join([word.text for word in line.findall('.//word')]) + '\n'
        return transcription.strip()

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        xml_path, image_path = self.samples[idx]
        transcription = self._parse_xml(xml_path)
        image = Image.open(image_path).convert('L')
        return image, transcription

# Example usage:
data_dir = './data'
dataset = IAMDataset(data_dir)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)

for images, transcriptions in dataloader:
    # Your training/validation loop
    pass
23/6:
class IAMDataset(Dataset):
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.samples = self._load_samples()

    def _load_samples(self):
        samples = []
        xml_dir = os.path.join(self.data_dir, 'xml')
        for file_name in os.listdir(xml_dir):
            if file_name.endswith('.xml'):
                xml_path = os.path.join(xml_dir, file_name)
                image_path = os.path.join(self.data_dir, 'img', file_name[:-4] + '.png')
                samples.append((xml_path, image_path))
        return samples

    def _parse_xml(self, xml_path):
        tree = ET.parse(xml_path)
        root = tree.getroot()
        transcription = ''
        for line in root.findall('.//line'):
            transcription += ' '.join([word.text for word in line.findall('.//word')]) + '\n'
        return transcription.strip()

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        xml_path, image_path = self.samples[idx]
        transcription = self._parse_xml(xml_path)
        image = Image.open(image_path).convert('L')
        return image, transcription

# Example usage:
data_dir = './data/'
dataset = IAMDataset(data_dir)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)

for images, transcriptions in dataloader:
    # Your training/validation loop
    pass
23/7:
! wget -P data/ -O lines https://fki.tic.heia-fr.ch/DBs/iamDB/data/lines.tgz
! wget -P data/ -O sentences https://fki.tic.heia-fr.ch/DBs/iamDB/data/sentences.tgz
! wget -P data/ -O words https://fki.tic.heia-fr.ch/DBs/iamDB/data/words.tgz
! wget -P data/ -O xml https://fki.tic.heia-fr.ch/DBs/iamDB/data/xml.tgz
23/8:
! wget -O data/lines https://fki.tic.heia-fr.ch/DBs/iamDB/data/lines.tgz
# ! wget -P data/ -O sentences https://fki.tic.heia-fr.ch/DBs/iamDB/data/sentences.tgz
# ! wget -P data/ -O words https://fki.tic.heia-fr.ch/DBs/iamDB/data/words.tgz
# ! wget -P data/ -O xml https://fki.tic.heia-fr.ch/DBs/iamDB/data/xml.tgz
23/9:
! wget -O data/lines https://fki.tic.heia-fr.ch/DBs/iamDB/data/lines.tgz
! wget -O data/sentences https://fki.tic.heia-fr.ch/DBs/iamDB/data/sentences.tgz
! wget -O data/words https://fki.tic.heia-fr.ch/DBs/iamDB/data/words.tgz
! wget -O data/xml https://fki.tic.heia-fr.ch/DBs/iamDB/data/xml.tgz
23/10:
class IAMDataset(Dataset):
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.samples = self._load_samples()

    def _load_samples(self):
        samples = []
        xml_dir = os.path.join(self.data_dir, 'xml')
        for file_name in os.listdir(xml_dir):
            if file_name.endswith('.xml'):
                xml_path = os.path.join(xml_dir, file_name)
                image_path = os.path.join(self.data_dir, 'img', file_name[:-4] + '.png')
                samples.append((xml_path, image_path))
        return samples

    def _parse_xml(self, xml_path):
        tree = ET.parse(xml_path)
        root = tree.getroot()
        transcription = ''
        for line in root.findall('.//line'):
            transcription += ' '.join([word.text for word in line.findall('.//word')]) + '\n'
        return transcription.strip()

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        xml_path, image_path = self.samples[idx]
        transcription = self._parse_xml(xml_path)
        image = Image.open(image_path).convert('L')
        return image, transcription

# Example usage:
data_dir = './data/'
dataset = IAMDataset(data_dir)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)

for images, transcriptions in dataloader:
    # Your training/validation loop
    pass
23/11:
class IAMDataset(Dataset):
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.samples = self._load_samples()

    def _load_samples(self):
        samples = []
        xml_dir = os.path.join(self.data_dir, 'xml')
        for file_name in os.listdir(xml_dir):
            if file_name.endswith('.xml'):
                xml_path = os.path.join(xml_dir, file_name)
                image_path = os.path.join(self.data_dir, 'img', file_name[:-4] + '.png')
                samples.append((xml_path, image_path))
        return samples

    def _parse_xml(self, xml_path):
        tree = ET.parse(xml_path)
        root = tree.getroot()
        transcription = ''
        for line in root.findall('.//line'):
            transcription += ' '.join([word.text for word in line.findall('.//word')]) + '\n'
        return transcription.strip()

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        xml_path, image_path = self.samples[idx]
        transcription = self._parse_xml(xml_path)
        image = Image.open(image_path).convert('L')
        return image, transcription

# Example usage:
data_dir = 'data/'
dataset = IAMDataset(data_dir)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)

for images, transcriptions in dataloader:
    # Your training/validation loop
    pass
23/12:
! wget -P data/ https://fki.tic.heia-fr.ch/DBs/iamDB/data/lines.tgz
# ! wget -O data/sentences https://fki.tic.heia-fr.ch/DBs/iamDB/data/sentences.tgz
# ! wget -O data/words https://fki.tic.heia-fr.ch/DBs/iamDB/data/words.tgz
# ! wget -O data/xml https://fki.tic.heia-fr.ch/DBs/iamDB/data/xml.tgz
23/13:
! curl -s -L https://fki.tic.heia-fr.ch/DBs/iamDB/data/lines.tgz
# ! wget -O data/sentences https://fki.tic.heia-fr.ch/DBs/iamDB/data/sentences.tgz
# ! wget -O data/words https://fki.tic.heia-fr.ch/DBs/iamDB/data/words.tgz
# ! wget -O data/xml https://fki.tic.heia-fr.ch/DBs/iamDB/data/xml.tgz
23/14:
wget --save-cookies data/cookies.txt \
 --keep-session-cookies \
 --post-data 'email=kabilanm@bu.edu&password=Z3wsbjRxrat@EcH' \
 --delete-after \
 https://fki.tic.heia-fr.ch/login
23/15:
wget --save-cookies ./data/cookies.txt \
 --keep-session-cookies \
 --post-data 'email=kabilanm@bu.edu&password=Z3wsbjRxrat@EcH' \
 --delete-after \
 https://fki.tic.heia-fr.ch/login
23/16:
wget --save-cookies cookies.txt \
 --keep-session-cookies \
 --post-data 'email=kabilanm@bu.edu&password=Z3wsbjRxrat@EcH' \
 --delete-after \
 https://fki.tic.heia-fr.ch/login
23/17:
! wget --save-cookies data/cookies.txt \
 --keep-session-cookies \
 --post-data 'email=kabilanm@bu.edu&password=Z3wsbjRxrat@EcH' \
 --delete-after \
 https://fki.tic.heia-fr.ch/login
23/18:
!wget -P data/ 
    --load-cookies cookies.txt \
     https://fki.tic.heia-fr.ch/DBs/iamDB/data/words.tgz
23/19:
! wget -P data/ \
    --load-cookies cookies.txt \
     https://fki.tic.heia-fr.ch/DBs/iamDB/data/words.tgz
23/20:
! wget -P data/ \
    --load-cookies data/cookies.txt \
     https://fki.tic.heia-fr.ch/DBs/iamDB/data/words.tgz
23/21:
# ! wget -P data/ \
#     --load-cookies data/cookies.txt \
#      https://fki.tic.heia-fr.ch/DBs/iamDB/data/words.tgz
! wget -P data/ \
    --load-cookies data/cookies.txt \
     https://fki.tic.heia-fr.ch/DBs/iamDB/data/lines.tgz
! wget -P data/ \
    --load-cookies data/cookies.txt \
     https://fki.tic.heia-fr.ch/DBs/iamDB/data/sentences.tgz
! wget -P data/ \
    --load-cookies data/cookies.txt \
     https://fki.tic.heia-fr.ch/DBs/iamDB/data/xml.tgz
24/1: ! tar -xvzf data/*.tgz
24/2:
# ! tar -xvzf data/*.tgz
! ls data/*.gz | xargs -n1 tar -xvzf
24/3:
# ! tar -xvzf data/*.tgz
! ls ./data/*.gz | xargs -n1 tar -xvzf
24/4:
# ! tar -xvzf data/*.tgz
# ! ls ./data/*.gz | xargs -n1 tar -xvzf
! ls data
24/5:
# ! tar -xvzf data/*.tgz
# ! ls data/*.gz | xargs -n1 tar -xvzf
! ls data/
24/6:
# ! tar -xvzf data/*.tgz
# ! ls data/*.gz | xargs -n1 tar -xvzf
! ls data/*.gz
24/7:
# ! tar -xvzf data/*.tgz
# ! ls data/*.gz | xargs -n1 tar -xvzf
! ls -d data/*.gz
24/8:
# ! tar -xvzf data/*.tgz
# ! ls data/*.gz | xargs -n1 tar -xvzf
! ls -d -- data/*.gz
24/9:
# ! tar -xvzf data/*.tgz
# ! ls data/*.gz | xargs -n1 tar -xvzf
! ls data/ | grep .gz | xargs -n1 tar -xvzf
24/10:
! tar -xvzf data/words.tgz
# ! ls data/*.gz | xargs -n1 tar -xvzf
# ! ls data/ | grep .gz | xargs -n1 tar -xvzf
24/11:
# ! tar -xvzf data/words.tgz
! tar -xvzf data/lines.tgz
! tar -xvzf data/sentences.tgz
! tar -xvzf data/xml.tgz
# ! ls data/*.gz | xargs -n1 tar -xvzf
# ! ls data/ | grep .gz | xargs -n1 tar -xvzf
24/12: ! pwd
24/13:
! tar -xvzf data/words.tgz -C data/words
# ! tar -xvzf data/lines.tgz
# ! tar -xvzf data/sentences.tgz
# ! tar -xvzf data/xml.tgz
# ! ls data/*.gz | xargs -n1 tar -xvzf
# ! ls data/ | grep .gz | xargs -n1 tar -xvzf
24/14:
! tar -xvzf data/words.tgz -C data/
# ! tar -xvzf data/lines.tgz
# ! tar -xvzf data/sentences.tgz
# ! tar -xvzf data/xml.tgz
# ! ls data/*.gz | xargs -n1 tar -xvzf
# ! ls data/ | grep .gz | xargs -n1 tar -xvzf
24/15:
# ! tar -xvzf data/words.tgz -C data/
# ! tar -xvzf data/lines.tgz
# ! tar -xvzf data/sentences.tgz
! tar -xvzf data/xml.tgz -C data/ --one-top-level
# ! ls data/*.gz | xargs -n1 tar -xvzf
# ! ls data/ | grep .gz | xargs -n1 tar -xvzf
24/16:
# ! mkdir -p data/{words,sentences,xml,lines}

! tar -xzf data/words.tgz -C data/words
! tar -xzf data/lines.tgz -C data/lines
! tar -xzf data/sentences.tgz data/sentences
! tar -xzf data/xml.tgz -C data/xml
24/17:
class IAMDataset(Dataset):
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.samples = self._load_samples()

    def _load_samples(self):
        samples = []
        xml_dir = os.path.join(self.data_dir, 'xml')
        for file_name in os.listdir(xml_dir):
            if file_name.endswith('.xml'):
                xml_path = os.path.join(xml_dir, file_name)
                image_path = os.path.join(self.data_dir, 'img', file_name[:-4] + '.png')
                samples.append((xml_path, image_path))
        return samples

    def _parse_xml(self, xml_path):
        tree = ET.parse(xml_path)
        root = tree.getroot()
        transcription = ''
        for line in root.findall('.//line'):
            transcription += ' '.join([word.text for word in line.findall('.//word')]) + '\n'
        return transcription.strip()

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        xml_path, image_path = self.samples[idx]
        transcription = self._parse_xml(xml_path)
        image = Image.open(image_path).convert('L')
        return image, transcription

# Example usage:
data_dir = 'data/'
dataset = IAMDataset(data_dir)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)

for images, transcriptions in dataloader:
    # Your training/validation loop
    pass
24/18:
import os
import xml.etree.ElementTree as ET
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
24/19:
class IAMDataset(Dataset):
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.samples = self._load_samples()

    def _load_samples(self):
        samples = []
        xml_dir = os.path.join(self.data_dir, 'xml')
        for file_name in os.listdir(xml_dir):
            if file_name.endswith('.xml'):
                xml_path = os.path.join(xml_dir, file_name)
                image_path = os.path.join(self.data_dir, 'img', file_name[:-4] + '.png')
                samples.append((xml_path, image_path))
        return samples

    def _parse_xml(self, xml_path):
        tree = ET.parse(xml_path)
        root = tree.getroot()
        transcription = ''
        for line in root.findall('.//line'):
            transcription += ' '.join([word.text for word in line.findall('.//word')]) + '\n'
        return transcription.strip()

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        xml_path, image_path = self.samples[idx]
        transcription = self._parse_xml(xml_path)
        image = Image.open(image_path).convert('L')
        return image, transcription

# Example usage:
data_dir = 'data/'
dataset = IAMDataset(data_dir)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)

for images, transcriptions in dataloader:
    # Your training/validation loop
    pass
24/20:
class IAMDataset(Dataset):
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.samples = self._load_samples()

    def _load_samples(self):
        samples = []
        xml_dir = os.path.join(self.data_dir, 'xml')
        for file_name in os.listdir(xml_dir):
            if file_name.endswith('.xml'):
                xml_path = os.path.join(xml_dir, file_name)
                print(xml_path)
                image_path = os.path.join(self.data_dir, 'img', file_name[:-4] + '.png')
                samples.append((xml_path, image_path))
        return samples

    def _parse_xml(self, xml_path):
        tree = ET.parse(xml_path)
        root = tree.getroot()
        transcription = ''
        for line in root.findall('.//line'):
            transcription += ' '.join([word.text for word in line.findall('.//word')]) + '\n'
        return transcription.strip()

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        xml_path, image_path = self.samples[idx]
        transcription = self._parse_xml(xml_path)
        image = Image.open(image_path).convert('L')
        return image, transcription

# Example usage:
data_dir = 'data/'
dataset = IAMDataset(data_dir)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)

for images, transcriptions in dataloader:
    # Your training/validation loop
    pass
24/21:
class IAMDataset(Dataset):
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.samples = self._load_samples()

    def _load_samples(self):
        samples = []
        xml_dir = os.path.join(self.data_dir, 'xml')
        for file_name in os.listdir(xml_dir):
            if file_name.endswith('.xml'):
                xml_path = os.path.join(xml_dir, file_name)
                print(xml_path)
                image_path = os.path.join(self.data_dir, 'img', file_name[:-4] + '.png')
                print(image_path)
                samples.append((xml_path, image_path))
        return samples

    def _parse_xml(self, xml_path):
        tree = ET.parse(xml_path)
        root = tree.getroot()
        transcription = ''
        for line in root.findall('.//line'):
            transcription += ' '.join([word.text for word in line.findall('.//word')]) + '\n'
        return transcription.strip()

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        xml_path, image_path = self.samples[idx]
        transcription = self._parse_xml(xml_path)
        image = Image.open(image_path).convert('L')
        return image, transcription

# Example usage:
data_dir = 'data/'
dataset = IAMDataset(data_dir)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)

for images, transcriptions in dataloader:
    # Your training/validation loop
    pass
24/22:
import os
from PIL import Image
from torch.utils.data import Dataset, DataLoader

class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = '/path/to/iam_dataset'
data_level = 'sentences'  # 'sentences', 'words', or 'lines'
transform = ...  # Apply any desired transformations

dataset = IAMDataset(root_dir, data_level, transform=transform)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
24/23:
import os
from PIL import Image
from torch.utils.data import Dataset, DataLoader

class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = 'data/'
data_level = 'sentences'  # 'sentences', 'words', or 'lines'
transform = ...  # Apply any desired transformations

dataset = IAMDataset(root_dir, data_level, transform=transform)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
24/24:
import os
from PIL import Image
from torch.utils.data import Dataset, DataLoader

class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = 'data/'
data_level = 'sentences'  # 'sentences', 'words', or 'lines'
# transform = ...  # Apply any desired transformations

dataset = IAMDataset(root_dir, data_level, 
                     # transform=transform
                    )
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
24/25:
import os
from PIL import Image
from torch.utils.data import Dataset, DataLoader

class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = 'data/'
data_level = 'sentences'  # 'sentences', 'words', or 'lines'
# transform = ...  # Apply any desired transformations

dataset = IAMDataset(root_dir, data_level, 
                     # transform=transform
                    )
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

for images, transcriptions in dataloader:
    # Your training/validation loop
    pass
24/26:
import os
from PIL import Image
from torch.utils.data import Dataset, DataLoader

class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = 'data/'
data_level = 'sentences'  # 'sentences', 'words', or 'lines'
# transform = ...  # Apply any desired transformations

dataset = IAMDataset(root_dir, data_level, 
                     # transform=transform
                    )
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

for images, labels in dataloader:
    # Your training/validation loop
    pass
24/27:
import os
from PIL import Image
from torch.utils.data import Dataset, DataLoader

class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        print(image_paths)
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = 'data/'
data_level = 'sentences'  # 'sentences', 'words', or 'lines'
# transform = ...  # Apply any desired transformations

dataset = IAMDataset(root_dir, data_level, 
                     # transform=transform
                    )
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

for images, labels in dataloader:
    # Your training/validation loop
    pass
24/28:
import os
from PIL import Image
from torch.utils.data import Dataset, DataLoader

class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        print(image_paths)
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = './data/'
data_level = 'sentences'  # 'sentences', 'words', or 'lines'
# transform = ...  # Apply any desired transformations

dataset = IAMDataset(root_dir, data_level, 
                     # transform=transform
                    )
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

for images, labels in dataloader:
    # Your training/validation loop
    pass
24/29:
# ! mkdir -p data/{words,sentences,xml,lines}

# ! tar -xzf data/words.tgz -C data/words
# ! tar -xzf data/lines.tgz -C data/lines
! tar -xzf data/sentences.tgz data/sentences
# ! tar -xzf data/xml.tgz -C data/xml
24/30:
# ! mkdir -p data/{words,sentences,xml,lines}

# ! tar -xzf data/words.tgz -C data/words
# ! tar -xzf data/lines.tgz -C data/lines
! tar -xzf data/sentences.tgz -C data/sentences
# ! tar -xzf data/xml.tgz -C data/xml
24/31:
class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        print(image_paths)
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = './data/'
data_level = 'sentences'  # 'sentences', 'words', or 'lines'
# transform = ...  # Apply any desired transformations

dataset = IAMDataset(root_dir, data_level, 
                     # transform=transform
                    )
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

for images, labels in dataloader:
    # Your training/validation loop
    pass
24/32:
class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = './data/'
data_level = 'sentences'  # 'sentences', 'words', or 'lines'
# transform = ...  # Apply any desired transformations

dataset = IAMDataset(root_dir, data_level, 
                     # transform=transform
                    )
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

for images, labels in dataloader:
    # Your training/validation loop
    print(labels)
24/33:
class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = './data/'
data_level = 'sentences'  # 'sentences', 'words', or 'lines'
# transform = ...  # Apply any desired transformations

dataset = IAMDataset(root_dir, data_level, 
                     # transform=transform
                    )
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

for images, labels in dataloader:
    # Your training/validation loop
#     print(labels)
    pass
24/34:
class IAMDataset(Dataset):
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.samples = self._load_samples()

    def _load_samples(self):
        samples = []
        xml_dir = os.path.join(self.data_dir, 'xml')
        for file_name in os.listdir(xml_dir):
            if file_name.endswith('.xml'):
                xml_path = os.path.join(xml_dir, file_name)
#                 print(xml_path)
                image_path = os.path.join(self.data_dir, 'img', file_name[:-4] + '.png')
#                 print(image_path)
                samples.append((xml_path, image_path))
        return samples

    def _parse_xml(self, xml_path):
        tree = ET.parse(xml_path)
        root = tree.getroot()
        transcription = ''
        for line in root.findall('.//line'):
            transcription += ' '.join([word.text for word in line.findall('.//word')]) + '\n'
        return transcription.strip()

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        xml_path, image_path = self.samples[idx]
        transcription = self._parse_xml(xml_path)
        image = Image.open(image_path).convert('L')
        return image, transcription

# Example usage:
data_dir = 'data/'
dataset = IAMDataset(data_dir)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)

for images, transcriptions in dataloader:
    # Your training/validation loop
    pass
24/35:
class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = './data/'
data_level = 'sentences'  # 'sentences', 'words', or 'lines'
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

dataset = IAMDataset(root_dir, data_level, 
                     transform=preprocess
                    )
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

for images, labels in dataloader:
    # Your training/validation loop
#     print(labels)
    pass
24/36:
import os
import xml.etree.ElementTree as ET
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import transforms
24/37:
class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = './data/'
data_level = 'sentences'  # 'sentences', 'words', or 'lines'
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

dataset = IAMDataset(root_dir, data_level, 
                     transform=preprocess
                    )
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

for images, labels in dataloader:
    # Your training/validation loop
#     print(labels)
    pass
24/38:
class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        print(image_paths[:10])
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = './data/'
data_level = 'sentences'  # 'sentences', 'words', or 'lines'
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

dataset = IAMDataset(root_dir, data_level, 
                     transform=preprocess
                    )
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

for images, labels in dataloader:
    # Your training/validation loop
    print(labels)
#     pass
24/39:
class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        print(image_paths[:10])
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = './data/'
data_level = 'sentences'  # 'sentences', 'words', or 'lines'
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

dataset = IAMDataset(root_dir, data_level, 
                     transform=preprocess
                    )
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

for images, labels in dataloader:
    # Your training/validation loop
    print(labels)
#     pass
24/40:
class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        print(image_paths)
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = './data/'
data_level = 'sentences'  # 'sentences', 'words', or 'lines'
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

dataset = IAMDataset(root_dir, data_level, 
                     transform=preprocess
                    )
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

for images, labels in dataloader:
    # Your training/validation loop
    print(labels)
#     pass
24/41:
class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        print(image_paths)
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = './data/'
data_level = 'words'  # 'sentences', 'words', or 'lines'
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

dataset = IAMDataset(root_dir, data_level, 
                     transform=preprocess
                    )
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

for images, labels in dataloader:
    # Your training/validation loop
    print(labels)
#     pass
24/42:
class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        print(image_paths)
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('RGB')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        label = os.path.basename(os.path.dirname(image_path))

        return image, label

# Example usage
root_dir = './data/'
data_level = 'words'  # 'sentences', 'words', or 'lines'
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

dataset = IAMDataset(root_dir, data_level, 
                     transform=preprocess
                    )
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

for images, labels in dataloader:
    # Your training/validation loop
    print(labels)
#     pass
24/43:
class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        print(image_paths)
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('RGB')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        # label = os.path.basename(os.path.dirname(image_path))

        return image, "0"

# Example usage
root_dir = './data/'
data_level = 'words'  # 'sentences', 'words', or 'lines'
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

dataset = IAMDataset(root_dir, data_level, 
                     transform=preprocess
                    )
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

for images, labels in dataloader:
    # Your training/validation loop
    print(labels)
#     pass
24/44:
class IAMDataset(Dataset):
    def __init__(self, root_dir, data_level, transform=None):
        self.root_dir = root_dir
        self.data_level = data_level
        self.transform = transform
        self.image_paths = self._get_image_paths()

    def _get_image_paths(self):
        image_paths = []
        for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
            for file in files:
                if file.endswith('.png'):
                    image_paths.append(os.path.join(root, file))
        print(image_paths)
        return image_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image_path = self.image_paths[index]
        image = Image.open(image_path).convert('RGB')  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        # Extract the label based on the data level
        # label = os.path.basename(os.path.dirname(image_path))

        return image, "0"

# Example usage
root_dir = './data/'
data_level = 'words'  # 'sentences', 'words', or 'lines'
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

dataset = IAMDataset(root_dir, data_level, 
                     transform=preprocess
                    )
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

for images, labels in dataloader:
    # Your training/validation loop
    print(labels)
#     pass
24/45:
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader
24/46:
def get_image_paths(level):
    image_paths = []
    for root, _, files in os.walk(os.path.join(self.root_dir, self.data_level)):
        for file in files:
            if file.endswith('.png'):
                image_paths.append(os.path.join(root, file))
    print(image_paths)
    return image_paths

dataset_levels = ["lines", "sentences"]
all_image_paths = []

for level in dataset_levels:
    all_image_paths += get_image_paths(level)

print(all_image_paths)
24/47:
def get_image_paths(root_dir, data_level):
    image_paths = []
    for root, _, files in os.walk(os.path.join(root_dir, data_level)):
        for file in files:
            if file.endswith('.png'):
                image_paths.append(os.path.join(root, file))
    return image_paths

dataset_levels = ["lines", "sentences"]
all_image_paths = []

for level in dataset_levels:
    all_image_paths += get_image_paths("data/", level)

print(all_image_paths)
24/48:
def get_image_paths(root_dir, data_level):
    image_paths = []
    for root, _, files in os.walk(os.path.join(root_dir, data_level)):
        for file in files:
            if file.endswith('.png'):
                image_paths.append(os.path.join(root, file))
    return image_paths

dataset_levels = ["lines", "sentences"]
all_image_paths = []

for level in dataset_levels:
    all_image_paths += get_image_paths("data/", level)

print(len(all_image_paths))
24/49:
def get_image_paths(root_dir, data_level):
    image_paths = []
    for root, _, files in os.walk(os.path.join(root_dir, data_level)):
        for file in files:
            if file.endswith('.png'):
                image_paths.append(os.path.join(root, file))
    return image_paths

dataset_levels = ["lines", "sentences", "words"]
all_image_paths = []

for level in dataset_levels:
    all_image_paths += get_image_paths("data/", level)

print(len(all_image_paths))
24/50:
def get_image_paths(root_dir, data_level):
    image_paths = []
    for root, _, files in os.walk(os.path.join(root_dir, data_level)):
        for file in files:
            if file.endswith('.png'):
                image_paths.append(os.path.join(root, file))
    return image_paths

dataset_levels = ["lines", "sentences"] # "words"
all_image_paths = []

for level in dataset_levels:
    all_image_paths += get_image_paths("data/", level)

print(len(all_image_paths))
24/51:
def get_image_paths(root_dir, data_level):
    image_paths = []
    for root, _, files in os.walk(os.path.join(root_dir, data_level)):
        for file in files:
            if file.endswith('.png'):
                image_paths.append(os.path.join(root, file))
    return image_paths

dataset_levels = ["lines", "sentences"] # "words"
hw_image_paths = []

for level in dataset_levels:
    hw_image_paths += get_image_paths("data/", level)

print(len(hw_image_paths))
24/52:
import os
import shutil

def copy_files(destination_folder, file_path_list):
    for file_name in file_path_list[:10]:
        source_path = file
        destination_path = os.path.join(destination_folder, file_name)
        shutil.copy2(source_path, destination_path)

destination_folder = 'data/all_data/handwritten'
24/53: copy_files(source_folder, destination_folder, hw_image_paths)
24/54:
destination_folder = 'data/all_data/handwritten'
copy_files(destination_folder, hw_image_paths)
24/55:
import os
import shutil

def copy_files(destination_folder, file_path_list):
    for source_path in file_path_list[:10]:
        destination_path = os.path.join(destination_folder, source_path.split("/")[-1])
        print(destination_path)
#         shutil.copy2(source_path, destination_path)
24/56:
destination_folder = 'data/all_data/handwritten'
copy_files(destination_folder, hw_image_paths)
24/57:
import os
import shutil

def copy_files(destination_folder, file_path_list):
    for source_path in file_path_list[:10]:
        destination_path = os.path.join(destination_folder, source_path.split("/")[-1])
#         print(destination_path)
        shutil.copy2(source_path, destination_path)
24/58:
destination_folder = 'data/all_data/handwritten'
copy_files(destination_folder, hw_image_paths)
24/59:
def copy_files(destination_folder, file_path_list):
    for source_path in file_path_list:
        destination_path = os.path.join(destination_folder, source_path.split("/")[-1])
        shutil.copy2(source_path, destination_path)
24/60:
destination_folder = 'data/all_data/handwritten'
copy_files(destination_folder, hw_image_paths)
24/61: ! wget -P data/FUNSD.zip https://guillaumejaume.github.io/FUNSD/dataset.zip
24/62: ! wget -O data/FUNSD.zip https://guillaumejaume.github.io/FUNSD/dataset.zip
24/63: ! wget -O data/FUNSD_data.zip https://guillaumejaume.github.io/FUNSD/dataset.zip
24/64: ! tar -xzf data/FUNSD_data.zip -C data/FUNSD
24/65: ! unzip data/FUNSD_data.zip
24/66: ! unzip data/FUNSD_data.zip -d data/FUNSD
24/67:
import os
import shutil
import json

import xml.etree.ElementTree as ET
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import transforms
24/68:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):

    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        image_path = os.path.join(image_folder, os.path.splitext(file_name)[0] + ".png")
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["answer"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((bounding_box['left'], bounding_box['top'],
                                                    bounding_box['right'], bounding_box['bottom']))
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_name+idx)
                        # Save the cropped image
                        cropped_image.save(output_path)
24/69:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):

    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        image_path = os.path.join(image_folder, os.path.splitext(file_name)[0] + ".png")
        print(annotation_path, image_path)
        
        
#         # Open the image
#         image = Image.open(image_path)
        
#         if os.path.isfile(annotation_path):
#             # Load bounding box data from JSON file
#             with open(annotation_path, 'r') as f:
#                 raw = json.load(f)
                
#                 for item in raw["form"]:
#                     box = item["box"]
#                     label = item["label"]
#                     idx = item["id"]
                    
#                     if label not in ["answer"]:
#                         # Crop the image using the bounding box coordinates
#                         cropped_image = image.crop((bounding_box['left'], bounding_box['top'],
#                                                     bounding_box['right'], bounding_box['bottom']))
                        
#                         # append bounding box id to the cropped image name
#                         output_path = os.path.join(output_folder, image_name+idx)
#                         # Save the cropped image
#                         cropped_image.save(output_path)
24/70:
# Example usage
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
24/71:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):

    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        image_path = os.path.join(image_folder, os.path.splitext(annotation_file)[0] + ".png")
        print(annotation_path, image_path)
        
        
#         # Open the image
#         image = Image.open(image_path)
        
#         if os.path.isfile(annotation_path):
#             # Load bounding box data from JSON file
#             with open(annotation_path, 'r') as f:
#                 raw = json.load(f)
                
#                 for item in raw["form"]:
#                     box = item["box"]
#                     label = item["label"]
#                     idx = item["id"]
                    
#                     if label not in ["answer"]:
#                         # Crop the image using the bounding box coordinates
#                         cropped_image = image.crop((bounding_box['left'], bounding_box['top'],
#                                                     bounding_box['right'], bounding_box['bottom']))
                        
#                         # append bounding box id to the cropped image name
#                         output_path = os.path.join(output_folder, image_name+idx)
#                         # Save the cropped image
#                         cropped_image.save(output_path)
24/72:
# Example usage
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
24/73:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):

    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        image_path = os.path.join(image_folder, os.path.splitext(annotation_file)[0] + ".png")        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["answer"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((bounding_box['left'], bounding_box['top'],
                                                    bounding_box['right'], bounding_box['bottom']))
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_name+idx)
                        # Save the cropped image
                        cropped_image.save(output_path)
24/74:
# Example usage
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
24/75:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):

    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        image_path = os.path.join(image_folder, os.path.splitext(annotation_file)[0] + ".png")        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["answer"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_name+idx)
                        # Save the cropped image
                        cropped_image.save(output_path)
24/76:
# Example usage
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
24/77:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):

    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["answer"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_file+idx)
                        # Save the cropped image
                        cropped_image.save(output_path)
24/78:
# Example usage
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
24/79:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):

    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["answer"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_file+str(idx))
                        # Save the cropped image
                        cropped_image.save(output_path)
24/80:
# Example usage
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
24/81:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):

    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["answer"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx))
                        # Save the cropped image
                        cropped_image.save(output_path)
24/82:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):

    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["answer"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx))
                        # Save the cropped image
                        cropped_image.save(output_path)
24/83:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):

    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["answer"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx))
                        # Save the cropped image
                        cropped_image.save(output_path)
24/84:
# Example usage
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
24/85:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):

    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["answer"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
                        # Save the cropped image
                        cropped_image.save(output_path, "PNG")
24/86:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):

    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["answer"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
                        # Save the cropped image
                        cropped_image.save(output_path, "PNG")
24/87:
# Example usage
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
24/88:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):
    
    unique_labels = defaultdict(int)

    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    unique_labels[label] += 1
    
    print(unique_labels)
                    
#                     if label not in ["answer"]:
#                         # Crop the image using the bounding box coordinates
#                         cropped_image = image.crop((box[0], box[1],
#                                                     box[2], box[3]))
                        
#                         # append bounding box id to the cropped image name
#                         output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
#                         # Save the cropped image
#                         cropped_image.save(output_path, "PNG")
24/89:
# Example usage
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
24/90:
from collections import defaultdict

image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
24/91:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):
    
    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["other"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
                        # Save the cropped image
                        cropped_image.save(output_path, "PNG")
24/92:
from collections import defaultdict

image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
24/93:
image_folder = 'data/FUNSD/dataset/testing_data/images'
annotation_folder = 'data/FUNSD/dataset/testing_data/annotations'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
24/94:
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
24/95:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=32, sampler=val_sampler)
24/96:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model
24/97:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
24/98:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
24/99: print(device)
24/100:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
24/101:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=32, sampler=val_sampler)
24/102:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
24/103:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
24/104:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        print("Batch in progress: ", batch_idx)
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
24/105:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
24/106:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        print("Batch in progress: ", batch_idx)
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
24/107:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, 1)

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        print("Batch in progress: ", batch_idx)
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
24/108:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes)

# Load the pre-trained VGG16 model
model = models.vgg16(pretrained=True)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        print("Batch in progress: ", batch_idx)
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
24/109:
# Set random seed for reproducibility
# torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes)

# Load the pre-trained VGG16 model
model = models.vgg16(pretrained=True)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        print("Batch in progress: ", batch_idx)
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
24/110:
# Set random seed for reproducibility
# torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes)

# Load the pre-trained VGG16 model
model = models.vgg16(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        print("Batch in progress: ", batch_idx)
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
24/111:
# Set random seed for reproducibility
# torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes)

# Load the pre-trained VGG16 model
model = models.vgg16(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        print("Batch in progress: ", batch_idx)
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
24/112:
# Set random seed for reproducibility
# torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes)

# Load the pre-trained VGG16 model
model = models.vgg16(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        print("Batch in progress: ", batch_idx)
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
24/113:
# Set random seed for reproducibility
# torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        print("Batch in progress: ", batch_idx)
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
24/114:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        print("Batch in progress: ", batch_idx)
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
24/115:
import os
import shutil
import json

import xml.etree.ElementTree as ET
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import transforms
24/116:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        print("Batch in progress: ", batch_idx)
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
24/117:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
24/118:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
24/119:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        print("Batch in progress: ", batch_idx)
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
25/1:
import os
import shutil
import json

import xml.etree.ElementTree as ET
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import transforms
25/2:
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
25/3:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
25/4:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        print("Batch in progress: ", batch_idx)
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
25/5:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        print("Batch in progress: ", batch_idx)
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
25/6:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        print("Batch in progress: ", batch_idx)
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            
            outputs = outputs.unsqueeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
25/7:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 10
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        print("Batch in progress: ", batch_idx)
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        outputs = outputs.unsqueeze(1)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            
            outputs = outputs.unsqueeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
25/8: torch.cuda.empty_cache()
26/1:
import os
import shutil
import json

import xml.etree.ElementTree as ET
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import transforms
26/2:
def get_image_paths(root_dir, data_level):
    image_paths = []
    for root, _, files in os.walk(os.path.join(root_dir, data_level)):
        for file in files:
            if file.endswith('.png'):
                image_paths.append(os.path.join(root, file))
    return image_paths


# Get IAM image paths
dataset_levels = ["lines", "sentences"] # "words"
hw_image_paths = []

for level in dataset_levels:
    hw_image_paths += get_image_paths("data/", level)

print(len(hw_image_paths))

# Get typed text image paths
# ty_image_paths = []

# for level in dataset_levels:
#     ty_image_paths += get_image_paths("data/", level)

# print(len(hw_image_paths))
26/3:
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
26/4:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
26/5:
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler

from sklearn.metrics import confusion_matrix
import seaborn as sn
import pandas as pd
26/6:
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler

from sklearn.metrics import confusion_matrix

! pip install seaborn
import seaborn as sn
import pandas as pd
26/7:
from sklearn.metrics import confusion_matrix

# ! pip install seaborn
import seaborn as sn
import pandas as pd
26/8:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        outputs = outputs.unsqueeze(1)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            outputs = outputs.unsqueeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
26/9:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        outputs = outputs.squeeze(1)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            outputs = outputs.squeeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
26/10:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        outputs = outputs.squeeze(1)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            outputs = outputs.squeeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
27/1:
import gc

gc.collect()

torch.cuda.empty_cache()
27/2:
import os
import shutil
import json

from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler

from sklearn.metrics import confusion_matrix

# ! pip install seaborn
import seaborn as sn
import pandas as pd
27/3:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
27/4:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        outputs = outputs.squeeze(1)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            outputs = outputs.squeeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
27/5:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        outputs = outputs.squeeze(1)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            outputs = outputs.squeeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
27/6:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
#         outputs = outputs.squeeze(1)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            outputs = outputs.squeeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
27/7:
import gc

gc.collect()

torch.cuda.empty_cache()
27/8:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
#         outputs = outputs.squeeze(1)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
#             outputs = outputs.squeeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
27/9:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        outputs = outputs.squeeze(1)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
#             outputs = outputs.squeeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
27/10:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        outputs = outputs.squeeze(1)
        loss = criterion(outputs, labels.float())

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            outputs = outputs.squeeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
27/11:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        print(labels)
        
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        outputs = outputs.squeeze(1)
        loss = criterion(outputs, labels.float())

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            outputs = outputs.squeeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
27/12:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        print(labels)
        print(labels.float())
        
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        outputs = outputs.squeeze(1)
        loss = criterion(outputs, labels.float())

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            outputs = outputs.squeeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/1:
import os
import shutil
import json

from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler

from sklearn.metrics import confusion_matrix

# ! pip install seaborn
import seaborn as sn
import pandas as pd
28/2:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
28/3:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        print(labels)
        print(labels.float())
        
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        outputs = outputs.squeeze(1)
        loss = criterion(outputs, labels.float())

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            outputs = outputs.squeeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/4:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        print(labels)
        print(labels.float())
        
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        outputs = outputs.squeeze(1)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            outputs = outputs.squeeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/5:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        outputs = outputs.squeeze(1)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            outputs = outputs.squeeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/6:
# constant for classes
classes = ('handwritten', 'typed')

# Build confusion matrix
cf_matrix = confusion_matrix(y_true, y_pred)
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/7:
# constant for classes
classes = ('handwritten', 'typed')

# targets = targets.argmax(1).cpu().numpy()

# Build confusion matrix
cf_matrix = confusion_matrix(y_true.argmax(1).cpu().numpy(), y_pred.argmax(1).cpu().numpy())
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/8:
# constant for classes
classes = ('handwritten', 'typed')

# targets = targets.argmax(1).cpu().numpy()

# Build confusion matrix
cf_matrix = confusion_matrix(y_true, y_pred)
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/9: y_true[0]
28/10: y_true[0].cpu()
28/11:
# constant for classes
classes = ('handwritten', 'typed')

# targets = targets.argmax(1).cpu().numpy()
y_true = [tensor.cpu() for tensor in y_true]
y_pred = [tensor.cpu() for tensor in y_pred]

# Build confusion matrix
cf_matrix = confusion_matrix(y_true, y_pred)
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/12:
# constant for classes
classes = ('handwritten', 'typed')

# targets = targets.argmax(1).cpu().numpy()
y_true = [tensor.cpu().numpy() for tensor in y_true]
y_pred = [tensor.cpu().numpy() for tensor in y_pred]

# Build confusion matrix
cf_matrix = confusion_matrix(y_true, y_pred)
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/13: y_true[0].cpu()
28/14:
# constant for classes
classes = ('handwritten', 'typed')

# targets = targets.argmax(1).cpu().numpy()
# y_true = [tensor.cpu().numpy() for tensor in y_true]
# y_pred = [tensor.cpu().numpy() for tensor in y_pred]

# Build confusion matrix
cf_matrix = confusion_matrix(y_true, y_pred)
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/15: y_true[0]
28/16: np.unique(y_true)
28/17:
import numpy as np
np.unique(y_true)
28/18:
import numpy as np

np.unique(y_pred)
28/19:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        outputs = outputs.squeeze(1)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            outputs = outputs.squeeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/20:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(weights=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        outputs = outputs.squeeze(1)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            outputs = outputs.squeeze(1)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/21:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(weights=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/22:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(weights=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/23:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(weights=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(outputs) # Save Prediction
            
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/24:
# constant for classes
classes = ('handwritten', 'typed')

# targets = targets.argmax(1).cpu().numpy()
# y_true = [tensor.cpu().numpy() for tensor in y_true]
# y_pred = [tensor.cpu().numpy() for tensor in y_pred]

# Build confusion matrix
cf_matrix = confusion_matrix(y_true, y_pred)
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/25:
y_true = [tensor.cpu().numpy() for tensor in y_true]
y_pred = [tensor.cpu().numpy() for tensor in y_pred]
28/26:
# constant for classes
classes = ('handwritten', 'typed')

# Build confusion matrix
cf_matrix = confusion_matrix(y_true, y_pred)
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/27:
import numpy as np

np.unique(y_pred)
28/28:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
28/29:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(weights=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(torch.round(torch.sigmoid(outputs))) # Save Prediction
            
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'Epoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/30:
y_true = [tensor.cpu().numpy() for tensor in y_true]
y_pred = [tensor.cpu().numpy() for tensor in y_pred]
28/31:
# constant for classes
classes = ('handwritten', 'typed')

# Build confusion matrix
cf_matrix = confusion_matrix(y_true, y_pred)
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/32:
import matplotlib.pyplot as plt

# constant for classes
classes = ('handwritten', 'typed')

# Build confusion matrix
cf_matrix = confusion_matrix(y_true, y_pred)
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/33:
import numpy as np

np.unique(y_pred)
28/34:
import numpy as np

pd.get_dummies(y_pred)
28/35:
import numpy as np

pd.get_dummies(pd.Series(y_pred))
28/36:
import numpy as np
from sklearn.preprocessing import OneHotEncoder

# Reshape the input array to a column vector
y_pred = y_pred.reshape(-1, 1)

# Create an instance of the OneHotEncoder class
encoder = OneHotEncoder()

# Fit and transform the input array to one-hot encoded representation
one_hot_labels = encoder.fit_transform(y_pred).toarray()

# Print the one-hot encoded array
print(one_hot_labels)
28/37:
import numpy as np

np.uniqu(y_pred)
28/38:
import os
import shutil
import json

from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler

from sklearn.metrics import confusion_matrix

# ! pip install seaborn
import seaborn as sn
import pandas as pd
28/39:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(weights=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(torch.round(torch.sigmoid(outputs))) # Save Prediction
            
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            total += labels.size(0)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/40:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(weights=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(torch.round(torch.sigmoid(outputs))) # Save Prediction
            
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            total += labels.size(0)
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/41:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(weights=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(labels)
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(torch.round(torch.sigmoid(outputs))) # Save Prediction
            
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/42: print(y_true, y_pred)
28/43:
y_true_cpu = [tensor.cpu().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().numpy() for tensor in y_pred]
28/44:
import matplotlib.pyplot as plt

# constant for classes
classes = ('handwritten', 'typed')

# Build confusion matrix
cf_matrix = confusion_matrix(y_true_cpu, y_pred_cpu)
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/45:
accuracy = np.sum(np.equal(y_true_cpu, y_pred_cpu))/len(y_true_cpu)
accuracy
28/46:
import matplotlib.pyplot as plt

# constant for classes
classes = ('handwritten', 'typed')

# Build confusion matrix
cf_matrix = confusion_matrix(pd.get_dummies(y_true_cpu), pd.get_dummies(y_pred_cpu))
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/47:
import matplotlib.pyplot as plt

# constant for classes
classes = ('handwritten', 'typed')

# Build confusion matrix
cf_matrix = confusion_matrix(pd.get_dummies(y_true_cpu.values), pd.get_dummies(y_pred_cpu.values))
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/48:
import matplotlib.pyplot as plt

# constant for classes
classes = ('handwritten', 'typed')

# Build confusion matrix
cf_matrix = confusion_matrix(pd.get_dummies(y_true_cpu[:].values), pd.get_dummies(y_pred_cpu[:].values))
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/49:
y_true_cpu = [tensor.cpu().numpy().values for tensor in y_true]
y_pred_cpu = [tensor.cpu().numpy().values for tensor in y_pred]
28/50:
y_true_cpu = [tensor.cpu().numpy().data for tensor in y_true]
y_pred_cpu = [tensor.cpu().numpy().values for tensor in y_pred]
28/51:
y_true_cpu = [tensor.cpu().numpy().data for tensor in y_true]
y_pred_cpu = [tensor.cpu().numpy().data for tensor in y_pred]
28/52:
import matplotlib.pyplot as plt

# constant for classes
classes = ('handwritten', 'typed')

# Build confusion matrix
cf_matrix = confusion_matrix(pd.get_dummies(y_true_cpu), pd.get_dummies(y_pred_cpu))
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/53:
y_true_cpu = [tensor.cpu().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().numpy() for tensor in y_pred]
28/54:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(weights=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 50
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(labels)
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(torch.round(torch.sigmoid(outputs))) # Save Prediction
            
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/55:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# Load the pre-trained ResNet model
model = models.resnet50(weights=True)

# Modify the first convolutional layer to accept grayscale images
model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# Modify the last fully connected layer to match the number of classes
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes-1)

# # Load the pre-trained VGG16 model
# model = models.vgg16(pretrained=True)

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 50
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

#     print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
#         print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(torch.round(torch.sigmoid(outputs))) # Save Prediction
            
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/56:
y_true_cpu = [tensor.cpu().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().numpy() for tensor in y_pred]
28/57:
import matplotlib.pyplot as plt

# constant for classes
classes = ('handwritten', 'typed')

# Build confusion matrix
cf_matrix = confusion_matrix(pd.get_dummies(y_true_cpu), pd.get_dummies(y_pred_cpu))
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/58:
y_true_cpu = [tensor.cpu().item() for tensor in y_true]
y_pred_cpu = [tensor.cpu().item() for tensor in y_pred]
28/59:
import matplotlib.pyplot as plt

# constant for classes
classes = ('handwritten', 'typed')

# Build confusion matrix
cf_matrix = confusion_matrix(pd.get_dummies(y_true_cpu), pd.get_dummies(y_pred_cpu))
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/60: print(y_pred_cpu)
28/61: dataset.size()
28/62: dataset.classes
28/63: dataset.samples
28/64:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 50
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

#     print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
#         print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(torch.round(torch.sigmoid(outputs))) # Save Prediction
            
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/65:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 50
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(torch.round(torch.sigmoid(outputs))) # Save Prediction
            
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/66:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
model = model.to(device)

# Training loop
num_epochs = 50
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            y_pred.extend(torch.round(torch.sigmoid(outputs))) # Save Prediction
            
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
28/67:
y_true_cpu = [tensor.cpu().item() for tensor in y_true]
y_pred_cpu = [tensor.cpu().item() for tensor in y_pred]
28/68:
accuracy = np.sum(np.equal(y_true_cpu, y_pred_cpu))/len(y_true_cpu)
accuracy
28/69:
for images, labels in val_loader:
    images = images.to(device)
    labels = labels.float().unsqueeze(1).to(device)

    y_true.extend(labels) # Save Truth

    outputs = model(images)

    loss = criterion(outputs, labels)

    val_loss += loss.item()

    predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
    y_pred.extend(predicted) # Save Prediction
    val_correct += (predicted == labels).sum().item()
    
    print((predicted == labels))
28/70:
# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
28/71:
for images, labels in val_loader:
    images = images.to(device)
    labels = labels.float().unsqueeze(1).to(device)

    y_true.extend(labels) # Save Truth

    outputs = model(images)

    loss = criterion(outputs, labels)

    val_loss += loss.item()

    predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
    y_pred.extend(predicted) # Save Prediction
    val_correct += (predicted == labels).sum().item()
    
    print((predicted == labels))
28/72:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
28/73:
for images, labels in val_loader:
    images = images.to(device)
    labels = labels.float().unsqueeze(1).to(device)

    y_true.extend(labels) # Save Truth

    outputs = model(images)

    loss = criterion(outputs, labels)

    val_loss += loss.item()

    predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
    y_pred.extend(predicted) # Save Prediction
    val_correct += (predicted == labels).sum().item()
    
    print((predicted == labels))
28/74:
accuracy = np.sum(np.equal(y_true_cpu, y_pred_cpu))/len(y_true_cpu)
accuracy
28/75:
accuracy = np.sum(np.equal(y_true_cpu, y_pred_cpu))/len(y_true_cpu)
accuracy
28/76:
accuracy = np.sum(np.equal(y_true_cpu, y_pred_cpu))/len(y_true_cpu)
accuracy
28/77:
accuracy = np.sum(np.equal(y_true_cpu, y_pred_cpu))/len(y_true_cpu)
accuracy
28/78:
accuracy = np.sum(np.equal(y_true_cpu, y_pred_cpu))/len(y_true_cpu)
accuracy
28/79:
accuracy = np.sum(np.equal(y_true_cpu, y_pred_cpu))/len(y_true_cpu)
accuracy
28/80:
accuracy = np.sum(np.equal(y_true_cpu, y_pred_cpu))/len(y_true_cpu)
accuracy
28/81:
accuracy = np.sum(np.equal(y_true_cpu, y_pred_cpu))/len(y_true_cpu)
accuracy
28/82:
accuracy = np.sum(np.equal(y_true_cpu, y_pred_cpu))/len(y_true_cpu)
accuracy
28/83:
y_true_cpu = [tensor.cpu().item() for tensor in y_true]
y_pred_cpu = [tensor.cpu().item() for tensor in y_pred]
28/84:
import matplotlib.pyplot as plt

# constant for classes
classes = ('handwritten', 'typed')

# Build confusion matrix
cf_matrix = confusion_matrix(pd.get_dummies(y_true_cpu), pd.get_dummies(y_pred_cpu))
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
28/85: dataset.samples[0]
28/86: dataset.samples[0][1]
28/87: dataset.samples[0][1] == 1
28/88: np.where(dataset.samples[0][1] == 1)
28/89: np.where(dataset.samples[0][1] == 1, dataset.samples)
28/90: np.where(dataset.samples[0][1] == 1, dataset.samples, dataset.samples)
28/91: np.where(dataset.samples[0][1] == '1', dataset.samples, dataset.samples)
28/92: np.where(dataset.samples[0][1] == '1', dataset.samples, "")
28/93: np.where(dataset.samples[0][1] == '1', dataset.samples, "").sum()
28/94: np.where(dataset.samples[0][1] == '1', 1, 0).sum()
28/95: np.where(dataset.samples[0][1] == 1, 1, 0).sum()
28/96: np.where(dataset.samples[0][1] == 1, dataset.samples, 0).sum()
28/97: np.where(dataset.samples[0][1] == 1, dataset.samples, 0)
28/98: np.where(dataset.samples[0][1] == 1, dataset.samples[0][1], 0)
28/99: np.where(dataset.samples[0][1] == 1, dataset.samples[0][1], 0).sum()
28/100: np.where(dataset.samples[0][1] == 1, dataset.samples[0][1], dataset.samples[0][1])
28/101: np.where(dataset.samples[0][1] == 1, dataset.samples[:][1], dataset.samples[:][1])
28/102: print((predicted == labels))
28/103: print((predicted == labels).sum())
28/104: print((predicted == labels).sum())
28/105: val_loader.dataset
28/106: len(val_loader.dataset)
28/107:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

print(indices, train_indices, val_indices)

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
28/108:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

print(indices, train_indices, val_indices)

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
28/109:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

print(indices, train_indices, val_indices)

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
28/110:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
#     transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

print(indices, train_indices, val_indices)

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
29/1:
import os
import shutil
import json

from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler

from sklearn.metrics import confusion_matrix

# ! pip install seaborn
import seaborn as sn
import pandas as pd
29/2:
def get_image_paths(root_dir, data_level):
    image_paths = []
    for root, _, files in os.walk(os.path.join(root_dir, data_level)):
        for file in files:
            if file.endswith('.png'):
                image_paths.append(os.path.join(root, file))
    return image_paths


# Get IAM image paths
dataset_levels = ["lines", "sentences"] # "words"
hw_image_paths = []

for level in dataset_levels:
    hw_image_paths += get_image_paths("data/", level)

print(len(hw_image_paths))

# Get typed text image paths
# ty_image_paths = []

# for level in dataset_levels:
#     ty_image_paths += get_image_paths("data/", level)

# print(len(hw_image_paths))
29/3:
def copy_files(destination_folder, file_path_list):
    for source_path in file_path_list:
        destination_path = os.path.join(destination_folder, source_path.split("/")[-1])
        shutil.copy2(source_path, destination_path)
29/4:
destination_folder = 'data/all_data/handwritten'
copy_files(destination_folder, hw_image_paths)
29/5:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
#     transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print(len(dataset), train_size, val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

print(indices, train_indices, val_indices)
print(len(indices), len(train_indices), len(val_indices))

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
29/6:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
#     transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print(len(dataset), train_size, val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

print(indices, train_indices, val_indices)
print(len(indices), len(train_indices), len(val_indices))

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
29/7:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
#     transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print(len(dataset), train_size, val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

print(indices, train_indices, val_indices)
print(len(indices), len(train_indices), len(val_indices))

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
30/1:
import os
import shutil
import json

from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler

from sklearn.metrics import confusion_matrix

# ! pip install seaborn
import seaborn as sn
import pandas as pd
30/2:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
#     transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print(len(dataset), train_size, val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

print(indices, train_indices, val_indices)
print(len(indices), len(train_indices), len(val_indices))

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
30/3:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
#     transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print(len(dataset), train_size, val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

print(indices, train_indices, val_indices)
print(len(indices), len(train_indices), len(val_indices))

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
31/1:
import os
import shutil
import json

from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler

from sklearn.metrics import confusion_matrix

# ! pip install seaborn
import seaborn as sn
import pandas as pd
31/2:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
#     transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print(len(dataset), train_size, val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

print(indices, train_indices, val_indices)
print(len(indices), len(train_indices), len(val_indices))

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
31/3:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print(len(dataset), train_size, val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

print(indices, train_indices, val_indices)
print(len(indices), len(train_indices), len(val_indices))

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
31/4:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print(len(dataset), train_size, val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

print(indices, train_indices, val_indices)
print(len(indices), len(train_indices), len(val_indices))

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
31/5:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
31/6:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print(len(dataset), train_size, val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

print(indices, train_indices, val_indices)
print(len(indices), len(train_indices), len(val_indices))

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
31/7:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
31/8: len(val_loader.dataset)
31/9: len(val_loader.dataset)-1
31/10:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)

# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print(len(dataset), train_size, val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

print(indices, train_indices, val_indices)
print(len(indices), len(train_indices), len(val_indices))

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
31/11:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
31/12:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print(len(dataset), train_size, val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

print(indices, train_indices, val_indices)
print(len(indices), len(train_indices), len(val_indices))

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
31/13:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print(len(dataset), train_size, val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# print(indices, train_indices, val_indices)
print(len(indices), len(train_indices), len(val_indices))

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
31/14:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
31/15:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
#     transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
31/16:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print(len(dataset), train_size, val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# print(indices, train_indices, val_indices)
print(len(indices), len(train_indices), len(val_indices))

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
31/17:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(pretrained=True)

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    # print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        # print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            y_pred.extend(predicted) # Save Prediction
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(train_loader.dataset)
    train_accuracy = train_correct / len(train_loader.dataset)

    val_loss /= len(val_loader.dataset)
    val_accuracy = val_correct / len(val_loader.dataset)

    print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
31/18: print(len(train_indices)/len(indices), len(val_indices)/len(indices))
31/19:
y_true_cpu = [tensor.cpu().item() for tensor in y_true]
y_pred_cpu = [tensor.cpu().item() for tensor in y_pred]
31/20:
from sklearn.metrics import f1_score

val_preds = []
val_targets = []

val_preds.extend(y_pred.cpu().detach().numpy())
val_targets.extend(y_true.cpu().detach().numpy())

val_f1 = f1_score(val_targets, val_preds)
val_f1
31/21:
from sklearn.metrics import f1_score

# val_preds = []
# val_targets = []

# val_preds.extend(y_pred.cpu().detach().numpy())
# val_targets.extend(y_true.cpu().detach().numpy())

val_f1 = f1_score(y_true_cpu, y_pred_cpu)
val_f1
31/22:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    # print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        # print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            y_pred.extend(predicted) # Save Prediction
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(val_indices)
    train_accuracy = train_correct / len(train_indices)

    val_loss /= len(val_indices)
    val_accuracy = val_correct / len(val_indices)

    print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
31/23:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 1
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0

    # print("Batch in progress: ")
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        # print(batch_idx, end=" ")
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            y_pred.extend(predicted) # Save Prediction
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= len(val_indices)
    train_accuracy = train_correct / len(train_indices)

    val_loss /= len(val_indices)
    val_accuracy = val_correct / len(val_indices)

    print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/best_model.pth')

print("Training complete!")
32/1:
import os
import shutil
import json

from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler

from sklearn.metrics import confusion_matrix, f1_score

# ! pip install seaborn
import seaborn as sn
import pandas as pd
32/2:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
#     transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
32/3:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print(len(dataset), train_size, val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 128

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
32/4:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
32/5:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    for batch_idx, (images, labels) in enumerate(train_loader):
        
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        train_correct += (predicted == labels).sum().item()

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_correct = 0
    
    y_pred = []
    y_true = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            y_true.extend(labels) # Save Truth

            outputs = model(images)
            
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            y_pred.extend(predicted) # Save Prediction
            val_correct += (predicted == labels).sum().item()

    # Calculate average loss and accuracy
    train_loss /= train_size
    train_accuracy = train_correct / train_size

    val_loss /= val_size
    val_accuracy = val_correct / val_size

    print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

    # Save the best model based on validation accuracy
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        torch.save(model.state_dict(), 'model/vgg16_best_model.pth')

print("Training complete!")
33/1:
import os
import shutil
import json

from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler

from sklearn.metrics import confusion_matrix, f1_score

# ! pip install seaborn
import seaborn as sn
import pandas as pd
33/2:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
#     transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
33/3:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print(len(dataset), train_size, val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
33/4:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
33/5:
# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
33/6:
import os
import shutil
import json

from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler

from sklearn.metrics import confusion_matrix, f1_score

# ! pip install seaborn
import seaborn as sn
import pandas as pd

from tqdm import tqdm
33/7:
import os
import shutil
import json

# ! pip install seaborn
import seaborn as sn
import pandas as pd

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
33/8:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=(train_size/batch_size)) as pbar:
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)

            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                y_true.extend(labels) # Save Truth

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                y_pred.extend(predicted) # Save Prediction
                val_correct += (predicted == labels).sum().item()

        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size

        print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

        # Save the best model based on validation accuracy
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')

print("Training complete!")
33/9:
import os
import shutil
import json

# ! pip install seaborn
import seaborn as sn
import pandas as pd

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
33/10:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=round(train_size/batch_size)) as pbar:
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)

            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                y_true.extend(labels) # Save Truth

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                y_pred.extend(predicted) # Save Prediction
                val_correct += (predicted == labels).sum().item()

        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size

        print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')

        # Save the best model based on validation accuracy
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')

print("Training complete!")
33/11:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
33/12:
# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
33/13:
# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
33/14:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels.cpu().detach().numpy()) # Save Truth
            train_pred.extend(predicted.cpu().detach().numpy()) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels.cpu().detach().numpy()) # Save Truth
                y_pred.extend(predicted.cpu().detach().numpy()) # Save Prediction

        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        train_f1 = f1_score(train_true, train_pred)
        val_f1 = f1_score(y_true, y_pred)

        print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f},
              f'Train Loss: {train_loss:.4f}, Train F1-score: {train_f1:.4f}, '
              f'Val Loss: {val_loss:.4f}, Val F1-score: {val_f1:.4f}')

        # Save the best model based on validation accuracy
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')

print("Training complete!")
33/15:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels.cpu().detach().numpy()) # Save Truth
            train_pred.extend(predicted.cpu().detach().numpy()) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels.cpu().detach().numpy()) # Save Truth
                y_pred.extend(predicted.cpu().detach().numpy()) # Save Prediction

        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        train_f1 = f1_score(train_true, train_pred)
        val_f1 = f1_score(y_true, y_pred)

        print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '
              f'Train Loss: {train_loss:.4f}, Train F1-score: {train_f1:.4f}, '
              f'Val Loss: {val_loss:.4f}, Val F1-score: {val_f1:.4f}')

        # Save the best model based on validation accuracy
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')

print("Training complete!")
33/16:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels.cpu().detach().numpy()) # Save Truth
            train_pred.extend(predicted.cpu().detach().numpy()) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels.cpu().detach().numpy()) # Save Truth
                y_pred.extend(predicted.cpu().detach().numpy()) # Save Prediction

        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        train_f1 = f1_score(train_true, train_pred)
        val_f1 = f1_score(y_true, y_pred)

        print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '
              f'Train Loss: {train_loss:.4f}, Train F1-score: {train_f1:.4f}, '
              f'Val Loss: {val_loss:.4f}, Val F1-score: {val_f1:.4f}')

        # Save the best model based on validation accuracy
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')

print("Training complete!")
33/17:
import os
import shutil
import json
import math

# ! pip install seaborn
import seaborn as sn
import pandas as pd

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
33/18:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels.cpu().detach().numpy()) # Save Truth
            train_pred.extend(predicted.cpu().detach().numpy()) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels.cpu().detach().numpy()) # Save Truth
                y_pred.extend(predicted.cpu().detach().numpy()) # Save Prediction

        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        train_f1 = f1_score(train_true, train_pred)
        val_f1 = f1_score(y_true, y_pred)

        print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '
              f'Train Loss: {train_loss:.4f}, Train F1-score: {train_f1:.4f}, '
              f'Val Loss: {val_loss:.4f}, Val F1-score: {val_f1:.4f}')

        # Save the best model based on validation accuracy
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')

print("Training complete!")
33/19:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels.cpu().detach().numpy()) # Save Truth
            train_pred.extend(predicted.cpu().detach().numpy()) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels.cpu().detach().numpy()) # Save Truth
                y_pred.extend(predicted.cpu().detach().numpy()) # Save Prediction

        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        train_f1 = f1_score(train_true, train_pred)
        val_f1 = f1_score(y_true, y_pred)

        print(f'\nEpoch [{epoch + 1}/{num_epochs}], '
              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '
              f'Train Loss: {train_loss:.4f}, Train F1-score: {train_f1:.4f}, '
              f'Val Loss: {val_loss:.4f}, Val F1-score: {val_f1:.4f}')

        # Save the best model based on validation accuracy
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')

print("Training complete!")
34/1:
import os
import shutil
import json
import math

# ! pip install seaborn
import seaborn as sn
import pandas as pd

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
34/2:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
#     transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
34/3:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
34/4:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
34/5:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'\nEpoch [{epoch + 1}/{num_epochs}], \n'
              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f},\n'
              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f}'
              )

        # Save the best model based on validation accuracy
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')

print("Training complete!")
35/1:
import os
import shutil
import json
import math

# ! pip install seaborn
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
35/2:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
#     transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
35/3:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
35/4:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
35/5:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("Training complete!")
35/6: print(y_true_cpu[0], y_pred_cpu[0])
35/7: print(type(y_true_cpu[0]), y_pred_cpu[0])
35/8:
# constant for classes
classes = ('handwritten', 'typed')

# Build confusion matrix
cf_matrix = confusion_matrix(y_true_cpu, y_pred_cpu)
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
35/9:
import os
import shutil
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
35/10:
# constant for classes
classes = ('handwritten', 'typed')

# Build confusion matrix
cf_matrix = confusion_matrix(y_true_cpu, y_pred_cpu)
df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],
                     columns = [i for i in classes])
plt.figure(figsize = (12,7))
sn.heatmap(df_cm, annot=True)
# plt.savefig('output.png')
35/11: print(type(y_true_cpu), y_pred_cpu[0])
35/12: print(pd.Series(y_true_cpu), y_pred_cpu[0])
35/13: print(y_true_cpu[0] == 1, y_pred_cpu[0])
35/14: print(y_true_cpu[0] == 0, y_pred_cpu[0])
35/15: from craft_text_detector import Craft
35/16: from craft_text_detector import Craft
37/1:
import os
import shutil
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
37/2:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
#     transforms.Resize(256),
#     transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
37/3:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
37/4:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
37/5:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("Training complete!")
37/6:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
#     transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
37/7:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
37/8:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("Training complete!")
38/1:
import os
import shutil
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
38/2:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
#     transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
38/3:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
38/4:
# Load the best model
best_model = resnet50(pretrained=False)
best_model.fc = nn.Linear(num_features, num_classes)
best_model.load_state_dict(torch.load('model/vgg16_best_model.pth'))
best_model = best_model.to(device)
38/5:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth'))
best_model = best_model.to(device)
38/6:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth'))
best_model = best_model.to(device)
38/7:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth'))
best_model = best_model.to(device)
38/8:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder('data/test_data', transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)

y_pred = []
y_true = []

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = model(images)

        loss = criterion(outputs, labels)

        val_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        val_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]

test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
38/9:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder('data/test_data', transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)

best_model.eval()
y_pred = []
y_true = []

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = model(images)

        loss = criterion(outputs, labels)

        val_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        val_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]

test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
38/10:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder('data/test_data/', transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)

best_model.eval()
y_pred = []
y_true = []

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = model(images)

        loss = criterion(outputs, labels)

        val_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        val_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]

test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
38/11:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
38/12:
# Define the list of folder names to exclude
exclude_folders = ['.ipynb_checkpoints']

# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess, exclude_folders=exclude_folders)
test_loader = DataLoader(test_dataset, batch_size=32)
38/13:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
38/14:
best_model.eval()
y_pred = []
y_true = []

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = model(images)

        loss = criterion(outputs, labels)

        val_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        val_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]

test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
38/15:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]

test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
38/16:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]

test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
38/17: y_true_cpu
38/18: y_true_cpu, y_pred_cpu
38/19: test_correct/len(test_loader)
38/20: test_correct/len(test_loader.dataset)
38/21:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
38/22:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
38/23:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]

test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
38/24: test_correct/len(test_loader.dataset)
38/25: y_true_cpu, y_pred_cpu
38/26:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
38/27:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
40/1:
import os
import shutil
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
40/2:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
40/3:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
40/4:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
40/5:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 1

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("Training complete!")
40/6: torch.save(model.state_dict(), 'model/vgg16_1_epoch.pth')
40/7:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_1_epoch.pth')) # vgg16_best_model
best_model = best_model.to(device)
40/8:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
40/9:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
40/10:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
41/1:
import os
import shutil
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
41/2:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
41/3:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
41/4:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/5:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
41/6:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
41/7:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/8:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
#     transforms.CenterCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
41/9:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
41/10:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/11:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.RandomCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
41/12:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
41/13:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/14:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.RandomCrop(300),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
41/15:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
41/16:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/17:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize(300),
    transforms.RandomCrop(300),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
41/18:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
41/19:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/20:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
41/21:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 1

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("Training complete!")
41/22: torch.save(model.state_dict(), 'model/vgg16_1_epoch.pth')
41/23:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_1_epoch.pth')) # vgg16_best_model
best_model = best_model.to(device)
41/24:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
41/25:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/26:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
41/27:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
41/28: test_correct/len(test_loader.dataset)
41/29: y_true_cpu, y_pred_cpu
41/30:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(10, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/31:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/32:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(8, 4, figsize=(12, 3))
    for i in range(32):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/33:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(8, 4, figsize=(12, 3))
    for i in range(8):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/34:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(8, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/35:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(8, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/36:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/37:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/38:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/39:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/40:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, 4, figsize=(12, 3))
    for i in range(16):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/41:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
41/42:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, 4, figsize=(12, 3))
    for i in range(16):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/43:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
#     break  # Only display one batch of transformed images
41/44:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize((300, 500)),
#     transforms.RandomCrop(300),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
41/45:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
41/46:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/47:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize((300, 800)),
#     transforms.RandomCrop(300),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
41/48:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
41/49:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/50:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize((300, 1200)),
#     transforms.RandomCrop(300),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
41/51:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
41/52:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
41/53:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 1

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("Training complete!")
41/54:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
41/55:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
#     break  # Only display one batch of transformed images
42/1:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
42/2:
import os
import shutil
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
42/3:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize((300, 1200)),
#     transforms.RandomCrop(300),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
42/4:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
42/5:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
42/6:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
42/7:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 1

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("Training complete!")
42/8:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize((256, 1000)),
#     transforms.RandomCrop(300),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
42/9:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
42/10:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
42/11:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize((256, 1000)),
    transforms.RandomCrop((256, 500)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
42/12:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
42/13:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
42/14:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 1

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("Training complete!")
43/1:
import os
import shutil
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
43/2:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize((256, 1000)),
    transforms.RandomCrop((256, 500)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
43/3:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
43/4:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
43/5:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
43/6:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
43/7:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
43/8:
# Set random seed for reproducibility
torch.manual_seed(42)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 1

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("Training complete!")
43/9:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
43/10:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
43/11:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
#     break  # Only display one batch of transformed images
43/12:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
43/13:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
43/14:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
43/15: test_correct/len(test_loader.dataset)
43/16:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 5

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("Training complete!")
43/17:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
43/18:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
43/19:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
#     break  # Only display one batch of transformed images
43/20:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
43/21:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
43/22: test_correct/len(test_loader.dataset)
43/23: y_true_cpu, y_pred_cpu
43/24: torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
43/25:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
43/26:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
43/27:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
#     break  # Only display one batch of transformed images
43/28:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
43/29:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
43/30: test_correct/len(test_loader.dataset)
43/31: y_true_cpu, y_pred_cpu
43/32:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 5

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("Training complete!")
43/33:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
43/34:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
43/35:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
#     break  # Only display one batch of transformed images
43/36:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
43/37:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
43/38: test_correct/len(test_loader.dataset)
43/39: y_true_cpu, y_pred_cpu
44/1:
# ! wget --save-cookies data/cookies.txt \
#  --keep-session-cookies \
#  --post-data 'email=<EMAIL>&password=<PASSWORD>' \
#  --delete-after \
#  https://fki.tic.heia-fr.ch/login
44/2:
# ! wget --save-cookies data/cookies.txt \
#  --keep-session-cookies \
#  --post-data 'email=<EMAIL>&password=<PASSWORD>' \
#  --delete-after \
#  https://fki.tic.heia-fr.ch/login
44/3:
# ! wget -P data/ \
#     --load-cookies data/cookies.txt \
#      https://fki.tic.heia-fr.ch/DBs/iamDB/data/words.tgz
# ! wget -P data/ \
#     --load-cookies data/cookies.txt \
#      https://fki.tic.heia-fr.ch/DBs/iamDB/data/lines.tgz
# ! wget -P data/ \
#     --load-cookies data/cookies.txt \
#      https://fki.tic.heia-fr.ch/DBs/iamDB/data/sentences.tgz
# ! wget -P data/ \
#     --load-cookies data/cookies.txt \
#      https://fki.tic.heia-fr.ch/DBs/iamDB/data/xml.tgz
44/4: # ! wget -O data/FUNSD_data.zip https://guillaumejaume.github.io/FUNSD/dataset.zip
44/5:
# ! mkdir -p data/{words,sentences,xml,lines}

# ! tar -xzf data/words.tgz -C data/words
# ! tar -xzf data/lines.tgz -C data/lines
# ! tar -xzf data/sentences.tgz -C data/sentences
# ! tar -xzf data/xml.tgz -C data/xml
44/6: # ! unzip -q data/FUNSD_data.zip -d data/FUNSD
43/40:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize((256, 1000)),
#     transforms.RandomCrop((256, 500)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
43/41:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
43/42:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
43/43:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 5

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("Training complete!")
44/7:
def get_image_paths(root_dir, data_level):
    image_paths = []
    for root, _, files in os.walk(os.path.join(root_dir, data_level)):
        for file in files:
            if file.endswith('.png'):
                image_paths.append(os.path.join(root, file))
    return image_paths


# Get IAM image paths
dataset_levels = ["lines", "sentences"] # "words"
hw_image_paths = []

for level in dataset_levels:
    hw_image_paths += get_image_paths("data/", level)

print(len(hw_image_paths))
44/8: import os
44/9:
import os
import json
44/10:
def get_image_paths(root_dir, data_level):
    image_paths = []
    for root, _, files in os.walk(os.path.join(root_dir, data_level)):
        for file in files:
            if file.endswith('.png'):
                image_paths.append(os.path.join(root, file))
    return image_paths


# Get IAM image paths
dataset_levels = ["lines", "sentences"] # "words"
hw_image_paths = []

for level in dataset_levels:
    hw_image_paths += get_image_paths("data/", level)

print(len(hw_image_paths))
44/11:
# destination_folder = 'data/all_data/handwritten'
# copy_files(destination_folder, hw_image_paths)
44/12:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):
    
    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["other"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
                        # Save the cropped image
                        cropped_image.save(output_path, "PNG")
44/13:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):
    
    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["other"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
                        # Save the cropped image
                        cropped_image.save(output_path, "PNG")
44/14:
# image_folder = 'data/FUNSD/dataset/training_data/images'
# annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
# output_folder = 'data/all_data/typed'

# crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
44/15:
# image_folder = 'data/FUNSD/dataset/training_data/images'
# annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
# output_folder = 'data/all_data/typed'

# crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
44/16:
# image_folder = 'data/FUNSD/dataset/testing_data/images'
# annotation_folder = 'data/FUNSD/dataset/testing_data/annotations'

# crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
44/17:
# mkdir images
# tar -xzf data/CVIT/iiit-hws.tar.gz -C data/CVIT/images

# mkdir groundtruth
# tar -xzf data/CVIT/groundtruth.tar.gz -C data/CVIT/groundtruth
44/18:
import scipy.io
mat = scipy.io.loadmat('file.mat')
44/19:
import scipy.io
mat = scipy.io.loadmat('data/CVIT/groundtruth/groundtruth/IIIT-HWS-10K.mat')
44/20:
# import scipy.io
# mat = scipy.io.loadmat('')

import numpy as np
import h5py

f = h5py.File('data/CVIT/groundtruth/groundtruth/IIIT-HWS-10K.mat','r')
# data = f.get('data/variable1')
# data = np.array(data) # For converting to a NumPy array
44/21:
# import scipy.io
# mat = scipy.io.loadmat('')

import numpy as np

! pip install h5py
import h5py

f = h5py.File('data/CVIT/groundtruth/groundtruth/IIIT-HWS-10K.mat','r')
# data = f.get('data/variable1')
# data = np.array(data) # For converting to a NumPy array
44/22:
# import scipy.io
# mat = scipy.io.loadmat('')

import numpy as np
import h5py

f = h5py.File('data/CVIT/groundtruth/groundtruth/IIIT-HWS-10K.mat','r')
# data = f.get('data/variable1')
# data = np.array(data) # For converting to a NumPy array
44/23:
# import scipy.io
# mat = scipy.io.loadmat('')

import numpy as np
import h5py

f = h5py.File('data/CVIT/groundtruth/groundtruth/IIIT-HWS-10K.mat','r')
# data = f.get('data/variable1')
# data = np.array(data) # For converting to a NumPy array
f
44/24:
# import scipy.io
# mat = scipy.io.loadmat('')

import numpy as np
import h5py

f = h5py.File('data/CVIT/groundtruth/groundtruth/IIIT-HWS-10K.mat','r')
# data = f.get('data/variable1')
# data = np.array(data) # For converting to a NumPy array
f.values
44/25:
# import scipy.io
# mat = scipy.io.loadmat('')

import numpy as np
import h5py

f = h5py.File('data/CVIT/groundtruth/groundtruth/IIIT-HWS-10K.mat','r')
data = f.get('data/variable1')
data = np.array(data) # For converting to a NumPy array
data
44/26:
# import scipy.io
# mat = scipy.io.loadmat('')

import numpy as np
import h5py

f = h5py.File('data/CVIT/groundtruth/groundtruth/IIIT-HWS-10K.mat','r')
data = f.get('data/variable')
data = np.array(data) # For converting to a NumPy array
data
44/27:
# import scipy.io
# mat = scipy.io.loadmat('')

import numpy as np
import h5py

f = h5py.File('data/CVIT/groundtruth/groundtruth/IIIT-HWS-10K.mat','r')
data = f.get('data/')
data = np.array(data) # For converting to a NumPy array
data
43/44:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
43/45:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
43/46:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
#     break  # Only display one batch of transformed images
43/47:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
43/48:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
43/49: test_correct/len(test_loader.dataset)
43/50: y_true_cpu, y_pred_cpu
43/51:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=4)
43/52:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
#     break  # Only display one batch of transformed images
43/53:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(8):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
#     break  # Only display one batch of transformed images
43/54:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
43/55:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(32):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
#     break  # Only display one batch of transformed images
43/56:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=4)
43/57:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=preprocess)
test_loader = DataLoader(test_dataset, batch_size=4)
43/58:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(32):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
#     break  # Only display one batch of transformed images
43/59:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
#     break  # Only display one batch of transformed images
44/28:
# import scipy.io
# mat = scipy.io.loadmat('')

import numpy as np
import h5py

f = h5py.File('data/CVIT/groundtruth/groundtruth/IIIT-HWS-10K.mat','r')
f.keys()
44/29:
data = f.get('data/list')
data = np.array(data) # For converting to a NumPy array
data
44/30:
data = f.get("data['list']")
data = np.array(data) # For converting to a NumPy array
data
44/31:
data = f['list']
data = np.array(data) # For converting to a NumPy array
data
44/32:
data = f['list']['ALLtext']
data = np.array(data) # For converting to a NumPy array
data
44/33:
data = f['list']
data = np.array(data) # For converting to a NumPy array
data
44/34:
data = f['list']['ALLlabels']
data = np.array(data) # For converting to a NumPy array
data
44/35:
data = f['list']
data = np.array(data) # For converting to a NumPy array
data
44/36:
data = f['list']['ALLnames']
data = np.array(data) # For converting to a NumPy array
data
44/37:
ref = f['list']['ALLnames'][0][0]

data = f['list']['ALLnames']
data = np.array(data[ref]) # For converting to a NumPy array
data
44/38:
ref = f['list']['ALLnames'][0][0]

data = f['list']['ALLnames']
data = np.array(f[ref]) # For converting to a NumPy array
data
44/39:
# ref = f['list']['ALLnames'][0][0]

# data = f['list']['ALLnames']
# data = np.array(f[ref]) # For converting to a NumPy array
# data
44/40:
# import scipy.io
# mat = scipy.io.loadmat('')

# import numpy as np
# import h5py

# f = h5py.File('data/CVIT/groundtruth/groundtruth/IIIT-HWS-10K.mat','r')
# f.keys()
44/41:
# wget http://images.cocodataset.org/zips/train2014.zip
# unzip -q train2014.zip
44/42:
with open('data/MSCOCO/cocotext.v2.json', 'r') as f:
    raw = json.dump(f)
    print(raw)
44/43:
with open('data/MSCOCO/cocotext.v2.json', 'r') as f:
    raw = json.load(f)
    print(raw)
44/44: ! git clone https://github.com/bgshih/coco-text
44/45: from coco-text import coco_text
44/46: from ./coco-text import coco_text
44/47: from .coco-text import coco_text
44/48: import coco_text
44/49: from coco-text import coco_text
44/50: from coco-text.coco_text import coco_text
44/51: import coco_text
44/52: import coco_text
44/53: from coco_text import coco_text
44/54:
import sys
sys.path.insert(0, 'data/MSCOCO/coco-text/')

import coco_text
44/55: ct = coco_text.COCO_Text('data/MSCOCO/cocotext.v2.json')
44/56: ct.info()
44/57: ct.info()
44/58:
imgs = ct.getImgIds(imgIds=ct.train, 
                    catIds=[('legibility','legible'),('class','machine printed')])
44/59: imgs
44/60:
imgs_ = ct.getImgIds(imgIds=ct.train, 
                    catIds=[('class','machine printed')])
44/61: imgs
44/62:
imgs_typed = ct.getImgIds(imgIds=ct.train, 
                    catIds=[('class','machine printed')])
44/63: len(imgs_typed)
44/64:
imgs_typed = ct.getImgIds(imgIds=ct.train, 
                    catIds=[('class','machine printed')])
len(imgs_typed)
44/65:
imgs_hw = ct.getImgIds(imgIds=ct.train, 
                    catIds=[('class','handwritten')])
len(imgs_hw)
44/66:
imgs_hw = ct.getImgIds(imgIds=ct.train, 
                    catIds=[('class','hand')])
len(imgs_hw)
44/67:
imgs_hw = ct.getImgIds(imgIds=ct.train, 
                    catIds=[('class','handwritten')])
len(imgs_hw)
44/68:
imgs_hw = ct.getImgIds(imgIds=ct.train, 
                    catIds=[('class','handwritten')])
len(imgs_hw)
44/69:
dataDir='data/MSCOCO/'
dataType='train2014'
44/70: ct.showAnns()
44/71:
%matplotlib inline
import numpy as np
import skimage.io as io
import matplotlib.pyplot as plt
import pylab
pylab.rcParams['figure.figsize'] = (10.0, 8.0)
44/72:
%matplotlib inline
import numpy as np
! pip install skimage
import skimage.io as io
import matplotlib.pyplot as plt
import pylab
pylab.rcParams['figure.figsize'] = (10.0, 8.0)
44/73:
%matplotlib inline
import numpy as np
! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab
pylab.rcParams['figure.figsize'] = (10.0, 8.0)
44/74:
# pick one at random
img = ct.loadImgs(imgs_typed[np.random.randint(0,len(imgs_typed))])[0]
44/75:
I = io.imread('%s/images/%s/%s'%(dataDir,dataType,img['file_name']))
print '/images/%s/%s'%(dataType,img['file_name'])
plt.figure()
plt.imshow(I)
44/76:
I = io.imread('%s/%s/%s'%(dataDir,dataType,img['file_name']))
print '/images/%s/%s'%(dataType,img['file_name'])
plt.figure()
plt.imshow(I)
44/77:
I = io.imread('%s/%s/%s'%(dataDir,dataType,img['file_name']))
print('/images/%s/%s'%(dataType,img['file_name']))
plt.figure()
plt.imshow(I)
44/78:
# load and display text annotations
plt.imshow(I)
annIds = ct.getAnnIds(imgIds=img['id'])
anns = ct.loadAnns(annIds)
ct.showAnns(anns)
44/79: img['id']
44/80: img['bbox']
44/81: img['bbox']
44/82: img
44/83: annIds
44/84: anns
44/85: anns['bbox']
44/86: anns
44/87: anns[:]['bbox']
44/88: anns[:]
44/89: anns[0]['bbox']
44/90:
for ann in anns:
 print(ann['bbox'])
44/91:
for ann in anns:
    print(ann['bbox'])
43/60:
test_preprocess = transforms.Compose([
    transforms.Resize((256, 1000)),
    transforms.RandomCrop((256, 500)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
43/61:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=4)
43/62:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
#     break  # Only display one batch of transformed images
43/63:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
43/64:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
43/65: test_correct/len(test_loader.dataset)
43/66: y_true_cpu, y_pred_cpu
43/67:
test_preprocess = transforms.Compose([
    transforms.Resize((256, 1000)),
#     transforms.RandomCrop((256, 500)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
43/68:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=4)
43/69:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
#     break  # Only display one batch of transformed images
43/70:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
43/71:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
43/72: y_true_cpu, y_pred_cpu
43/73:
test_preprocess = transforms.Compose([
    transforms.Resize((256, 1000)),
    transforms.CentreCrop((256, 500)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
43/74:
test_preprocess = transforms.Compose([
    transforms.Resize((256, 1000)),
    transforms.CenterCrop((256, 500)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
43/75:
test_preprocess = transforms.Compose([
    transforms.Resize((256, 1000)),
    transforms.CenterCrop((256, 500)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
43/76:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=4)
43/77:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
#     break  # Only display one batch of transformed images
43/78:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
43/79:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
43/80: test_correct/len(test_loader.dataset)
43/81: y_true_cpu, y_pred_cpu
44/92:
for ann in anns:
    print(ann['class'], ann['bbox'])
44/93:
for ann in anns:
    print(ann['class'], "-> 
          " ann['bbox'])
44/94:
for ann in anns:
    print(ann['class'], "-> ", ann['bbox'])
44/95:
# Load all images with machine printed text
imgs_typed = ct.getImgIds(imgIds=ct.train, 
                    catIds=[('class','machine printed')])
len(imgs_typed)
44/96:
# Get all images using the image IDs
img = ct.loadImgs(imgs_typed[imgs_typed])
44/97:
# Get all images using the image IDs
img = ct.loadImgs(imgs_typed[[imgs_typed]])
44/98:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):
    
    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["other"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
                        # Save the cropped image
                        cropped_image.save(output_path, "PNG")
44/99: type(ann['bbox'])
44/100:
def crop_images_with_bounding_boxes(image_path, bbox, output_folder):
             
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[2], bbox[3]))
                        
    # append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".jpg")
    # Save the cropped image
    cropped_image.save(output_path, "JPG")
44/101:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in imgs_typed[:10]:
    img = ct.loadImgs(imgs_typed[img_id])
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine typed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder)
44/102: imgs_typed
44/103:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in range(len(imgs_typed[:10])):
    img = ct.loadImgs(imgs_typed[img_id])
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine typed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder)
44/104:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in range(len(imgs_typed[:10])):
    print(img_id)
    img = ct.loadImgs(imgs_typed[img_id])
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine typed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder)
44/105: img
44/106:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in range(len(imgs_typed[:10])):
    print(img_id)
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine typed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder)
44/107:
def crop_images_with_bounding_boxes(image_path, bbox, output_folder):
             
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[2], bbox[3]))
                        
    print(image_path.split("/")[-1])
    # append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_path.split("/")[-1]+"-"+str(idx)+".jpg")
    # Save the cropped image
    cropped_image.save(output_path, "JPG")
44/108:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in range(len(imgs_typed[:10])):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine typed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder)
44/109:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in range(len(imgs_typed[:10])):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder)
44/110:
import os
import json

from PIL import Image
44/111:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in range(len(imgs_typed[:10])):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder)
44/112:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in range(len(imgs_typed[:1])):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder)
44/113:
def crop_images_with_bounding_boxes(image_path, bbox, output_folder):
             
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[0]+bbox[2], bbox[1]+bbox[3]))
                        
    print(image_path.split("/")[-1])
    # append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_path.split("/")[-1]+"-"+str(idx)+".jpg")
    # Save the cropped image
#     cropped_image.save(output_path, "JPG")
44/114:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in range(len(imgs_typed[:1])):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder)
44/115: ann
44/116:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in range(len(imgs_typed[:1])):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder, ann['id'])
44/117:
def crop_images_with_bounding_boxes(image_path, bbox, output_folder, idx):
             
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[0]+bbox[2], bbox[1]+bbox[3]))
                        
    print(image_path.split("/")[-1])
    # append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_path.split("/")[-1]+"-"+str(idx)+".jpg")
    # Save the cropped image
#     cropped_image.save(output_path, "JPG")
44/118:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in range(len(imgs_typed[:1])):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder, ann['id'])
44/119:
def crop_images_with_bounding_boxes(image_path, bbox, output_folder, idx):
             
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[0]+bbox[2], bbox[1]+bbox[3]))
                        
    print(image_path.split("/")[-1][-4])
    # append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_path.split("/")[-1]+"-"+str(idx)+".jpg")
    # Save the cropped image
#     cropped_image.save(output_path, "JPG")
44/120:
def crop_images_with_bounding_boxes(image_path, bbox, output_folder, idx):
             
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[0]+bbox[2], bbox[1]+bbox[3]))
                        
    print(image_path.split("/")[-1][-4])
    # append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_path.split("/")[-1]+"-"+str(idx)+".jpg")
    # Save the cropped image
#     cropped_image.save(output_path, "JPG")
44/121:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in range(len(imgs_typed[:1])):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder, ann['id'])
44/122:
def crop_images_with_bounding_boxes(image_path, bbox, output_folder, idx):
             
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[0]+bbox[2], bbox[1]+bbox[3]))
                        
    print(image_path.split("/")[-1][:-4])
    # append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_path.split("/")[-1]+"-"+str(idx)+".jpg")
    # Save the cropped image
#     cropped_image.save(output_path, "JPG")
44/123:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in range(len(imgs_typed[:1])):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder, ann['id'])
44/124:
def crop_images_with_bounding_boxes(image_path, bbox, output_folder, idx):
             
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[0]+bbox[2], bbox[1]+bbox[3]))
                        
    print(image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    # append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    # Save the cropped image
#     cropped_image.save(output_path, "JPG")
44/125:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in range(len(imgs_typed[:1])):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder, ann['id'])
44/126:
def crop_images_with_bounding_boxes(image_path, bbox, output_folder, idx):
             
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[0]+bbox[2], bbox[1]+bbox[3]))
                        
    # print(image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    # append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    # Save the cropped image
    cropped_image.save(output_path, "JPG")
44/127:
def crop_images_with_bounding_boxes(image_path, bbox, output_folder, idx):
             
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[0]+bbox[2], bbox[1]+bbox[3]))
                        
    # print(image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    # append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    # Save the cropped image
    cropped_image.save(output_path, "JPG")
44/128:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in range(len(imgs_typed[:1])):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder, ann['id'])
44/129:
def crop_images_with_bounding_boxes(image_path, bbox, output_folder, idx):
             
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[0]+bbox[2], bbox[1]+bbox[3]))
                        
    # print(image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    # append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    # Save the cropped image
    cropped_image.save(output_path, "JPEG")
44/130:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in range(len(imgs_typed[:1])):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder, ann['id'])
44/131:
# Get all images using the image IDs
output_folder = "data/all_data/typed/"

for img_id in range(len(imgs_typed[1:])):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder, ann['id'])
43/82:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
43/83:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize((256, 1000)),
#     transforms.RandomCrop((256, 500)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
44/132:
# Get all images IDs with machine printed text
imgs_hw = ct.getImgIds(imgIds=ct.train, 
                    catIds=[('class','handwritten')])
len(imgs_hw)
44/133:
def COCO_crop_images_with_bounding_boxes(image_path, bbox, output_folder, idx):
             
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[0]+bbox[2], bbox[1]+bbox[3]))
                        
    # print(image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    # append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    # Save the cropped image
    cropped_image.save(output_path, "JPEG")
44/134:
# Get all images using the image IDs
output_folder = "data/all_data/handwritten/"

# Crop machine printed text from images using bounding box coordinates
# and save the cropped images in the "typed" folder
for img_id in range(len(imgs_typed)):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "handwritten"):
            COCO_crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder, ann['id'])
43/84:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
43/85:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
43/86:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 5

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("Training complete!")
43/87:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize((256, 500)),
    transforms.RandomCrop((256, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
43/88: ! ls data/all_data/handwritten | wc -l
43/89: ! ls data/all_data/typed/ | wc -l
44/135:
# Get all images IDs with machine printed text
imgs_typed = ct.getImgIds(imgIds=ct.train, 
                    catIds=[('class','handwritten')])
len(imgs_typed)
44/136:
def COCO_crop_images_with_bounding_boxes(image_path, bbox, output_folder, idx):
             
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[0]+bbox[2], bbox[1]+bbox[3]))
                        
    # print(image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    # append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    # Save the cropped image
    cropped_image.save(output_path, "JPEG")
44/137:
# Get all images using the image IDs
output_folder = "data/all_data/handwritten/"

# Crop machine printed text from images using bounding box coordinates
# and save the cropped images in the "typed" folder
for img_id in range(len(imgs_typed)):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "handwritten"):
            COCO_crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder, ann['id'])
43/90:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
45/1:
import os
import shutil
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
45/2:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize((256, 500)),
    transforms.RandomCrop((256, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
45/3: ! ls data/all_data/handwritten | wc -l
45/4: ! ls data/all_data/typed/ | wc -l
45/5:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
45/6:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
45/7:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
45/8:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 5

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy and f1-score
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("Training complete!")
45/9:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
45/10:
test_preprocess = transforms.Compose([
    transforms.Resize((256, 1000)),
    transforms.CenterCrop((256, 500)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
45/11:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=4)
45/12:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
45/13:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
45/14:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
45/15: test_correct/len(test_loader.dataset)
45/16: y_true_cpu, y_pred_cpu
44/138: ! ls data/CVIT/
44/139: ! ls data/CVIT/Images_90K_Normalized/ | wc -l
44/140:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

# Iterate over files in the images folder
for img_file in os.listdir(folder):
    img_path = os.path.join(folder, img_file)
    cvit_img_paths.append(img_path)

cvit_img_paths[:10]
44/141: len(cvit_img_paths)
44/142:
import os
import ipywidgets as widgets
from IPython.display import display, Image

def view_images(folder_path):
    image_files = sorted([f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.png', '.jpeg'))])
    num_images = len(image_files)
    
    if num_images == 0:
        print("No images found in the folder.")
        return
    
    image_index = 0
    
    def display_image(image_index):
        image_path = os.path.join(folder_path, image_files[image_index])
        display(Image(filename=image_path))
    
    # Initial display
    display_image(image_index)
    
    # Image navigation buttons
    prev_button = widgets.Button(description='Previous')
    next_button = widgets.Button(description='Next')
    
    def on_prev_button_clicked(b):
        nonlocal image_index
        image_index = (image_index - 1) % num_images
        display_image(image_index)
    
    def on_next_button_clicked(b):
        nonlocal image_index
        image_index = (image_index + 1) % num_images
        display_image(image_index)
    
    prev_button.on_click(on_prev_button_clicked)
    next_button.on_click(on_next_button_clicked)
    
    # Display navigation buttons
    display(widgets.HBox([prev_button, next_button]))
44/143:
# Provide the folder path containing the images
folder_path = 'data/CVIT/Images_90K_Normalized/'

# Call the function to view the images interactively
view_images(folder_path)
44/144:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

# Iterate over files in the images folder
for img_file in os.walk(folder):
    img_path = os.path.join(folder, img_file)
    cvit_img_paths.append(img_path)

cvit_img_pathso[]:10]
44/145:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

# Iterate over files in the images folder
for img_file in os.walk(folder):
    img_path = os.path.join(folder, img_file)
    cvit_img_paths.append(img_path)

cvit_img_pathso[:10]
44/146:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 100000

# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    for file in files:
        img_path = os.path.join(root, file)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            return  # Break out of the loop if the desired limit is reached
44/147:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 100000

# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    for file in files:
        img_path = os.path.join(root, file)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/148: len(cvit_img_paths)
44/149: ! unzip
44/150:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 10

# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    for file in files:
        img_path = os.path.join(root, file)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/151: len(cvit_img_paths)
44/152:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 10

# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    for file in files:
        img_path = os.path.join(root, file)
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/153:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 10

# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    print(root, dirs, files)
    for file in files:
        img_path = os.path.join(root, file)
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/154:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 10

# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    print(root, dirs, files)
    for file in files:
        img_path = os.path.join(root, file)
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/155:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 10
44/156:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    print(root, dirs, files)
    for file in files:
        img_path = os.path.join(root, file)
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/157:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    print(root, dirs, files)
    for file in files:
        img_path = os.path.join(root, file)
#         print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/158:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 10
44/159:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    print(root, dirs, files)
    for file in files:
        img_path = os.path.join(root, file)
#         print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/160:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 10
44/161:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
#     print(root, dirs, files)
    for file in files:
        img_path = os.path.join(root, file)
#         print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/162: len(cvit_img_paths)
44/163:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
#     print(root, dirs, files)
    for file in files:
        img_path = os.path.join(root, file)
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/164:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 10
num_files = 0
44/165:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 10
num_files = 0
44/166:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
#     print(root, dirs, files)
    for file in files:
        img_path = os.path.join(root, file)
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/167: len(cvit_img_paths)
44/168:
# Iterate over files in the images folder
for root, _, files in os.walk(folder):
#     print(root, dirs, files)
    for file in files:
        img_path = os.path.join(root, file)
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/169: len(cvit_img_paths)
44/170:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
#     print(root, dirs, files)
    for file in files:
        img_path = os.path.join(root, dirs, file)
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/171: len(cvit_img_paths)
44/172:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 10
num_files = 0
44/173:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
#     print(root, dirs, files)
    for file in files:
        img_path = os.path.join(root, dirs, file)
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/174: len(cvit_img_paths)
44/175:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    print(files)
    for file in files:
        img_path = os.path.join(root, dirs, file)
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/176:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 10
num_files = 0
44/177:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    print(files)
    for file in files:
        img_path = os.path.join(root, dirs, file)
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/178:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    print(dirs)
    for file in files:
        img_path = os.path.join(root, dirs, file)
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/179:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    print(dirs)
    for file in files:
        img_path = os.path.join(root, dirs, file)
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/180:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 10
num_files = 0
44/181:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    print(dirs)
    for dir_ in dirs:
        img_path = os.walk(os.path.join(root, dirs))
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/182:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    print(dirs)
    for dir_ in dirs:
        img_path = os.walk(os.path.join(root, dirs))
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/183: len(cvit_img_paths)
44/184:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    print(dirs)
    for dir_ in dirs:
        img_path = os.walk(os.path.join(root, dir_))
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/185:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
    print(dirs)
    for dir_ in dirs:
        img_path = os.walk(os.path.join(root, dir_))
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/186: len(cvit_img_paths)
44/187: cvit_img_paths
44/188:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
#     print(root, dirs, files)
    for file in files:
        img_path = os.path.join(root, file)
#         print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/189: len(cvit_img_paths)
44/190:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 10
num_files = 0
44/191:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
#     print(root, dirs, files)
    for file in files:
        img_path = os.path.join(root, file)
#         print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/192: len(cvit_img_paths)
44/193:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
#     print(root, dirs, files)
    for file in files:
        print(file)
        img_path = os.path.join(root, file)
#         print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
44/194: len(cvit_img_paths)
46/1:
import os
import json

from PIL import Image
46/2:
folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 10
num_files = 0
46/3:
# Iterate over files in the images folder
for root, dirs, files in os.walk(folder):
#     print(root, dirs, files)
    for file in files:
        print(file)
        img_path = os.path.join(root, file)
#         print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
46/4: len(cvit_img_paths)
46/5:
cvit_sub_folders = [f for f in os.listdir(folder_path)]
cvit_sub_folders
46/6:
cvit_sub_folders = [f for f in os.listdir(folder)]
cvit_sub_folders
46/7:
cvit_sub_folders = [f for f in os.listdir(folder)]
len(cvit_sub_folders)
46/8:
cvit_folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 10
num_files = 0
46/9:
cvit_sub_folders = [f for f in os.listdir(cvit_folder)]
len(cvit_sub_folders)
46/10:
# Iterate over files in the images folder
for sub_folder in cvit_sub_folders[:10]:
    
    folder_path = os.path.join(cvit_folder, sub_folder)
    files = os.listdir(folder_path)
    
    for file in files:
        img_path = os.path.join(root, file)
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
46/11:
# Iterate over files in the images folder
for sub_folder in cvit_sub_folders[:10]:
    
    folder_path = os.path.join(cvit_folder, sub_folder)
    files = os.listdir(folder_path)
    
    for file in files:
        img_path = os.path.join(folder_path, file)
        print(img_path)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
46/12: len(cvit_img_paths)
46/13:
cvit_folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 1000
num_files = 0
46/14:
cvit_sub_folders = [f for f in os.listdir(cvit_folder)]
len(cvit_sub_folders)
46/15:
# Iterate over files in the images folder
for sub_folder in cvit_sub_folders[:10]:
    
    folder_path = os.path.join(cvit_folder, sub_folder)
    files = os.listdir(folder_path)
    
    for file in files:
        img_path = os.path.join(folder_path, file)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
46/16: len(cvit_img_paths)
46/17:
# Iterate over files in the images folder
for sub_folder in cvit_sub_folders:
    
    folder_path = os.path.join(cvit_folder, sub_folder)
    files = os.listdir(folder_path)
    
    for file in files:
        img_path = os.path.join(folder_path, file)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
46/18: len(cvit_img_paths)
46/19:
cvit_folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []

max_num_files = 1000
num_files = 0
46/20:
cvit_sub_folders = [f for f in os.listdir(cvit_folder)]
len(cvit_sub_folders)
46/21:
# Iterate over files in the images folder
for sub_folder in cvit_sub_folders:
    
    folder_path = os.path.join(cvit_folder, sub_folder)
    files = os.listdir(folder_path)
    
    for file in files:
        img_path = os.path.join(folder_path, file)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
46/22: len(cvit_img_paths)
46/23: cvit_img_paths
46/24:
# Iterate over files in the images folder
for sub_folder in cvit_sub_folders:
    
    folder_path = os.path.join(cvit_folder, sub_folder)
    print(folder_path)
    files = os.listdir(folder_path)
    
    for file in files:
        img_path = os.path.join(folder_path, file)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
46/25:
max_num_files = 1000
num_files = 0

# Iterate over files in the images folder
for sub_folder in cvit_sub_folders:
    
    folder_path = os.path.join(cvit_folder, sub_folder)
    print(folder_path)
    files = os.listdir(folder_path)
    
    print(files)
    
    for file in files:
        img_path = os.path.join(folder_path, file)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
46/26:
max_num_files = 1000
num_files = 0

# Iterate over files in the images folder
for sub_folder in cvit_sub_folders:
    print(sub_folder)
    
    folder_path = os.path.join(cvit_folder, sub_folder)
    print(folder_path)
    files = os.listdir(folder_path)
    
    print(files)
    
    for file in files:
        img_path = os.path.join(folder_path, file)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break  # Break out of the loop if the desired limit is reached
    break
46/27: cvit_sub_folders[:10]
46/28:
max_num_files = 1000
num_files = 0
break_flag = 0

# Iterate over files in the images folder
for sub_folder in cvit_sub_folders:    
    folder_path = os.path.join(cvit_folder, sub_folder)
    files = os.listdir(folder_path)
        
    for file in files:
        img_path = os.path.join(folder_path, file)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break_flag = 1
            break  # Break out of the loop if the desired limit is reached
    
    if break_flag == 1:
        break  # Break out of the loop if the desired limit is reached
46/29: len(cvit_img_paths)
46/30:
cvit_folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []
46/31:
cvit_sub_folders = [f for f in os.listdir(cvit_folder)]
len(cvit_sub_folders)
46/32: cvit_sub_folders[:10]
46/33:
max_num_files = 1000
num_files = 0
break_flag = 0

# Iterate over files in the images folder
for sub_folder in cvit_sub_folders:    
    folder_path = os.path.join(cvit_folder, sub_folder)
    files = os.listdir(folder_path)
        
    for file in files:
        img_path = os.path.join(folder_path, file)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break_flag = 1
            break  # Break out of the loop if the desired limit is reached
    
    if break_flag == 1:
        break  # Break out of the loop if the desired limit is reached
46/34: len(cvit_img_paths)
46/35: view_images(cvit_img_paths)
46/36:
import ipywidgets as widgets
from IPython.display import display, Image

def view_images(image_paths_list):
    
    num_images = len(image_paths_list)
    image_index = 0
    
    def display_image(image_index):
        image_path = image_paths_list[image_index]
        display(Image(filename=image_path))
    
    # Initial display
    display_image(image_index)
    
    # Image navigation buttons
    prev_button = widgets.Button(description='Previous')
    next_button = widgets.Button(description='Next')
    
    def on_prev_button_clicked(b):
        nonlocal image_index
        image_index = (image_index - 1) % num_images
        display_image(image_index)
    
    def on_next_button_clicked(b):
        nonlocal image_index
        image_index = (image_index + 1) % num_images
        display_image(image_index)
    
    prev_button.on_click(on_prev_button_clicked)
    next_button.on_click(on_next_button_clicked)
    
    # Display navigation buttons
    display(widgets.HBox([prev_button, next_button]))
46/37: view_images(cvit_img_paths)
46/38:
import ipywidgets as widgets
from IPython.display import display, Image

def view_images(image_paths_list):
    
    num_images = len(image_paths_list)
    image_index = 0
    
    def display_image(image_index):
        image_path = image_paths_list[image_index]
        display(Image(filename=image_path))
    
    # Initial display
    display_image(image_index)
    
    # Image navigation buttons
    prev_button = widgets.Button(description='Previous')
    next_button = widgets.Button(description='Next')
    
    def on_prev_button_clicked(b):
        nonlocal image_index
        image_index = (image_index - 1) % num_images
        clear_output(wait=True)
        display_image(image_index)
    
    def on_next_button_clicked(b):
        nonlocal image_index
        image_index = (image_index + 1) % num_images
        clear_output(wait=True)
        display_image(image_index)
    
    prev_button.on_click(on_prev_button_clicked)
    next_button.on_click(on_next_button_clicked)
    
    # Display navigation buttons
    display(widgets.HBox([prev_button, next_button]))
46/39: view_images(cvit_img_paths)
46/40:
import ipywidgets as widgets
from IPython.display import display, Image, clear_output

def view_images(image_paths_list):
    
    num_images = len(image_paths_list)
    image_index = 0
    
    def display_image(image_index):
        image_path = image_paths_list[image_index]
        display(Image(filename=image_path))
    
    # Initial display
    display_image(image_index)
    
    # Image navigation buttons
    prev_button = widgets.Button(description='Previous')
    next_button = widgets.Button(description='Next')
    
    def on_prev_button_clicked(b):
        nonlocal image_index
        image_index = (image_index - 1) % num_images
        clear_output(wait=True)
        display_image(image_index)
    
    def on_next_button_clicked(b):
        nonlocal image_index
        image_index = (image_index + 1) % num_images
        clear_output(wait=True)
        display_image(image_index)
    
    prev_button.on_click(on_prev_button_clicked)
    next_button.on_click(on_next_button_clicked)
    
    # Display navigation buttons
    display(widgets.HBox([prev_button, next_button]))
46/41: view_images(cvit_img_paths)
46/42:
import ipywidgets as widgets
from IPython.display import display, Image, clear_output

def view_images(image_paths_list):
    
    num_images = len(image_paths_list)
    image_index = 0
    
    def display_image(image_index):
        image_path = image_paths_list[image_index]
        display(Image(filename=image_path))
    
    # Initial display
    display_image(image_index)
    
    # Image navigation buttons
    prev_button = widgets.Button(description='Previous')
    next_button = widgets.Button(description='Next')
    
    def on_prev_button_clicked(b):
        nonlocal image_index
        image_index = (image_index - 1) % num_images
        clear_output(wait=True)
        display_image(image_index)
    
    def on_next_button_clicked(b):
        nonlocal image_index
        image_index = (image_index + 1) % num_images
        clear_output(wait=True)
        display_image(image_index)
    
    prev_button.on_click(on_prev_button_clicked)
    next_button.on_click(on_next_button_clicked)
    
    # Display navigation buttons
    display(widgets.HBox([prev_button, next_button]))
46/43: view_images(cvit_img_paths)
46/44:
import ipywidgets as widgets
from IPython.display import display, Image, clear_output

def view_images(image_paths_list):
    
    num_images = len(image_paths_list)
    image_index = 0
    
    def display_image(image_index):
        image_path = image_paths_list[image_index]
        with open(image_path, 'rb') as file:
            image_widget.value = file.read()
    
    image_widget = widgets.Image(format='png')
    
    # Image navigation buttons
    prev_button = widgets.Button(description='Previous')
    next_button = widgets.Button(description='Next')
    
    def on_prev_button_clicked(b):
        nonlocal image_index
        image_index = (image_index - 1) % num_images
        clear_output(wait=True)
        display_image(image_index)
    
    def on_next_button_clicked(b):
        nonlocal image_index
        image_index = (image_index + 1) % num_images
        clear_output(wait=True)
        display_image(image_index)
    
    # Initial display
    display_image(image_index)
    
    prev_button.on_click(on_prev_button_clicked)
    next_button.on_click(on_next_button_clicked)
    
    # Display navigation buttons
    display(widgets.HBox([prev_button, next_button]))
46/45: view_images(cvit_img_paths)
46/46:
import ipywidgets as widgets
from IPython.display import display, Image, clear_output

def view_images(image_paths_list):
    
    num_images = len(image_paths_list)
    image_index = 0
    
    def display_image(image_index):
        image_path = image_paths_list[image_index]
        with open(image_path, 'rb') as file:
            image_widget.value = file.read()
    
    image_widget = widgets.Image(format='png')
    
    # Image navigation buttons
    prev_button = widgets.Button(description='Previous')
    next_button = widgets.Button(description='Next')
    
    def on_prev_button_clicked(b):
        nonlocal image_index
        image_index = (image_index - 1) % num_images
        display_image(image_index)
    
    def on_next_button_clicked(b):
        nonlocal image_index
        image_index = (image_index + 1) % num_images
        display_image(image_index)
    
    # Initial display
    display_image(image_index)
    
    prev_button.on_click(on_prev_button_clicked)
    next_button.on_click(on_next_button_clicked)
    
    # Display navigation buttons
    display(widgets.HBox([prev_button, next_button]))
46/47: view_images(cvit_img_paths)
46/48:
import ipywidgets as widgets
from IPython.display import display, Image, clear_output

def view_images(image_paths_list):
    
    num_images = len(image_paths_list)
    image_index = 0
    
    def display_image(image_index):
        image_path = image_paths_list[image_index]
        with open(image_path, 'rb') as file:
            image_widget.value = file.read()
    
    image_widget = widgets.Image(format='png')
    
    # Image navigation buttons
    prev_button = widgets.Button(description='Previous')
    next_button = widgets.Button(description='Next')
    
    def on_prev_button_clicked(b):
        nonlocal image_index
        image_index = (image_index - 1) % num_images
        display_image(image_index)
    
    def on_next_button_clicked(b):
        nonlocal image_index
        image_index = (image_index + 1) % num_images
        display_image(image_index)
    
    display(image_widget)
    
    # Initial display
    display_image(image_index)
    
    prev_button.on_click(on_prev_button_clicked)
    next_button.on_click(on_next_button_clicked)
    
    # Display navigation buttons
    display(widgets.HBox([prev_button, next_button]))
46/49:
import ipywidgets as widgets
from IPython.display import display, Image, clear_output

def view_images(image_paths_list):
    
    num_images = len(image_paths_list)
    image_index = 0
    
    def display_image(image_index):
        image_path = image_paths_list[image_index]
        with open(image_path, 'rb') as file:
            image_widget.value = file.read()
    
    image_widget = widgets.Image(format='png')
    
    # Image navigation buttons
    prev_button = widgets.Button(description='Previous')
    next_button = widgets.Button(description='Next')
    
    def on_prev_button_clicked(b):
        nonlocal image_index
        image_index = (image_index - 1) % num_images
        display_image(image_index)
    
    def on_next_button_clicked(b):
        nonlocal image_index
        image_index = (image_index + 1) % num_images
        display_image(image_index)
    
    display(image_widget)
    
    # Initial display
    display_image(image_index)
    
    prev_button.on_click(on_prev_button_clicked)
    next_button.on_click(on_next_button_clicked)
    
    # Display navigation buttons
    display(widgets.HBox([prev_button, next_button]))
46/50: view_images(cvit_img_paths)
46/51: from torchvision.transforms.functional import affine
46/52: img = Image.open()
46/53:
import os
import json

from PIL import Image
46/54: img = Image.open(cvit_img_paths[0])
46/55:
img = Image.open(cvit_img_paths[0])
img
46/56:
transformed_img = affine(img, shear=0.5)
transformed_img
46/57:
transformed_img = affine(img, angle=0, translate=[0,0], scale=1.0, shear=0.5)
transformed_img
46/58:
transformed_img = affine(img, angle=0, translate=[0,0], scale=1.0, shear=1.0)
transformed_img
46/59:
transformed_img = affine(img, angle=0, translate=[0,0], scale=1.0, shear=10.0)
transformed_img
46/60:
transformed_img = affine(img, angle=0, translate=[0,0], scale=1.0, shear=100.0)
transformed_img
46/61:
transformed_img = affine(img, angle=0, translate=[0,0], scale=1.0, shear=30.0)
transformed_img
46/62:
transformed_img = affine(img, angle=0, translate=[0,0], scale=1.0, shear=(30, 10))
transformed_img
46/63:
transformed_img = affine(img, angle=0, translate=[0,0], scale=1.0, shear=(30, 10), fill=0)
transformed_img
46/64:
transformed_img = affine(img, angle=0, translate=[0,0], scale=1.0, shear=(30, 10), fill=(255,255,255))
transformed_img
46/65:
transformed_img = affine(img, angle=0, translate=[0,0], scale=1.0, shear=(30, 10), fill=(255))
transformed_img
46/66:
transformed_img = affine(img, angle=0, translate=[0,0], scale=1.0, shear=(30, 10), fill=(255))
transformed_img.show()
46/67:
transformed_img = affine(img, angle=0, translate=[0,0], scale=1.0, shear=(30, 10), fill=(255))
transformed_img
46/68:
transformed_img = affine(img, angle=0, translate=[0,0], scale=1.0, shear=(10, 30), fill=(255))
transformed_img
46/69:
transformed_img = affine(img, angle=0, translate=(0,0]), scale=1.0, shear=(10, 30), fill=(255))
transformed_img
46/70:
transformed_img = affine(img, angle=0, translate=(0,0), scale=1.0, shear=(10, 30), fill=(255))
transformed_img
47/1:
import os
import shutil
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
47/2:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize((256, 500)),
    transforms.RandomCrop((256, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
47/3:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
47/4:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
47/5:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
47/6:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
47/7:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
47/8:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
47/9:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
47/10:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 5

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
46/71:
transformed_img = affine(img, angle=0, translate=(0,0), scale=1.0, shear=(25, 15), fill=(255))
transformed_img
46/72:
transformed_img = affine(img, angle=0, translate=(0,0), scale=1.0, shear=(25, 0), fill=(255))
transformed_img
46/73:
transformed_img = affine(img, angle=0, translate=(0,0), scale=1.0, shear=(35, 0), fill=(255))
transformed_img
46/74:
transformed_img = affine(img, angle=0, translate=(0,0), scale=1.0, shear=(-35, 0), fill=(255))
transformed_img
46/75:
transformed_img = affine(img, angle=0, translate=(0,0), scale=1.0, shear=(-40, 0), fill=(255))
transformed_img
46/76:
transformed_img = affine(img, angle=0, translate=(0,0), scale=1.0, shear=(-40, -10), fill=(255))
transformed_img

# RamdomAffine with shear ()
46/77:
transformed_img = affine(img, angle=0, translate=(0,0), scale=1.0, shear=(-40, 10), fill=(255))
transformed_img

# RamdomAffine with shear ()
46/78: img.shape
46/79: img.shape()
46/80: img.size
47/11:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
47/12:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((256, 500)),
    transforms.CenterCrop((256, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
47/13:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=4)
47/14:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
47/15:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
47/16:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
47/17:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((256, 500)),
    transforms.CenterCrop((256, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
47/18:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=4)
47/19:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
47/20:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
47/21:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
47/22:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
47/23:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
47/24: test_correct/len(test_loader.dataset)
47/25: y_true_cpu, y_pred_cpu
47/26:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
        train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
        train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
        y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
        y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
        train_f1 = f1_score(train_true_cpu, train_pred_cpu)
        val_f1 = f1_score(y_true_cpu, y_pred_cpu)

        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
46/81:
# Define the custom conditional transform
class ConditionalTransform:
    def __init__(self, threshold, transform1, transform2):
        self.threshold = threshold
        self.transform1 = transform1
        self.transform2 = transform2
    
    def __call__(self, image):
        width, height = image.size
        
        if width > self.threshold and height > self.threshold:
            return self.transform1(image)
        else:
            return self.transform2(image)
46/82:
# Define the transforms pipeline
transforms_pipeline = transforms.Compose([
    ConditionalTransform(500, , transforms.Resize((256, 600)), transforms.Identity()),
    transforms.ToTensor()
])
46/83:
# Define the transforms pipeline
transforms_pipeline = transforms.Compose([
    ConditionalTransform(500,  transforms.Resize((256, 600)), transforms.Identity()),
    transforms.ToTensor()
])
46/84:
import os
import json

from PIL import Image
from torchvision.transforms import transforms
46/85:
# Define the transforms pipeline
transforms_pipeline = transforms.Compose([
    ConditionalTransform(500,  transforms.Resize((256, 600)), transforms.Identity()),
    transforms.ToTensor()
])
46/86:
# Define the custom conditional transform
class ConditionalTransform:
    def __init__(self, threshold, transform1, transform2):
        self.threshold = threshold
        self.transform1 = transform1
        self.transform2 = transform2
    
    def __call__(self, image):
        width, height = image.size
        
        if width > self.threshold and height > self.threshold:
            return self.transform1(image)
        else:
            return image
46/87:
# Define the transforms pipeline
transforms_pipeline = transforms.Compose([
    ConditionalTransform(500,  transforms.Resize((256, 600))),
    transforms.ToTensor()
])
46/88:
# Define the custom conditional transform
class ConditionalTransform:
    def __init__(self, threshold, transform1):
        self.threshold = threshold
        self.transform1 = transform1
        self.transform2 = transform2
    
    def __call__(self, image):
        width, height = image.size
        
        if width > self.threshold and height > self.threshold:
            return self.transform1(image)
        else:
            return image
46/89:
# Define the transforms pipeline
transforms_pipeline = transforms.Compose([
    ConditionalTransform(500,  transforms.Resize((256, 600))),
    transforms.ToTensor()
])
46/90:
# Define the custom conditional transform
class ConditionalTransform:
    def __init__(self, threshold, transform1):
        self.threshold = threshold
        self.transform1 = transform1
    
    def __call__(self, image):
        width, height = image.size
        
        if width > self.threshold and height > self.threshold:
            return self.transform1(image)
        else:
            return image
46/91:
# Define the transforms pipeline
transforms_pipeline = transforms.Compose([
    ConditionalTransform(500,  transforms.Resize((256, 600))),
    transforms.ToTensor()
])
46/92:
# Define the custom conditional transform
class ConditionalTransform:
    def __init__(self, h_threshold, w_thresh, transform1):
        self.threshold = threshold
        self.transform1 = transform1
    
    def __call__(self, image):
        width, height = image.size
        
        if width > self.threshold and height > self.threshold:
            return self.transform1(image)
        else:
            return image
46/93:
t = magic.from_file(cvit_img_paths[0])
t
46/94:
import magic
t = magic.from_file(cvit_img_paths[0])
t
46/95: ! pip install python-magic
46/96:
import magic
t = magic.from_file(cvit_img_paths[0])
t
46/97:
import magic
t = magic.from_file(cvit_img_paths[0])
t.split(",")[2]
46/98:
import magic
t = magic.from_file(cvit_img_paths[0])
t.split(",")[1]
46/99: size_str.split()
46/100:
import magic
t = magic.from_file(cvit_img_paths[0])
size_str = t.split(",")[1]
46/101: size_str.split()
46/102:
import magic
t = magic.from_file(cvit_img_paths[0])
t
46/103: re.search('(\d+) x (\d+)', t).groups()
46/104:
import os
import json
import re

from PIL import Image
from torchvision.transforms import transforms

import magic
46/105:
import os
import json
import re

from PIL import Image
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine

import magic
46/106:
import os
import json
import re
import sys

from PIL import Image
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine

import magic
46/107:
import os
import json
import re
import sys

from PIL import Image
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine

import magic

%matplotlib inline
import numpy as np
! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab
46/108: re.search('(\d+) x (\d+)', t).groups()
46/109:
! wget -P data/IAM \
    --load-cookies data/cookies.txt \ 
    https://fki.tic.heia-fr.ch/DBs/iamOnDB/data/lineImages-all.tar.gz
46/110:
! wget -P data/IAM/ \
    --load-cookies data/cookies.txt \ 
    https://fki.tic.heia-fr.ch/DBs/iamOnDB/data/lineImages-all.tar.gz
46/111:
! wget -P data/IAM/ \
    --load-cookies data/cookies.txt \ 
     https://fki.tic.heia-fr.ch/DBs/iamOnDB/data/lineImages-all.tar.gz
46/112:
! wget -P data/IAM/ \
    --load-cookies data/cookies.txt \ 
https://fki.tic.heia-fr.ch/DBs/iamOnDB/data/lineImages-all.tar.gz
46/113: ! wget -P data/IAM/ --load-cookies data/cookies.txt https://fki.tic.heia-fr.ch/DBs/iamOnDB/data/lineImages-all.tar.gz
46/114: !  tar -xvzf data/IAM/lineImages-all.tar.gz -C data/online/
46/115: !  tar -xvzf data/IAM/lineImages-all.tar.gz -C data/IAM/online/
46/116: !  tar -xvzf data/IAM/lineImages-all.tar.gz -C data/IAM/online/
47/27:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
47/28:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(val_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
46/117:
def get_image_paths(root_dir, data_level):
    image_paths = []
    for root, _, files in os.walk(os.path.join(root_dir, data_level)):
        for file in files:
            if file.endswith('.png'):
                image_paths.append(os.path.join(root, file))
    return image_paths


# Get IAM image paths
dataset_levels = ["lines", "sentences"] # "words"
hw_image_paths = []

for level in dataset_levels:
    hw_image_paths += get_image_paths("data/", level)

print(len(hw_image_paths))
46/118:
def get_image_paths(root_dir, data_level):
    image_paths = []
    for root, _, files in os.walk(os.path.join(root_dir, data_level)):
        for file in files:
            if file.endswith('.png'):
                image_paths.append(os.path.join(root, file))
    return image_paths
46/119:
def copy_files(destination_folder, file_path_list):
    for source_path in file_path_list:
        destination_path = os.path.join(destination_folder, source_path.split("/")[-1])
        shutil.copy2(source_path, destination_path)
46/120:
root_dir = "data/IAM/"
online_img_paths = []

dataset_levels = ["online"]

online_img_paths = get_image_paths(root_dir, dataset_levels)
len(online_img_paths)
46/121:
root_dir = "data/IAM/"
online_img_paths = []

dataset_levels = "online"

online_img_paths = get_image_paths(root_dir, dataset_levels)
len(online_img_paths)
46/122:
def get_image_paths(root_dir, data_level):
    image_paths = []
    for root, _, files in os.walk(os.path.join(root_dir, data_level)):
        for file in files:
            if file.endswith('.tiff'):
                image_paths.append(os.path.join(root, file))
    return image_paths
46/123:
# # Get IAM image paths
# dataset_levels = ["lines", "sentences"] # "words"
# hw_image_paths = []

# for level in dataset_levels:
#     hw_image_paths += get_image_paths("data/", level)

# print(len(hw_image_paths))
46/124:
def copy_files(destination_folder, file_path_list):
    for source_path in file_path_list:
        destination_path = os.path.join(destination_folder, source_path.split("/")[-1])
        shutil.copy2(source_path, destination_path)
46/125:
root_dir = "data/IAM/"
online_img_paths = []

dataset_levels = "online"

online_img_paths = get_image_paths(root_dir, dataset_levels)
len(online_img_paths)
47/29:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(val_loader[0]), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
47/30:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    dummy = next(val_loader)
#     enumerate(train_loader)
    with tqdm(dummy, total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
47/31:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    dummy = next(iter(val_loader))
#     enumerate(train_loader)
    with tqdm(dummy, total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
47/32:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    dummy = next(iter(val_loader))
#     enumerate(train_loader)
    with tqdm(dummy, total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
#         for batch_idx, (images, labels) in pbar:
        for (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
47/33:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(val_loader), total=math.ceil(val_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
46/126:
root_dir = "data/IAM/online"
online_img_paths = []

dataset_levels = "lineImages"

online_img_paths = get_image_paths(root_dir, dataset_levels)
len(online_img_paths)
46/127:
def get_image_paths(root_dir, data_level):
    image_paths = []
    for root, _, files in os.walk(os.path.join(root_dir, data_level)):
        for file in files:
            if file.endswith('.tif'):
                image_paths.append(os.path.join(root, file))
    return image_paths
46/128:
root_dir = "data/IAM/online"
online_img_paths = []

dataset_levels = "lineImages"

online_img_paths = get_image_paths(root_dir, dataset_levels)
len(online_img_paths)
46/129:
import os
import json
import re
import sys

from PIL import Image
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine

import magic

%matplotlib inline
import numpy as np
! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab
46/130:
def get_image_paths(root_dir, data_level):
    image_paths = []
    for root, _, files in os.walk(os.path.join(root_dir, data_level)):
        for file in files:
            if file.endswith('.tif'):
                image_paths.append(os.path.join(root, file))
    return image_paths
46/131:
# # Get IAM image paths
# dataset_levels = ["lines", "sentences"] # "words"
# hw_image_paths = []

# for level in dataset_levels:
#     hw_image_paths += get_image_paths("data/", level)

# print(len(hw_image_paths))
46/132:
def copy_files(destination_folder, file_path_list):
    for source_path in file_path_list:
        destination_path = os.path.join(destination_folder, source_path.split("/")[-1])
        shutil.copy2(source_path, destination_path)
46/133:
root_dir = "data/IAM/online"
online_img_paths = []

dataset_levels = "lineImages"

online_img_paths = get_image_paths(root_dir, dataset_levels)
len(online_img_paths)
47/34:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
46/134:
# Define the transforms pipeline
transforms_iam_online = transforms.Compose([
    transforms.Resize((256, 1200)),
    transforms.CenterCrop((256, 400)),
])
46/135:
for img_path in online_img_paths[0]:
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    
    print(transformed_image.shape)
    
#     # Save the image
#     image.save('output.jpg')
46/136:
for img_path in online_img_paths[0]:
    
    print(img_path)
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    
    print(transformed_image.shape)
    
#     # Save the image
#     image.save('output.jpg')
46/137:
cvit_folder = "data/CVIT/Images_90K_Normalized/"
cvit_img_paths = []
46/138:
cvit_sub_folders = [f for f in os.listdir(cvit_folder)]
len(cvit_sub_folders)
46/139: cvit_sub_folders[:10]
46/140:
max_num_files = 1000
num_files = 0
break_flag = 0

# Iterate over files in the images folder
for sub_folder in cvit_sub_folders:    
    folder_path = os.path.join(cvit_folder, sub_folder)
    files = os.listdir(folder_path)
        
    for file in files:
        img_path = os.path.join(folder_path, file)
        cvit_img_paths.append(img_path)

        num_files += 1
        if num_files >= max_num_files:
            break_flag = 1
            break  # Break out of the loop if the desired limit is reached
    
    if break_flag == 1:
        break  # Break out of the loop if the desired limit is reached
46/141: len(cvit_img_paths)
46/142:
import ipywidgets as widgets
from IPython.display import display, Image, clear_output

def view_images(image_paths_list):
    
    num_images = len(image_paths_list)
    image_index = 0
    
    def display_image(image_index):
        image_path = image_paths_list[image_index]
        with open(image_path, 'rb') as file:
            image_widget.value = file.read()
    
    image_widget = widgets.Image(format='png')
    
    # Image navigation buttons
    prev_button = widgets.Button(description='Previous')
    next_button = widgets.Button(description='Next')
    
    def on_prev_button_clicked(b):
        nonlocal image_index
        image_index = (image_index - 1) % num_images
        display_image(image_index)
    
    def on_next_button_clicked(b):
        nonlocal image_index
        image_index = (image_index + 1) % num_images
        display_image(image_index)
    
    display(image_widget)
    
    # Initial display
    display_image(image_index)
    
    prev_button.on_click(on_prev_button_clicked)
    next_button.on_click(on_next_button_clicked)
    
    # Display navigation buttons
    display(widgets.HBox([prev_button, next_button]))
46/143: view_images(cvit_img_paths)
46/144:
img = Image.open(cvit_img_paths[0])
img
46/145:
root_dir = "data/IAM/online"
online_img_paths = []

dataset_levels = "lineImages"

online_img_paths = get_image_paths(root_dir, dataset_levels)
len(online_img_paths)
46/146: online_img_paths[0]
46/147:
img = Image.open(cvit_img_paths[0])
img
46/148: view_images(cvit_img_paths)
46/149: cvit_img_paths[0:10]
46/150:
img = Image.open(cvit_img_paths[1])
img
46/151:
import os
import json
import re
import sys

from PIL import Image
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine

import magic

%matplotlib inline
import numpy as np
! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab
46/152:
for img_path in online_img_paths[0:10]:
    
    print(img_path)
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    
    print(transformed_image.shape)
    
#     # Save the image
#     image.save('output.jpg')
46/153:
for img_path in online_img_paths[0:10]:
    
    print(img_path)
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    
    print(transformed_image.size)
    
#     # Save the image
#     image.save('output.jpg')
46/154:
for img_path in online_img_paths[0:10]:
    
    print(img_path)
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    transformed_image.show()
    
    print(transformed_image.size)
    
#     # Save the image
#     image.save('output.jpg')
46/155:
for img_path in online_img_paths[0:10]:
    
    print(img_path)
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    transformed_image.show()
    
#     print(transformed_image.size)
    
#     # Save the image
#     image.save('output.jpg')
46/156: transformed_image.show()
46/157: transformed_image[0].show()
46/158:
# Define the transforms pipeline
transforms_iam_online = transforms.Compose([
    transforms.Resize((256, 1200)),
    transforms.FiveCrop((256, 400)),
    Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # returns a 4D tensor
])
46/159:
# Define the transforms pipeline
transforms_iam_online = transforms.Compose([
    transforms.Resize((256, 1200)),
    transforms.FiveCrop((256, 400)),
    transforms.Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # returns a 4D tensor
])
46/160:
for img_path in online_img_paths[0:10]:
    
    print(img_path)
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    transformed_image.show()
    
#     print(transformed_image.size)
    
#     # Save the image
#     image.save('output.jpg')
46/161:
import os
import json
import re
import sys

from PIL import Image
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine

import magic

%matplotlib inline
import numpy as np
! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab
46/162:
# Define the transforms pipeline
transforms_iam_online = transforms.Compose([
    transforms.Resize((256, 1200)),
    transforms.FiveCrop((256, 400)),
    transforms.Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # returns a 4D tensor
])
46/163:
for img_path in online_img_paths[0:10]:
    
    print(img_path)
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    transformed_image.show()
    
#     print(transformed_image.size)
    
#     # Save the image
#     image.save('output.jpg')
46/164:
# Define the transforms pipeline
transforms_iam_online = transforms.Compose([
    transforms.Resize((256, 1200)),
    transforms.FiveCrop((256, 400)),
    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])) # returns a 4D tensor
])
46/165:
for img_path in online_img_paths[0:10]:
    
    print(img_path)
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    transformed_image.show()
    
#     print(transformed_image.size)
    
#     # Save the image
#     image.save('output.jpg')
46/166:
for img_path in online_img_paths[0:10]:
    
    print(img_path)
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    
    for image in transformed_image:
        transforms.ToPILImage(image).show()
    
#     print(transformed_image.size)
    
#     # Save the image
#     image.save('output.jpg')
46/167:
for img_path in online_img_paths[0:10]:
    
    print(img_path)
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    
    for image in transformed_image:
        temp = transforms.ToPILImage(image)
        temp.show()
    
#     print(transformed_image.size)
    
#     # Save the image
#     image.save('output.jpg')
47/35:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
47/36:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
47/37:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((256, 500)),
    transforms.CenterCrop((256, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
47/38:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((256, 500)),
    transforms.CenterCrop((256, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
47/39:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
47/40:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
47/41:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
47/42:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
47/43:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
47/44:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
47/45:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
47/46:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
47/47: test_correct/len(test_loader.dataset)
47/48: test_correct/len(test_loader.dataset)
47/49: y_true_cpu, y_pred_cpu
47/50: y_true_cpu, y_pred_cpu
47/51:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
47/52:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((256, 500)),
    transforms.CenterCrop((256, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
47/53:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((256, 500)),
    transforms.CenterCrop((256, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
47/54:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
47/55:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
47/56:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
47/57:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
47/58: test_correct/len(test_loader.dataset)
46/168:
for img_path in online_img_paths[0:10]:
    
    print(img_path)
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    
    for image in transformed_image:
        temp = transforms.ToPILImage(image)
        temp.show()
    
#     print(transformed_image.size)
    
#     # Save the image
#     image.save('output.jpg')
46/169:
for img_path in online_img_paths[0:10]:
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    
    for image in transformed_image:
        temp = transforms.ToPILImage()(image)
        temp.show()
    
#     print(transformed_image.size)
    
#     # Save the image
#     image.save('output.jpg')
46/170:
for img_path in online_img_paths[0:1]:
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    
    for image in transformed_image:
        temp = transforms.ToPILImage()(image)
        temp.show()
    
#     print(transformed_image.size)
    
#     # Save the image
#     image.save('output.jpg')
46/171:
for img_path in online_img_paths[0:1]:
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    
    for image in transformed_image:
        temp = transforms.ToPILImage()(image)
    temp.show()
    
#     print(transformed_image.size)
    
#     # Save the image
#     image.save('output.jpg')
46/172:
import os
import json
import re
import sys

from PIL import Image
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine

import magic

%matplotlib inline
import numpy as np
! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab
46/173:
for img_path in online_img_paths[0:1]:
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    
    for image in transformed_image:
        temp = transforms.ToPILImage()(image)
    temp.show()
    
#     print(transformed_image.size)
    
#     # Save the image
#     image.save('output.jpg')
46/174:
for img_path in online_img_paths[0:1]:
    count = 0
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    
    for image in transformed_image:
        temp = transforms.ToPILImage()(image)
        
        img_name = img_path.split("/")[-1][:-4]
        
        # Save the image
        temp.save("data/IAM/"+ img_name +'.jpg')
        
        count += 1
46/175:
for img_path in online_img_paths[0:1]:
    count = 0
    
    # Open image
    img = Image.open(img_path)
    
    # Apply transform
    transformed_image = transforms_iam_online(img)
    
    for image in transformed_image:
        temp = transforms.ToPILImage()(image)
        
        img_name = img_path.split("/")[-1][:-4]
        
        # Save the image
        temp.save("data/IAM/"+ img_name + str(count) +'.jpg')
        
        count += 1
46/176: online_img_paths[0]
46/177: copy_files("data/all_data/handwritten/", online_img_paths)
46/178:
import os
import json
import re
import sys
import shutil

from PIL import Image
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine

import magic

%matplotlib inline
import numpy as np
! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab
46/179: copy_files("data/all_data/handwritten/", online_img_paths)
47/59: ! ls data/all_data/handwritten | wc -l
47/60: ! ls data/all_data/typed/ | wc -l
47/61: ! ls data/all_data/handwritten | wc -l
47/62: ! ls data/all_data/typed/ | wc -l
47/63: ! ls data/all_data/handwritten | wc -l
47/64: ! ls data/all_data/handwritten | wc -l
47/65: ! ls data/all_data/handwritten | wc -l
47/66: ! ls data/all_data/handwritten | wc -l
47/67: ! ls data/all_data/handwritten | wc -l
47/68: ! ls data/all_data/handwritten | wc -l
47/69: ! ls data/all_data/handwritten | wc -l
47/70:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
47/71:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
47/72:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize((256, 600)),
    transforms.CenterCrop((256, 350)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
47/73: ! ls data/all_data/handwritten | wc -l
47/74: ! ls data/all_data/typed/ | wc -l
47/75:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
47/76:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
47/77:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
47/78:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

        train_pred = []
        train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
            train_true.extend(labels) # Save Truth
            train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
47/79:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
47/80:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((256, 600)),
    transforms.CenterCrop((256, 350)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
47/81:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
47/82:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
47/83:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
47/84:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
47/85: test_correct/len(test_loader.dataset)
48/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
48/2:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize((256, 600)),
    transforms.CenterCrop((256, 350)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
48/3: ! ls data/all_data/handwritten | wc -l
48/4: ! ls data/all_data/typed/ | wc -l
48/5:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
48/6:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
48/7:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
48/8:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
48/9:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
48/10:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
48/11:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
                y_true.extend(labels) # Save Truth
                y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
49/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
49/2:
dataset_path = "data/all_data/"
preprocess = transforms.Compose([
    transforms.Resize((256, 600)),
    transforms.CenterCrop((256, 350)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset
dataset = ImageFolder(dataset_path, transform=preprocess)
49/3: ! ls data/all_data/handwritten | wc -l
49/4: ! ls data/all_data/typed/ | wc -l
49/5:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
49/6:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
49/7:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
49/8:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

#         # Save the best model based on validation loss and early stopping
#         if val_loss < best_val_loss:
#             best_val_loss = val_loss
#             counter = 0
#             # Save the model
#             print("Saving model...")
#             torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
#         else:
#             counter += 1
#             # Check if the counter reaches the patience limit
#             if counter >= patience:
#                 print('Early stopping triggered...')
#             break

print("\n=== Training complete! ===\n")
50/1: ! wget -P data/IAM/ --load-cookies data/cookies.txthttps://fki.tic.heia-fr.ch/DBs/iamHistDB/data/washingtondb-v1.0.zip
50/2: ! wget -P data/IAM/ --load-cookies data/cookies.txt https://fki.tic.heia-fr.ch/DBs/iamHistDB/data/washingtondb-v1.0.zip
50/3: ! unzip -q data/IAM/washingtondb-v1.0.zip
50/4: ! unzip -q data/IAM/washingtondb-v1.0.zip -d data/IAM/
50/5:
import os
import json
import re
import sys
import shutil

from PIL import Image
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine

import magic

%matplotlib inline
import numpy as np
! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab
50/6:
folder = "data/IAM/washingtondb-v1.0"

for root, dirs, files in os.walk(folder):
    
    print(dirs)
50/7:
folder = "data/IAM/washingtondb-v1.0/data"

for root, dirs, files in os.walk(folder):
    
    print(dirs)
50/8:
folder = "data/IAM/washingtondb-v1.0/data"
washington_img_paths []

for root, dirs, files in os.walk(folder):

    for file in files:
        washington_img_paths.append(file)
50/9:
folder = "data/IAM/washingtondb-v1.0/data"
washington_img_paths = []

for root, dirs, files in os.walk(folder):

    for file in files:
        washington_img_paths.append(file)
50/10: washington_img_paths[0]
50/11:
folder = "data/IAM/washingtondb-v1.0/data"
washington_img_paths = []

for root, dirs, files in os.walk(folder):

    for file in files:
        washington_img_paths.append(os.path.join(root, file))
50/12: washington_img_paths[0]
50/13:
folder = "data/IAM/washingtondb-v1.0/data"
washington_img_paths = []

for root, dirs, files in os.walk(folder):
    
    print(root)

    for file in files:
        washington_img_paths.append(os.path.join(root, file))
50/14:
# Define the custom conditional transform
class ConditionalTransform:
    def __init__(self, h_threshold, w_threshold):
        self.threshold = threshold
        self.transform1 = transform1
    
    def __call__(self, image):
        width, height = image.size
        
        if width > self.w_threshold and height > self.h_threshold:
            transforms.Resize((256, 500)),
            transforms.CenterCrop((256, 300)),
            return self.transform1(image)
        else:
            return image
51/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
51/2:
dataset_path = "data/all_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize((256, 600)),
    transforms.CenterCrop((256, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
51/3: ! ls data/all_data/handwritten | wc -l
51/4: ! ls data/all_data/typed/ | wc -l
51/5:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
51/6:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
51/7:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
51/8:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20

patience = 5
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

#         # Save the best model based on validation loss and early stopping
#         if val_loss < best_val_loss:
#             best_val_loss = val_loss
#             counter = 0
#             # Save the model
#             print("Saving model...")
#             torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
#         else:
#             counter += 1
#             # Check if the counter reaches the patience limit
#             if counter >= patience:
#                 print('Early stopping triggered...')
#             break

print("\n=== Training complete! ===\n")
50/15:
# Define the custom transforms pipeline
washington_transforms_lines = transforms.Compose([
    transforms.Resize((256, 960)),
    transforms.FiveCrop((256, 300)),
    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
])
50/16:
# Define the custom transforms pipeline
washington_transforms_words = transforms.Compose([
    transforms.Pad(),
    transforms.Resize(256, 300)
    transforms.ToTensor()
])
50/17:
# Define the custom transforms pipeline
washington_transforms_words = transforms.Compose([
    transforms.Pad(),
    transforms.Resize(256, 300),
    transforms.ToTensor()
])
50/18:
# Define the custom transforms pipeline
washington_transforms_lines = transforms.Compose([
    transforms.GaussianBlur(3, sigma=(0.1, 2.0)),
    transforms.Resize((256, 960)),
    transforms.FiveCrop((256, 300)),
    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
])
50/19:
# Define the custom transforms pipeline
washington_transforms_words = transforms.Compose([
    transforms.GaussianBlur(3, sigma=(0.1, 2.0)),
    transforms.Pad(),
    transforms.Resize(256, 300),
    transforms.ToTensor()
])
50/20:
# Define the custom transforms pipeline
washington_transforms_words = transforms.Compose([
    transforms.GaussianBlur(3, sigma=(0.1, 2.0)),
    transforms.Pad([120, 68, 120, 68]),
    transforms.Resize(256, 300),
    transforms.ToTensor()
])
50/21:
# Define the custom transforms pipeline
washington_transforms_words = transforms.Compose([
    transforms.GaussianBlur(3, sigma=(0.1, 2.0)),
    transforms.Pad([120, 68, 120, 68]),
    transforms.Resize((256, 300)),
    transforms.ToTensor()
])
50/22:
combined_transforms = ConditionalTransform(washington_transforms_lines, 
                                           washington_transforms_words,
                                          (120, 600))
50/23:
# All images have a width of 120 pixels. 

# Define the custom conditional transform
class ConditionalTransform:
    def __init__(self, transform1, transform2, img_size_threshold:tuple):
        self.transform1 = transform1
        self.transform2 = transform2
        self.img_size_threshold = img_size_threshold
    
    def __call__(self, image):
        width, height = image.size
        
        if width >= self.img_size_threshold[1] and height >= self.img_size_threshold[0]:
            t = self.transform1(image)
        else:
            t = self.transform2(image)
            
            
        for image in t:
            temp = transforms.ToPILImage()(image)

            img_name = img_path.split("/")[-1][:-4]

            # Save the image
            temp.save("data/IAM/"+ img_name + str(count) +'.jpg')

            count += 1
50/24:
# Define the custom transforms pipeline
washington_transforms_lines = transforms.Compose([
    transforms.GaussianBlur(3, sigma=(0.1, 2.0)),
    transforms.Resize((256, 960)),
    transforms.FiveCrop((256, 300)),
    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
])
50/25:
# Define the custom transforms pipeline
washington_transforms_words = transforms.Compose([
    transforms.GaussianBlur(3, sigma=(0.1, 2.0)),
    transforms.Pad([120, 68, 120, 68]),
    transforms.Resize((256, 300)),
    transforms.ToTensor()
])
50/26:
combined_transforms = ConditionalTransform(washington_transforms_lines, 
                                           washington_transforms_words,
                                          (120, 600))
50/27:
for img in washington_img_paths[0:2]:
    combined_transforms(img)
50/28:
for img in washington_img_paths[0:2]:
    combined_transforms(Image.open(img))
50/29:
# Define the custom transforms pipeline
washington_transforms_words = transforms.Compose([
    transforms.functional.GaussianBlur(3, sigma=(0.1, 2.0)),
    transforms.Pad([120, 68, 120, 68]),
    transforms.Resize((256, 300)),
    transforms.ToTensor()
])
50/30:
import os
import json
import re
import sys
import shutil

from PIL import Image
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine
import torchvision.transforms.functional as TF

import magic

%matplotlib inline
import numpy as np
! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab
50/31:
# Define the custom transforms pipeline
washington_transforms_lines = transforms.Compose([
    TF.GaussianBlur(3, sigma=(0.1, 2.0)),
    transforms.Resize((256, 960)),
    transforms.FiveCrop((256, 300)),
    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
])
50/32:
# Define the custom transforms pipeline
washington_transforms_lines = transforms.Compose([
    TF.gaussianblur(3, sigma=(0.1, 2.0)),
    transforms.Resize((256, 960)),
    transforms.FiveCrop((256, 300)),
    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
])
50/33:
# Define the custom transforms pipeline
washington_transforms_lines = transforms.Compose([
    TF.gaussian_blur(3, sigma=(0.1, 2.0)),
    transforms.Resize((256, 960)),
    transforms.FiveCrop((256, 300)),
    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
])
50/34:
# Define the custom transforms pipeline
washington_transforms_lines = transforms.Compose([
    transforms.GaussianBlur(3, sigma=(0.1, 2.0)),
    transforms.Resize((256, 960)),
    transforms.FiveCrop((256, 300)),
    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
])
50/35:
# Define the custom transforms pipeline
washington_transforms_words = transforms.Compose([
    transforms.GaussianBlur(3, sigma=(0.1, 2.0)),
    transforms.Pad([120, 68, 120, 68]),
    transforms.Resize((256, 300)),
    transforms.ToTensor()
])
50/36:
combined_transforms = ConditionalTransform(washington_transforms_lines, 
                                           washington_transforms_words,
                                          (120, 600))
50/37:
for img_path in washington_img_paths[0:2]:
    combined_transforms(Image.open(img_path))
50/38:
for img_path in washington_img_paths[0:2]:
    combined_transforms(transforms.ToPILImage(Image.open(img_path)))
50/39:
for img_path in washington_img_paths[0:2]:
    combined_transforms(Image.open(img_path))
50/40:
# Define the custom transforms pipeline for line images
washington_transforms_lines = transforms.Compose([
    transforms.GaussianBlur(3, sigma=(0.1, 2.0)),
    transforms.Resize((256, 960)),
    transforms.FiveCrop((256, 300)),
    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
])
50/41:
# Define the custom transforms pipeline for word images
washington_transforms_words = transforms.Compose([
    transforms.GaussianBlur(3, sigma=(0.1, 2.0)),
    transforms.Pad([120, 68, 120, 68]),
    transforms.Resize((256, 300)),
    transforms.ToTensor()
])
50/42:
combined_transforms = ConditionalTransform(washington_transforms_lines, 
                                           washington_transforms_words,
                                          (120, 600))
50/43:
for img_path in washington_img_paths[0:2]:
    combined_transforms(Image.open(img_path))
50/44:
for img_path in washington_img_paths[0:2]:
    img = Image.open(img_path)
    combined_transforms(img)
50/45:
# Define the custom transforms pipeline for word images
washington_transforms_words = transforms.Compose([
    transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
    transforms.Pad([120, 68, 120, 68]),
    transforms.Resize((256, 300)),
    transforms.ToTensor()
])
50/46:
# Define the custom transforms pipeline for line images
washington_transforms_lines = transforms.Compose([
    transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
    transforms.Resize((256, 960)),
    transforms.FiveCrop((256, 300)),
    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
])
50/47:
# Define the custom transforms pipeline for word images
washington_transforms_words = transforms.Compose([
    transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
    transforms.Pad([120, 68, 120, 68]),
    transforms.Resize((256, 300)),
    transforms.ToTensor()
])
50/48:
combined_transforms = ConditionalTransform(washington_transforms_lines, 
                                           washington_transforms_words,
                                          (120, 600))
50/49:
for img_path in washington_img_paths[0:2]:
    img = Image.open(img_path)
    combined_transforms(img)
52/1:
import os
import json
import re
import sys
import shutil

from PIL import Image
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine
import torchvision.transforms.functional as TF

import magic

%matplotlib inline
import numpy as np
! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab
52/2:
# All images have a width of 120 pixels. 

# Define the custom conditional transform
def conditional_transforms(image, transform1, transform2, img_size_threshold:tuple):
    
    count = 0
    width, height = image.size
    
    if(width >= img_size_threshold[1] and heigh >= img_size_threshold[0]):
        t = transform1(image)
    else:
        t = transform2(image)

    for tensor in t:

        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]

        # Save the image
        tensor_to_image.save("data/IAM/"+ img_name + str(count) +'.jpg')

        count += 1
52/3:
# Collect image paths to preprocess
folder = "data/IAM/washingtondb-v1.0/data"
washington_img_paths = []

for root, dirs, files in os.walk(folder):
    for file in files:
        washington_img_paths.append(os.path.join(root, file))
52/4: washington_img_paths[0]
52/5:
# Define the custom transforms pipeline for line images
washington_transforms_lines = transforms.Compose([
    transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
    transforms.Resize((256, 960)),
    transforms.FiveCrop((256, 300)),
    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
])
52/6:
# Define the custom transforms pipeline for word images
washington_transforms_words = transforms.Compose([
    transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
    transforms.Pad([120, 68, 120, 68]),
    transforms.Resize((256, 300)),
    transforms.ToTensor()
])
52/7:
# All images have a width of 120 pixels. 

# Define the custom conditional transform
def conditional_transforms(image, transform1, transform2, img_size_threshold:tuple):
    
    count = 0
    width, height = image.size
    
    if(width >= img_size_threshold[1] and heigh >= img_size_threshold[0]):
        t = transform1(image)
    else:
        t = transform2(image)

    for tensor in t:

        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]

        # Save the image
        tensor_to_image.save("data/IAM/"+ img_name + str(count) +'.jpg')

        count += 1
52/8:
# combined_transforms = ConditionalTransform(washington_transforms_lines, 
#                                            washington_transforms_words,
#                                           (120, 600))
52/9:
for img_path in washington_img_paths[0:2]:
    img = Image.open(img_path)
    combined_transforms(img)
52/10:
for img_path in washington_img_paths[0:2]:
    img = Image.open(img_path)
    conditional_transforms(img)
52/11:
for img_path in washington_img_paths[0:2]:
    img = Image.open(img_path)
    conditional_transforms(img, washington_transforms_lines, washington_transforms_words)
52/12:
for img_path in washington_img_paths[0:2]:
    img = Image.open(img_path)
    conditional_transforms(img, washington_transforms_lines, washington_transforms_words, (120, 600))
52/13:
# All images have a width of 120 pixels. 

# Define the custom conditional transform
def conditional_transforms(image, transform1, transform2, img_size_threshold:tuple):
    
    count = 0
    width, height = image.size
    
    if(width >= img_size_threshold[1] and height >= img_size_threshold[0]):
        t = transform1(image)
    else:
        t = transform2(image)

    for tensor in t:

        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]

        # Save the image
        tensor_to_image.save("data/IAM/"+ img_name + str(count) +'.jpg')

        count += 1
52/14:
# combined_transforms = ConditionalTransform(washington_transforms_lines, 
#                                            washington_transforms_words,
#                                           (120, 600))
52/15:
for img_path in washington_img_paths[0:2]:
    img = Image.open(img_path)
    conditional_transforms(img, washington_transforms_lines, washington_transforms_words, (120, 600))
52/16:
# Define the custom transforms pipeline for word images
washington_transforms_words = transforms.Compose([
    transforms.ToTensor(),
    transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
    transforms.Pad([120, 68, 120, 68]),
    transforms.Resize((256, 300)),
    transforms.ToTensor()
])
52/17:
# Define the custom transforms pipeline for line images
washington_transforms_lines = transforms.Compose([
    transforms.ToTensor(),
    transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
    transforms.Resize((256, 960)),
    transforms.FiveCrop((256, 300)),
    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
])
52/18:
# All images have a width of 120 pixels. 

# Define the custom conditional transform
def conditional_transforms(image, transform1, transform2, img_size_threshold:tuple):
    
    count = 0
    width, height = image.size
    
    if(width >= img_size_threshold[1] and height >= img_size_threshold[0]):
        t = transform1(image)
    else:
        t = transform2(image)

    for tensor in t:

        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]

        # Save the image
        tensor_to_image.save("data/IAM/"+ img_name + str(count) +'.jpg')

        count += 1
52/19:
# combined_transforms = ConditionalTransform(washington_transforms_lines, 
#                                            washington_transforms_words,
#                                           (120, 600))
52/20:
for img_path in washington_img_paths[0:2]:
    img = Image.open(img_path)
    conditional_transforms(img, washington_transforms_lines, washington_transforms_words, (120, 600))
52/21:
# Define the custom transforms pipeline for line images
washington_transforms_lines = transforms.Compose([
    transforms.ToTensor(),
    transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
    transforms.Resize((256, 960)),
    transforms.FiveCrop((256, 300)),
    transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
])
52/22:
# Define the custom transforms pipeline for word images
washington_transforms_words = transforms.Compose([
    transforms.ToTensor(),
    transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
    transforms.Pad([120, 68, 120, 68]),
    transforms.Resize((256, 300)),
    transforms.ToTensor()
])
52/23:
# All images have a width of 120 pixels. 

# Define the custom conditional transform
def conditional_transforms(image, transform1, transform2, img_size_threshold:tuple):
    
    count = 0
    width, height = image.size
    
    if(width >= img_size_threshold[1] and height >= img_size_threshold[0]):
        t = transform1(image)
    else:
        t = transform2(image)

    for tensor in t:

        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]

        # Save the image
        tensor_to_image.save("data/IAM/"+ img_name + str(count) +'.jpg')

        count += 1
52/24:
# combined_transforms = ConditionalTransform(washington_transforms_lines, 
#                                            washington_transforms_words,
#                                           (120, 600))
52/25:
for img_path in washington_img_paths[0:2]:
    img = Image.open(img_path)
    conditional_transforms(img, washington_transforms_lines, washington_transforms_words, (120, 600))
52/26:
# Define the custom transforms pipeline for line images
washington_transforms_lines = transforms.Compose([
    transforms.ToTensor(),
    transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
    transforms.Resize((256, 960)),
    transforms.FiveCrop((256, 300)),
    transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
])
52/27:
# Define the custom transforms pipeline for word images
washington_transforms_words = transforms.Compose([
    transforms.ToTensor(),
    transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
    transforms.Pad([120, 68, 120, 68]),
    transforms.Resize((256, 300)),
])
52/28:
# All images have a width of 120 pixels. 

# Define the custom conditional transform
def conditional_transforms(image, transform1, transform2, img_size_threshold:tuple):
    
    count = 0
    width, height = image.size
    
    if(width >= img_size_threshold[1] and height >= img_size_threshold[0]):
        t = transform1(image)
    else:
        t = transform2(image)

    for tensor in t:

        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]

        # Save the image
        tensor_to_image.save("data/IAM/"+ img_name + str(count) +'.jpg')

        count += 1
52/29:
# combined_transforms = ConditionalTransform(washington_transforms_lines, 
#                                            washington_transforms_words,
#                                           (120, 600))
52/30:
for img_path in washington_img_paths[0:2]:
    img = Image.open(img_path)
    conditional_transforms(img, washington_transforms_lines, washington_transforms_words, (120, 600))
52/31:
for img_path in washington_img_paths:
    img = Image.open(img_path)
    conditional_transforms(img, washington_transforms_lines, washington_transforms_words, (120, 600))
52/32:
# All images have a width of 120 pixels. 

# Define the custom conditional transform
def conditional_transforms(img_path, transform1, transform2, img_size_threshold:tuple):
    image = Image.open(img_path)
    width, height = image.size
    count = 0
    
    if(width >= img_size_threshold[1] and height >= img_size_threshold[0]):
        t = transform1(image)
    else:
        t = transform2(image)

    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]

        # Save the image
        tensor_to_image.save("data/all_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/33:
for img_path in washington_img_paths:
    conditional_transforms(img_path, washington_transforms_lines, washington_transforms_words, (120, 600))
52/34: len(washington_img_paths)
52/35:
# Collect image paths to preprocess
folder = "data/IAM/washingtondb-v1.0/data"
washington_img_paths = []

for root, dirs, files in os.walk(folder):
    for file in files:
        if file in [".DS_Store"]
        washington_img_paths.append(os.path.join(root, file))
52/36:
# Collect image paths to preprocess
folder = "data/IAM/washingtondb-v1.0/data"
washington_img_paths = []

for root, dirs, files in os.walk(folder):
    for file in files:
        if file in [".DS_Store"]:
            continue
        washington_img_paths.append(os.path.join(root, file))
52/37: len(washington_img_paths)
52/38: washington_img_paths[0]
52/39:
# Define the custom transforms pipeline for line images
washington_transforms_lines = transforms.Compose([
    transforms.ToTensor(),
    transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
    transforms.Resize((256, 960)),
    transforms.FiveCrop((256, 300)),
    transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
])
52/40:
# Define the custom transforms pipeline for word images
washington_transforms_words = transforms.Compose([
    transforms.ToTensor(),
    transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
    transforms.Pad([120, 68, 120, 68]),
    transforms.Resize((256, 300)),
])
52/41:
# All images have a width of 120 pixels. 

# Define the custom conditional transform
def conditional_transforms(img_path, transform1, transform2, img_size_threshold:tuple):
    image = Image.open(img_path)
    width, height = image.size
    count = 0
    
    if(width >= img_size_threshold[1] and height >= img_size_threshold[0]):
        t = transform1(image)
    else:
        t = transform2(image)

    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]

        # Save the image
        tensor_to_image.save("data/all_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/42:
for img_path in washington_img_paths:
    conditional_transforms(img_path, washington_transforms_lines, washington_transforms_words, (120, 600))
52/43:
import os
import json
import re
import sys
import shutil

from PIL import Image
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine
import torchvision.transforms.functional as TF

import magic

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab
51/9:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
51/10:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((256, 600)),
    transforms.CenterCrop((256, 350)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/11:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/12:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/13:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/14:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/15: test_correct/len(test_loader.dataset)
51/16: y_true_cpu, y_pred_cpu
51/17:
dataset_path = "data/all_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize((256, 600)),
    transforms.CenterCrop((256, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
51/18: ! ls data/all_data/handwritten | wc -l
51/19: ! ls data/all_data/typed/ | wc -l
51/20:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
51/21:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
51/22:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 5

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

#         # Save the best model based on validation loss and early stopping
#         if val_loss < best_val_loss:
#             best_val_loss = val_loss
#             counter = 0
#             # Save the model
#             print("Saving model...")
#             torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
#         else:
#             counter += 1
#             # Check if the counter reaches the patience limit
#             if counter >= patience:
#                 print('Early stopping triggered...')
#             break

print("\n=== Training complete! ===\n")
52/44: # For typewriter looking text
51/23:
dataset_path = "data/all_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
51/24: ! ls data/all_data/handwritten | wc -l
51/25: ! ls data/all_data/typed/ | wc -l
51/26:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
51/27:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
51/28:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
51/29:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
51/30:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 5

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

#         # Save the best model based on validation loss and early stopping
#         if val_loss < best_val_loss:
#             best_val_loss = val_loss
#             counter = 0
#             # Save the model
#             print("Saving model...")
#             torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
#         else:
#             counter += 1
#             # Check if the counter reaches the patience limit
#             if counter >= patience:
#                 print('Early stopping triggered...')
#             break

print("\n=== Training complete! ===\n")
51/31: torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
51/32:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
51/33:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/34:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/35:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/36:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/37:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/38: test_correct/len(test_loader.dataset)
51/39: y_true_cpu, y_pred_cpu
51/40:
idxs_mask = ((y_pred_cpu == y_true_cpu) == False).nonzero()
idxs_mask
51/41:
idxs_mask = ((y_pred_cpu == y_true_cpu) == False)
idxs_mask
51/42: type(y_true_cpu)
51/43:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/44:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(len(test_loader.dataset)):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/45:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(5):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/46:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, len(test_loader.dataset), figsize=(12, 3))
    for i in range(len(test_loader.dataset)):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/47:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 128)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/48:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/49:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/50:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/51:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/52: test_correct/len(test_loader.dataset)
51/53: y_true_cpu, y_pred_cpu
51/54:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 256)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/55:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/56:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/57:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/58:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/59: test_correct/len(test_loader.dataset)
51/60: y_true_cpu, y_pred_cpu
51/61:
idxs_mask = ((y_pred_cpu == y_true_cpu) == False)
idxs_mask
51/62:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, len(test_loader.dataset), figsize=(12, 3))
    for i in range(len(test_loader.dataset)):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/63:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, len(test_loader.dataset)/4, figsize=(12, 3))
    for i in range(len(test_loader.dataset)/4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/64:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, int(len(test_loader.dataset)/4), figsize=(12, 3))
    for i in range(int(len(test_loader.dataset)/4)):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/65:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/66:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, 4, figsize=(12, 3))
    for i in range(len(test_loader.dataset)/4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/67:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, 4, figsize=(12, 3))
    for i in range(len(test_loader.dataset)):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/68:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(len(test_loader.dataset)):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/69:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 16, figsize=(12, 3))
    for i in range(len(test_loader.dataset)):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/70:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(2, 8, figsize=(12, 3))
    for i in range(len(test_loader.dataset)):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/71:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 16, figsize=(12, 3))
    for i in range(len(test_loader.dataset)):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/72:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 16, figsize=(12, 3))
    for i in range(len(test_loader.dataset)):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/73:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 256)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/74:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 16, figsize=(12, 3))
    for i in range(len(test_loader.dataset)):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/75: torch.save(model.state_dict(), 'model/vgg16_best_model_0-82.pth')
51/76: # torch.save(model.state_dict(), 'model/vgg16_best_model_0-82.pth')
51/77:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
51/78:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 256)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/79:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/80:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/81:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/82:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/83: test_correct/len(test_loader.dataset)
51/84: y_true_cpu, y_pred_cpu
51/85: type(y_true_cpu)
51/86:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, int(len(test_loader.dataset)/4), figsize=(12, 3))
    for i in range(4):
        for j in range(int(len(test_loader.dataset)/4))
            axes[i][j].imshow(images[i])
            axes[i][j].axis('off')
    plt.show()
51/87:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model_0-82.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
51/88:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 256)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/89:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/90:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/91:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/92:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/93: test_correct/len(test_loader.dataset)
51/94: y_true_cpu, y_pred_cpu
51/95:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, int(len(test_loader.dataset)/4), figsize=(12, 3))
    for i in range(4):
        for j in range(int(len(test_loader.dataset)/4)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            img_no += 1
    plt.show()
51/96:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((256, 600)),
    transforms.CenterCrop((256, 256)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/97:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/98:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/99:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/100:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/101: test_correct/len(test_loader.dataset)
51/102:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((256, 600)),
    transforms.CenterCrop((256, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/103:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/104:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/105:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/106:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/107: test_correct/len(test_loader.dataset)
51/108:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 256)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/109:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/110:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/111:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/112:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/113: test_correct/len(test_loader.dataset)
51/114:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/115:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/116:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/117:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/118:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/119: test_correct/len(test_loader.dataset)
51/120:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 256)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/121:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/122:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/123:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/124:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/125: test_correct/len(test_loader.dataset)
51/126: y_true_cpu, y_pred_cpu
51/127:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
51/128:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 256)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/129:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/130:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/131:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/132:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/133: test_correct/len(test_loader.dataset)
51/134:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/135:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/136:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/137:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/138:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/139: test_correct/len(test_loader.dataset)
51/140:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((256, 600)),
    transforms.CenterCrop((256, 350)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/141:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/142:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/143:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/144:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/145: test_correct/len(test_loader.dataset)
51/146: y_true_cpu, y_pred_cpu
51/147:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 500)),
    transforms.CenterCrop((128, 250)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/148:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/149:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/150:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/151:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/152:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 00)),
    transforms.CenterCrop((128, 128)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/153:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 128)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/154:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/155:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/156:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/157:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/158: test_correct/len(test_loader.dataset)
51/159:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/160:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/161:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/162:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/163:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/164: test_correct/len(test_loader.dataset)
51/165:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model_0-75.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
51/166:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model_0-82.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
51/167:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/168: torch.save(model.state_dict(), 'model/vgg16_best_model_0-75.pth')
51/169:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model_0-82.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
51/170:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model_0-75.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
51/171:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/172:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/173:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/174:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/175:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/176: test_correct/len(test_loader.dataset)
51/177: y_true_cpu, y_pred_cpu
51/178:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, int(len(test_loader.dataset)/4), figsize=(12, 3))
    for i in range(4):
        for j in range(int(len(test_loader.dataset)/4)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            img_no += 1
    plt.show()
51/179:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 600)),
    transforms.CenterCrop((128, 350)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/180:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/181:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/182:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/183:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/184:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((256, 600)),
    transforms.CenterCrop((256, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/185:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/186:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/187:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/188:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/189: test_correct/len(test_loader.dataset)
51/190: ! ls data/all_data/handwritten | wc -l
51/191: ! ls data/all_data/typed/ | wc -l
51/192:
# Define the percentage of samples for the validation set
val_split = 0.1  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
51/193:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
51/194:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
51/195:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 5

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            pass
#             counter += 1
#             # Check if the counter reaches the patience limit
#             if counter >= patience:
#                 print('Early stopping triggered...')
#             break

print("\n=== Training complete! ===\n")
51/196:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 8

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            pass
#             counter += 1
#             # Check if the counter reaches the patience limit
#             if counter >= patience:
#                 print('Early stopping triggered...')
#             break

print("\n=== Training complete! ===\n")
52/45:
import os
import json
import re
import sys
import shutil

from PIL import Image
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine
import torchvision.transforms.functional as TF

import magic

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
52/46:
import os
import json
import re
import sys
import shutil

from PIL import Image
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine
import torchvision.transforms.functional as TF

import magic

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
52/47:
# Util function to collect image paths from subdirectories
def get_image_paths(root_dir, data_levels=None, file_exts=None):
    """
    root_dir (str): Root directory to start from
    data_levels (list): Subdirectories to search for
    file_exts (tuple): File extensions to look for
    """
    
    image_paths = []
    
    for data_level in data_levels:
        if(data_level == None or file_ext == None):
            # just collect the image paths (no filters)
            for root, _, files in os.walk(os.path.join(root_dir, data_level)):
                for file in files:
                    image_paths.append(os.path.join(root, file))

        else:
            for root, _, files in os.walk(os.path.join(root_dir, data_level)):
                for file in files:
                    if file.endswith(file_exts):
                        image_paths.append(os.path.join(root, file))
                        
    return image_paths
52/48:
# Preprocessing pipeline
transforms_iam_lines = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.FiveCrop((100, 300)),
    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
])

transforms_iam_words = transforms.Compose([
    transforms.Resize([100, ])
])
52/49:
# Util function to collect image paths from subdirectories
def get_image_paths(root_dir, data_levels=None, file_exts=None):
    """
    root_dir (str): Root directory to start from
    data_levels (list): Subdirectories to search for
    file_exts (tuple): File extensions to look for
    """
    
    image_paths = []
    
    for data_level in data_levels:
        if(data_level == None or file_ext == None):
            # just collect the image paths (no filters)
            for root, _, files in os.walk(os.path.join(root_dir, data_level)):
                for file in files:
                    image_paths.append(os.path.join(root, file))

        else:
            for root, _, files in os.walk(os.path.join(root_dir, data_level)):
                for file in files:
                    if file.endswith(file_exts):
                        image_paths.append(os.path.join(root, file))
                        
    return image_paths
52/50:
# Preprocessing pipeline
transforms_iam_lines = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.FiveCrop((100, 300)),
    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
])

transforms_iam_words = transforms.Compose([
    transforms.Resize([100, ])
])
52/51:
# Get IAM image paths
dataset_levels = ["lines", "words"] # "lines", "sentences", "words"
file_exts = (".png")
image_paths = []

image_paths = get_image_paths("data/IAM/", dataset_levels, file_exts)

print(len(hw_image_paths))
52/52:
# Get IAM image paths
dataset_levels = ["lines", "words"] # "lines", "sentences", "words"
file_exts = (".png")
image_paths = []

image_paths = get_image_paths("data/IAM/", dataset_levels, file_exts)

print(len(hw_image_paths))
52/53:
# Util function to collect image paths from subdirectories
def get_image_paths(root_dir, data_levels=None, file_exts=None):
    """
    root_dir (str): Root directory to start from
    data_levels (list): Subdirectories to search for
    file_exts (tuple): File extensions to look for
    """
    
    image_paths = []
    
    for data_level in data_levels:
        if(data_level == None or file_exts == None):
            # just collect the image paths (no filters)
            for root, _, files in os.walk(os.path.join(root_dir, data_level)):
                for file in files:
                    image_paths.append(os.path.join(root, file))

        else:
            for root, _, files in os.walk(os.path.join(root_dir, data_level)):
                for file in files:
                    if file.endswith(file_exts):
                        image_paths.append(os.path.join(root, file))
                        
    return image_paths
52/54:
# Preprocessing pipeline
transforms_iam_lines = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.FiveCrop((100, 300)),
    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
])

transforms_iam_words = transforms.Compose([
    transforms.Resize([100, ])
])
52/55:
# Get IAM image paths
dataset_levels = ["lines", "words"] # "lines", "sentences", "words"
file_exts = (".png")
image_paths = []

image_paths = get_image_paths("data/IAM/", dataset_levels, file_exts)

print(len(hw_image_paths))
52/56: len(hw_image_paths)
52/57: len(image_paths)
52/58:
# Define the custom conditional transform
class IAMConditionalTransform(word_or_line:str):
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ])
    ])
    
    def __init__(self, img_paths):
        self.word_or_line = word_or_line
    
    def __call__(self, image):
        if(word_or_line == "line"):
            t = transforms_iam_lines(image)
        elif(word_or_line == "word"):
            t = transforms_iam_words(image)        
    
        return t
52/59:
# Define the custom conditional transform
class IAMConditionalTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ])
    ])
    
    def __init__(self, word_or_line:str):
        self.word_or_line = word_or_line
    
    def __call__(self, image):
        if(word_or_line == "line"):
            t = transforms_iam_lines(image)
        elif(word_or_line == "word"):
            t = transforms_iam_words(image)        
    
        return t
52/60:
# Get IAM image paths
dataset_levels = ["lines", "words"] # "lines", "sentences", "words"
file_exts = (".png")
img_paths = []

img_paths = get_image_paths("data/IAM/", dataset_levels, file_exts)
52/61: len(img_paths)
52/62:
# Define the transforms pipeline
iam_transforms_pipeline = transforms.Compose([
    IAMConditionalTransform(),
    transforms.ToTensor()
])
52/63:
for img_path in img_paths[0:1]:
    count = 0
    
    # Open image
    img = Image.open(img_path)
    conditional = "word" if ("word" in img_path) "line"
    
    # Apply transform
    t = conditional_transforms(conditional, img)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]

        # Save the image
        tensor_to_image.save("data/all_data/"+ img_name + str(count) +'.jpg')
        count += 1
52/64:
for img_path in img_paths[0:1]:
    count = 0
    
    # Open image
    img = Image.open(img_path)
    conditional = "word" if ("word" in img_path) else "line"
    
    # Apply transform
    t = conditional_transforms(conditional, img)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]

        # Save the image
        tensor_to_image.save("data/all_data/"+ img_name + str(count) +'.jpg')
        count += 1
52/65:
for img_path in img_paths[0:1]:
    count = 0
    
    # Open image
    img = Image.open(img_path)
    conditional = "word" if ("word" in img_path) else "line"
    
    # Apply transform
    t = IAMConditionalTransform(conditional, img)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]

        # Save the image
        tensor_to_image.save("data/all_data/"+ img_name + str(count) +'.jpg')
        count += 1
52/66:
# Define the custom conditional transform for IAM images
class IAMConditionalTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.ToTensor()
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(word == True):
            t = transforms_iam_words(image)
        elif(word == "word"):
            t = transforms_iam_lines(image)        
    
        return t
52/67:
for img_path in img_paths[0:1]:
    count = 0
    
    # Open image
    img = Image.open(img_path)
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMConditionalTransform(conditional)(img)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]

        # Save the image
        tensor_to_image.save("data/all_data/"+ img_name + str(count) +'.jpg')
        count += 1
52/68:
# Define the custom conditional transform for IAM images
class IAMConditionalTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.ToTensor()
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(word == True):
            t = transforms_iam_words(image)
        else:
            t = transforms_iam_lines(image)        
    
        return t
52/69:
for img_path in img_paths[0:1]:
    count = 0
    
    # Open image
    img = Image.open(img_path)
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMConditionalTransform(conditional)(img)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]

        # Save the image
        tensor_to_image.save("data/all_data/"+ img_name + str(count) +'.jpg')
        count += 1
52/70:
# Define the custom conditional transform for IAM images
class IAMConditionalTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.ToTensor()
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = transforms_iam_words(image)
        else:
            t = transforms_iam_lines(image)        
    
        return t
52/71:
for img_path in img_paths[0:1]:
    count = 0
    
    # Open image
    img = Image.open(img_path)
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMConditionalTransform(conditional)(img)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]

        # Save the image
        tensor_to_image.save("data/all_data/"+ img_name + str(count) +'.jpg')
        count += 1
52/72:
# Get image paths
root_dir = "data/IAM/"
dataset_levels = ["lines", "words"] # "lines", "sentences", "words"
file_exts = (".png")
img_paths = []

img_paths = get_image_paths(root_dir, dataset_levels, file_exts)
52/73: len(img_paths)
52/74:
# Get image paths
root_dir = "data/IAM/online"
dataset_levels = ["lineImages"]
file_exts = (".tif")
online_img_paths = []

online_img_paths = get_image_paths(root_dir, dataset_levels)
52/75: len(online_img_paths)
52/76: online_img_paths[0]
52/77: img_paths[0]
52/78:
# Define the custom conditional transform for IAM images
class IAMTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.ToTensor()
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = transforms_iam_words(image)
        else:
            t = transforms_iam_lines(image)        
    
        return t
51/197:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
51/198:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/199:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/200:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/201:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.FiveCrop((128, 250)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/202:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/203:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/204:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.FiveCrop((128, 250)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/205:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/206:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/207:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.FiveCrop((128, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/208:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.FiveCrop((128, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/209:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.FiveCrop((128, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/210:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/211:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/212:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CentreCrop((128, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/213:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/214:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/215:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/216:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/217:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/218: test_correct/len(test_loader.dataset)
51/219: torch.save(model.state_dict(), 'model/vgg16_best_model_0-82_128x300.pth')
52/79:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMTransform(conditional)(img)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten"+ img_name + str(count) +'.jpg')
        count += 1
52/80:
# Define the custom conditional transform for IAM images
class IAMTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.ToTensor()
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = transforms_iam_words(image)
        else:
            try:   
                t = transforms_iam_lines(image)
            except ValueError:
                t = transforms_iam_words(image)
    
        return t
52/81:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMTransform(conditional)(img)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/82:
# Define the custom conditional transform for IAM images
class IAMTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.ToTensor()
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = [transforms_iam_words(image)]
        else:
            try:   
                t = transforms_iam_lines(image)
            except ValueError:
                t = [transforms_iam_words(image)]
                
    
        return t
52/83:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMTransform(conditional)(img)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/84:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMTransform(conditional)(img)
    
    print(img_path)
    img.show()
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/85:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMTransform(conditional)(img)
    
    print(img_path)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/86:
# Define the custom conditional transform for IAM images
class IAMTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.ToTensor()
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = transforms_iam_words(image)
        else:
            print("here")
            try:   
                t = transforms_iam_lines(image)
            except ValueError:
                t = transforms_iam_words(image)
                
    
        return t
52/87:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMTransform(conditional)(img)
    
    print(img_path)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/88:
# Define the custom conditional transform for IAM images
class IAMTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.ToTensor()
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = transforms_iam_words(image)
        else:
            print("here")
            try:   
                t = transforms_iam_lines(image)
            except ValueError:
                print("in except")
                t = transforms_iam_words(image)
                
    
        return t
52/89:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMTransform(conditional)(img)
    
    print(img_path)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/90:
# Define the custom conditional transform for IAM images
class IAMTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
#         transforms.ToTensor()
        transforms.Lambda(lambda img: torch.stack([transforms.ToTensor()(img)]))
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = transforms_iam_words(image)
        else:
            print("here")
            try:   
                t = transforms_iam_lines(image)
            except ValueError:
                print("in except")
                t = transforms_iam_words(image)
                
    
        return t
52/91:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMTransform(conditional)(img)
    
    print(img_path)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/92:
# Define the custom conditional transform for IAM images
class IAMTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
#         transforms.ToTensor()
        transforms.Lambda(lambda img: torch.stack([transforms.ToTensor()(img)]))
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = transforms_iam_words(image)
        else:
            print("here")
            try:   
                t = transforms_iam_lines(image)
            except ValueError:
                print("in except")
                t = transforms_iam_words(image)
                print(type(t))
                
    
        return t
52/93:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMTransform(conditional)(img)
    
    print(img_path)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/94:
# Define the custom conditional transform for IAM images
class IAMTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
#         transforms.ToTensor()
        transforms.Lambda(lambda img: torch.stack([transforms.ToTensor()(img)]))
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = transforms_iam_words(image)
        else:
            print("here")
            try:   
                t = transforms_iam_lines(image)
                print(type(t))
            except ValueError:
                print("in except")
                t = transforms_iam_words(image)
                print(type(t))
                
    
        return t
52/95:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMTransform(conditional)(img)
    
    print(img_path)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/96:
# Define the custom conditional transform for IAM images
class IAMTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.ToTensor(),
        transforms.Lambda(lambda img: torch.stack([transforms.ToTensor()(img)]))
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = transforms_iam_words(image)
        else:
            print("here")
            try:   
                t = transforms_iam_lines(image)
                print(type(t))
            except ValueError:
                print("in except")
                t = transforms_iam_words(image)
                print(type(t))
                
    
        return t
52/97:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMTransform(conditional)(img)
    
    print(img_path)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/98:
# Define the custom conditional transform for IAM images
class IAMTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
#         transforms.ToTensor(),
        transforms.Lambda(lambda imgs: torch.stack([transforms.ToTensor()(img) for img in imgs]))
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = transforms_iam_words(image)
        else:
            print("here")
            try:   
                t = transforms_iam_lines(image)
                print(type(t))
            except ValueError:
                print("in except")
                t = transforms_iam_words(image)
                print(type(t))
                
    
        return t
52/99:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMTransform(conditional)(img)
    
    print(img_path)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/100:
# Define the custom conditional transform for IAM images
class IAMTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.ToTensor().unsqeeze(0),
#         transforms.Lambda(lambda imgs: torch.stack([transforms.ToTensor()(img) for img in imgs]))
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = transforms_iam_words(image)
        else:
            print("here")
            try:   
                t = transforms_iam_lines(image)
                print(type(t))
            except ValueError:
                print("in except")
                t = transforms_iam_words(image)
                print(type(t))
                
    
        return t
52/101:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMTransform(conditional)(img)
    
    print(img_path)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/102:
# Define the custom conditional transform for IAM images
class IAMTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.ToTensor().unsqeeze(0),
        transforms.Lambda(lambda imgs: torch.stack([transforms.ToTensor()(img) for img in imgs]))
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = transforms_iam_words(image)
        else:
            print("here")
            try:   
                t = transforms_iam_lines(image)
                print(type(t))
            except ValueError:
                print("in except")
                t = transforms_iam_words(image)
                print(type(t))
                
    
        return t
52/103:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMTransform(conditional)(img)
    
    print(img_path)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/104:
# Define the custom conditional transform for IAM images
class IAMTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.ToTensor().unsqeeze(0),
        transforms.Lambda(lambda imgs: torch.stack([transforms.ToTensor()(img) for img in imgs]))
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = transforms_iam_words(image)
        else:
            try:   
                t = transforms_iam_lines(image)
            except ValueError:
                t = transforms_iam_words(image)
    
        return t
52/105:
# Define the custom conditional transform for IAM images
class IAMTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.ToTensor().unsqeeze(0),
        transforms.Lambda(lambda imgs: torch.stack([transforms.ToTensor()(img) for img in imgs]))
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = transforms_iam_words(image)
        else:
            try:   
                t = transforms_iam_lines(image)
            except ValueError:
                t = transforms_iam_words(image)
    
        return t
52/106:
# Define the custom conditional transform for IAM images
class IAMTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.ToTensor(),
        transforms.Lambda(lambda imgs: torch.stack([transforms.ToTensor()(img) for img in imgs]))
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = transforms_iam_words(image)
        else:
            try:   
                t = transforms_iam_lines(image)
            except ValueError:
                t = transforms_iam_words(image)
    
        return t
52/107:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = IAMTransform(conditional)(img)
    
    print(img_path)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/108:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = torch.Tensor(IAMTransform(conditional)(img))
    
    print(img_path)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/109:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = transforms.ToTensor(IAMTransform(conditional)(img))
    
    print(img_path)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/110:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    # Apply transform
    t = transforms.ToTensor()(IAMTransform(conditional)(img))
    
    print(img_path)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/111:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    try:
        # Apply transform
        t = IAMTransform(conditional)(img)
    except ValueError:
        t = IAMTransform(True)(img)
    
#     print(img_path)
        
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
52/112:
# Define the custom conditional transform for IAM images
class IAMTransform:
    
    # Preprocessing pipeline
    transforms_iam_lines = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))
    ])

    transforms_iam_words = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.ToTensor()
    ])
    
    def __init__(self, word:bool):
        self.word = word
    
    def __call__(self, image):
        if(self.word == True):
            t = IAMTransform.transforms_iam_words(image)
        else: 
            t = IAMTransform.transforms_iam_lines(image)
    
        return t
52/113:
for img_path in img_paths:
    count = 0
    # Open image
    img = Image.open(img_path)
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    try:
        # Apply transform
        t = IAMTransform(conditional)(img)
    except ValueError:
        t = IAMTransform(True)(img)
            
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save("data/all_preprocessed_data/handwritten/"+ img_name + str(count) +'.jpg')
        count += 1
51/220:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.CenterCrop((128, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/221:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/222:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/223:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/224:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/225: test_correct/len(test_loader.dataset)
51/226:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.RandomCrop((128, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/227:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/228:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/229:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/230:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/231: test_correct/len(test_loader.dataset)
51/232: y_true_cpu, y_pred_cpu
51/233:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, int(len(test_loader.dataset)/4), figsize=(12, 3))
    for i in range(4):
        for j in range(int(len(test_loader.dataset)/4)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            img_no += 1
    plt.show()
52/114: os.path(output_dir)
52/115:
output_dir = "data/all_preprocessed_data/handwritten/"
os.path(output_dir)
52/116:
output_dir = "data/all_preprocessed_data/handwritten/"

for img_path in img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
        
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    try:
        # Apply transform
        t = IAMTransform(conditional)(img)
    except ValueError:
        t = IAMTransform(True)(img)
            
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save(os.path(output_dir)+ img_name + str(count) +'.jpg')
        count += 1
52/117:
output_dir = "data/all_preprocessed_data/handwritten/"

for img_path in img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
        
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    try:
        # Apply transform
        t = IAMTransform(conditional)(img)
    except ValueError:
        t = IAMTransform(True)(img)
            
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        # Save the image
        tensor_to_image.save(output_dir + img_name + str(count) +'.jpg')
        count += 1
52/118:
output_dir = "data/all_preprocessed_data/handwritten/"

for img_path in img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
        
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    try:
        # Apply transform
        t = IAMTransform(conditional)(img)
    except ValueError:
        t = IAMTransform(True)(img)
            
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save()
        count += 1
52/119:
import os
import json
import re
import sys
import shutil

from PIL import Image, UnidentifiedImageError
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine
import torchvision.transforms.functional as TF

import magic

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
52/120:
import os
import json
import re
import sys
import shutil

from PIL import Image, UnidentifiedImageError
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine
import torchvision.transforms.functional as TF

import magic

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
52/121:
output_dir = "data/all_preprocessed_data/handwritten/"

for img_path in img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
        
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    try:
        # Apply transform
        t = IAMTransform(conditional)(img)
    except ValueError:
        t = IAMTransform(True)(img)
            
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save()
        count += 1
52/122:
output_dir = "data/all_preprocessed_data/handwritten/"

for img_path in img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
        
    # Define conditional
    conditional = True if ("word" in img_path) else False
    
    try:
        # Apply transform
        t = IAMTransform(conditional)(img)
    except ValueError:
        t = IAMTransform(True)(img)
            
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
52/123:
# Get image paths
root_dir = "data/IAM/online/"
dataset_levels = ["lineImages"]
file_exts = (".tif")
online_img_paths = []

online_img_paths = get_image_paths(root_dir, dataset_levels, file_exts)
52/124: len(online_img_paths)
52/125: online_img_paths[0]
52/126:
# Define the custom conditional transform for IAM online images
class IAMOnlineTransform:
    
    # Preprocessing pipeline
    transforms_iam_online = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((256, 400)),
        # returns a 4D tensor of list of cropped images
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])) 
    ])

    def __init__(self):
        pass
    
    def __call__(self, image):
        t = IAMOnlineTransform.transforms_iam_online(image) 
    
        return t
52/127:
output_dir = "data/all_data/handwritten/"

for img_path in online_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    t = transforms_iam_online()(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
52/128:
output_dir = "data/all_data/handwritten/"

for img_path in online_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    t = IAMOnlineTransform()(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
52/129:
# Define the custom conditional transform for IAM online images
class IAMOnlineTransform:
    
    # Preprocessing pipeline
    transforms_iam_online = transforms.Compose([
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        # returns a 4D tensor of list of cropped images
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])) 
    ])

    def __init__(self):
        pass
    
    def __call__(self, image):
        t = IAMOnlineTransform.transforms_iam_online(image) 
    
        return t
52/130:
output_dir = "data/all_data/handwritten/"

for img_path in online_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    t = IAMOnlineTransform()(img)
    
#     # Apply transform
#     try:
#         t = IAMTransform(conditional)(img)
#     except ValueError:
#         t = IAMTransform(True)(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
52/131:
output_dir = "data/all_data/handwritten/"

for img_path in online_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = IAMOnlineTransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
52/132:
output_dir = "data/all_preprocessed_data/handwritten/"

for img_path in online_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = IAMOnlineTransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
52/133:
# Collect image paths to preprocess
# Get image paths
root_dir = "data/IAM/washingtondb-v1.0/"
dataset_levels = None
file_exts = (".jpg")
washington_img_paths = []

washington_img_paths = get_image_paths(root_dir, dataset_levels, file_exts)

# for root, dirs, files in os.walk(folder):
#     for file in files:
#         if file in [".DS_Store"]:
#             continue
#         washington_img_paths.append(os.path.join(root, file))
52/134:
# Util function to collect image paths from subdirectories
def get_image_paths(root_dir, data_levels=None, file_exts=None):
    """
    root_dir (str): Root directory to start from
    data_levels (list): Subdirectories to search for
    file_exts (tuple): File extensions to look for
    """
    
    image_paths = []
    
    if(data_level == None or file_exts == None):
        # just collect the image paths (no filters)
        for root, _, files in os.walk(os.path.join(root_dir, data_level)):
            for file in files:
                image_paths.append(os.path.join(root, file))
        
    else:
        for data_level in data_levels:
            for root, _, files in os.walk(os.path.join(root_dir, data_level)):
                for file in files:
                    if file.endswith(file_exts):
                        image_paths.append(os.path.join(root, file))
                        
    return image_paths
52/135:
# Collect image paths to preprocess
# Get image paths
root_dir = "data/IAM/washingtondb-v1.0/"
dataset_levels = None
file_exts = (".jpg")
washington_img_paths = []

washington_img_paths = get_image_paths(root_dir, dataset_levels, file_exts)

# for root, dirs, files in os.walk(folder):
#     for file in files:
#         if file in [".DS_Store"]:
#             continue
#         washington_img_paths.append(os.path.join(root, file))
52/136:
# Util function to collect image paths from subdirectories
def get_image_paths(root_dir, data_levels=None, file_exts=None):
    """
    root_dir (str): Root directory to start from
    data_levels (list): Subdirectories to search for
    file_exts (tuple): File extensions to look for
    """
    
    image_paths = []
    
    if(data_levels == None or file_exts == None):
        # just collect the image paths (no filters)
        for root, _, files in os.walk(os.path.join(root_dir, data_level)):
            for file in files:
                image_paths.append(os.path.join(root, file))
        
    else:
        for data_level in data_levels:
            for root, _, files in os.walk(os.path.join(root_dir, data_level)):
                for file in files:
                    if file.endswith(file_exts):
                        image_paths.append(os.path.join(root, file))
                        
    return image_paths
52/137:
# Get image paths
root_dir = "data/IAM/washingtondb-v1.0/"
dataset_levels = None
file_exts = (".jpg")
washington_img_paths = []

washington_img_paths = get_image_paths(root_dir, dataset_levels, file_exts)

# for root, dirs, files in os.walk(folder):
#     for file in files:
#         if file in [".DS_Store"]:
#             continue
#         washington_img_paths.append(os.path.join(root, file))
52/138:
# Util function to collect image paths from subdirectories
def get_image_paths(root_dir, data_levels=None, file_exts=None):
    """
    root_dir (str): Root directory to start from
    data_levels (list): Subdirectories to search for
    file_exts (tuple): File extensions to look for
    """
    
    image_paths = []
    
    if(data_levels == None or file_exts == None):
        # just collect the image paths (no filters)
        for root, _, files in os.walk(os.path.join(root_dir)):
            for file in files:
                image_paths.append(os.path.join(root, file))
        
    else:
        for data_level in data_levels:
            for root, _, files in os.walk(os.path.join(root_dir, data_level)):
                for file in files:
                    if file.endswith(file_exts):
                        image_paths.append(os.path.join(root, file))
                        
    return image_paths
52/139:
# Get image paths
root_dir = "data/IAM/washingtondb-v1.0/"
dataset_levels = None
file_exts = (".jpg")
washington_img_paths = []

washington_img_paths = get_image_paths(root_dir, dataset_levels, file_exts)

# for root, dirs, files in os.walk(folder):
#     for file in files:
#         if file in [".DS_Store"]:
#             continue
#         washington_img_paths.append(os.path.join(root, file))
52/140: len(washington_img_paths)
52/141:
# Get image paths
root_dir = "data/IAM/washingtondb-v1.0/"
dataset_levels = None
file_exts = (".jpg")
washington_img_paths = []

washington_img_paths = get_image_paths(root_dir, dataset_levels, file_exts)
52/142: len(washington_img_paths)
52/143: washington_img_paths[0]
52/144:
# Util function to collect image paths from subdirectories
def get_image_paths(root_dir, data_levels=None, file_exts=None):
    """
    root_dir (str): Root directory to start from
    data_levels (list): Subdirectories to search for
    file_exts (tuple): File extensions to look for
    """
    
    image_paths = []
    
    if(data_levels == None or file_exts == None):
        # just collect the image paths (no filters)
        for root, _, files in os.walk(os.path.join(root_dir)):
            for file in files:
                if file.endswith(file_exts):
                    image_paths.append(os.path.join(root, file))
        
    else:
        for data_level in data_levels:
            for root, _, files in os.walk(os.path.join(root_dir, data_level)):
                for file in files:
                    if file.endswith(file_exts):
                        image_paths.append(os.path.join(root, file))
                        
    return image_paths
52/145:
# Get image paths
root_dir = "data/IAM/washingtondb-v1.0/"
dataset_levels = None
file_exts = (".jpg")
washington_img_paths = []

washington_img_paths = get_image_paths(root_dir, dataset_levels, file_exts)
52/146: len(washington_img_paths)
52/147:
# Get image paths
root_dir = "data/IAM/washingtondb-v1.0/"
dataset_levels = None
file_exts = (".png")
washington_img_paths = []

washington_img_paths = get_image_paths(root_dir, dataset_levels, file_exts)
52/148:
# Get image paths
root_dir = "data/IAM/washingtondb-v1.0/"
dataset_levels = None
file_exts = (".png")
washington_img_paths = []

washington_img_paths = get_image_paths(root_dir, dataset_levels, file_exts)
52/149:
# Get image paths
root_dir = "data/IAM/washingtondb-v1.0/"
dataset_levels = None
file_exts = (".png")
washington_img_paths = []

washington_img_paths = get_image_paths(root_dir, dataset_levels, file_exts)
52/150: len(washington_img_paths)
52/151: washington_img_paths[0]
52/152:
# All images have a width of 120 pixels.

class IAMWashingtonTranform:
    
    # Define the custom transforms pipeline for line images
    transforms_washington_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
    ])
    
    # Define the custom transforms pipeline for word images
    transforms_washington_words = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
#         transforms.Pad([120, 68, 120, 68]),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        width, height = image.size
        
        if(width >= img_size_threshold[1] and height >= img_size_threshold[0]):
            t = transforms_washington_lines(image)
        else:
            t = transforms_washington_words(image)
        
        return t
52/153:
# All images have a width of 120 pixels.

class IAMWashingtonTranform:
    
    # Define the custom transforms pipeline for line images
    transforms_washington_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
    ])
    
    # Define the custom transforms pipeline for word images
    transforms_washington_words = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
#         transforms.Pad([120, 68, 120, 68]),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self, dim_thresh):
        self.dim_thresh = dim_thresh
    
    def __call__(self, image):
        width, height = image.size
        
        if(width >= dim_thresh[1] and height >= dim_thresh[0]):
            t = transforms_washington_lines(image)
        else:
            t = transforms_washington_words(image)
        
        return t
52/154:
for img_path in washington_img_paths:
    IAMWashingtonTranform()(dim_thresh=(120, 600))
52/155:
for img_path in washington_img_paths:
    IAMWashingtonTranform((120, 600))
52/156:
for img_path in washington_img_paths:
    IAMWashingtonTranform((120, 600))

output_dir = "data/all_preprocessed_data/handwritten/"

for img_path in washington_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = IAMWashingtonTranform((120, 600))(img)
    except ValueError:
        t = IAMTransform(True)(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
52/157:
# All images have a width of 120 pixels.

class IAMWashingtonTranform:
    
    # Define the custom transforms pipeline for line images
    transforms_washington_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
    ])
    
    # Define the custom transforms pipeline for word images
    transforms_washington_words = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
#         transforms.Pad([120, 68, 120, 68]),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self, dim_thresh:tuple):
        self.dim_thresh = dim_thresh
    
    def __call__(self, image):
        width, height = image.size
        
        if(width >= dim_thresh[1] and height >= dim_thresh[0]):
            t = transforms_washington_lines(image)
        else:
            t = transforms_washington_words(image)
        
        return t
52/158:
output_dir = "data/all_preprocessed_data/handwritten/"

for img_path in washington_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = IAMWashingtonTranform((120, 600))(img)
    except ValueError:
        t = IAMTransform(True)(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
52/159:
output_dir = "data/all_preprocessed_data/handwritten/"

for img_path in washington_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = IAMWashingtonTranform((120, 600))(img)
    except ValueError:
        t = IAMTransform(True)(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
52/160:
# All images have a width of 120 pixels.

class IAMWashingtonTranform:
    
    # Define the custom transforms pipeline for line images
    transforms_washington_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
    ])
    
    # Define the custom transforms pipeline for word images
    transforms_washington_words = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
#         transforms.Pad([120, 68, 120, 68]),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self, dim_thresh:tuple):
        self.dim_thresh = dim_thresh
    
    def __call__(self, image):
        width, height = image.size
        
        if(width >= self.dim_thresh[1] and height >= self.dim_thresh[0]):
            t = transforms_washington_lines(image)
        else:
            t = transforms_washington_words(image)
        
        return t
52/161:
# All images have a width of 120 pixels.

class IAMWashingtonTranform:
    
    # Define the custom transforms pipeline for line images
    transforms_washington_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
    ])
    
    # Define the custom transforms pipeline for word images
    transforms_washington_words = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
#         transforms.Pad([120, 68, 120, 68]),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self, dim_thresh:tuple):
        self.dim_thresh = dim_thresh
    
    def __call__(self, image):
        width, height = image.size
        
        if(width >= self.dim_thresh[1] and height >= self.dim_thresh[0]):
            t = transforms_washington_lines(image)
        else:
            t = transforms_washington_words(image)
        
        return t
52/162:
output_dir = "data/all_preprocessed_data/handwritten/"

for img_path in washington_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = IAMWashingtonTranform((120, 600))(img)
    except ValueError:
        t = IAMTransform(True)(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
52/163:
# All images have a width of 120 pixels.

class IAMWashingtonTranform:
    
    # Define the custom transforms pipeline for line images
    transforms_washington_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
    ])
    
    # Define the custom transforms pipeline for word images
    transforms_washington_words = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
#         transforms.Pad([120, 68, 120, 68]),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self, dim_thresh:tuple):
        self.dim_thresh = dim_thresh
    
    def __call__(self, image):
        width, height = image.size
        
        if(width >= self.dim_thresh[1] and height >= self.dim_thresh[0]):
            t = IAMWashingtonTranform.transforms_washington_lines(image)
        else:
            t = IAMWashingtonTranform.transforms_washington_words(image)
        
        return t
52/164:
output_dir = "data/all_preprocessed_data/handwritten/"

for img_path in washington_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = IAMWashingtonTranform((120, 600))(img)
    except ValueError:
        t = IAMTransform(True)(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
51/234:
dataset_path = "data/all_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
51/235:
dataset_path = "data/all_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
51/236: ! ls data/all_data/handwritten | wc -l
51/237: ! ls data/all_data/typed/ | wc -l
51/238:
# Define the percentage of samples for the validation set
val_split = 0.1  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
51/239:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
51/240:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 8

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            pass
#             counter += 1
#             # Check if the counter reaches the patience limit
#             if counter >= patience:
#                 print('Early stopping triggered...')
#             break

print("\n=== Training complete! ===\n")
51/241:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            pass
#             counter += 1
#             # Check if the counter reaches the patience limit
#             if counter >= patience:
#                 print('Early stopping triggered...')
#             break

print("\n=== Training complete! ===\n")
51/242:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
51/243:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.RandomCrop((128, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/244:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/245:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/246:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/247:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/248: test_correct/len(test_loader.dataset)
51/249: y_true_cpu, y_pred_cpu
51/250:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, int(len(test_loader.dataset)/4), figsize=(12, 3))
    for i in range(4):
        for j in range(int(len(test_loader.dataset)/4)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            img_no += 1
    plt.show()
51/251:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/252:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
51/253:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
51/254:
# 
test_preprocess = transforms.Compose([
    transforms.Resize((128, 300)),
    transforms.RandomCrop((128, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
51/255:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
51/256:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
51/257:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
51/258:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
51/259: test_correct/len(test_loader.dataset)
51/260: y_true_cpu, y_pred_cpu
51/261:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, int(len(test_loader.dataset)/4), figsize=(12, 3))
    for i in range(4):
        for j in range(int(len(test_loader.dataset)/4)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            img_no += 1
    plt.show()
55/1:
import os
import json
import re
import sys
import shutil

from PIL import Image, UnidentifiedImageError
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine
import torchvision.transforms.functional as TF

import magic

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
55/2:
import os
import json
import re
import sys
import shutil

from PIL import Image, UnidentifiedImageError
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine
import torchvision.transforms.functional as TF

import magic

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
55/3:
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
55/4:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):
    
    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["other"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        cropped_image.show()
                        
#                         # append bounding box id to the cropped image name
#                         output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
#                         # Save the cropped image
#                         cropped_image.save(output_path, "PNG")
55/5:
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
55/6:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):
    
    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["other"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        plt.imshow(cropped_image)
                        
#                         # append bounding box id to the cropped image name
#                         output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
#                         # Save the cropped image
#                         cropped_image.save(output_path, "PNG")
55/7:
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
55/8:
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
55/9:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):
    
    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["other"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
                        # Save the cropped image
                        cropped_image.save(output_path, "PNG")
55/10:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):
    
    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["other"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
                        # Save the cropped image
                        cropped_image.save(output_path, "PNG")
55/11:
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
55/12:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):
    
    img_sizes = []
    
    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["other"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        img_sizes.append(cropped_image.size())
                        
#                         # append bounding box id to the cropped image name
#                         output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
#                         # Save the cropped image
#                         cropped_image.save(output_path, "PNG")
    return img_sizes
55/13:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):
    
    img_sizes = []
    
    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["other"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        img_sizes.append(cropped_image.size())
                        
#                         # append bounding box id to the cropped image name
#                         output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
#                         # Save the cropped image
#                         cropped_image.save(output_path, "PNG")
    return img_sizes
55/14:
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

temp = crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
55/15:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):
    
    img_sizes = []
    
    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["other"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        img_sizes.append(cropped_image.size
                        
#                         # append bounding box id to the cropped image name
#                         output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
#                         # Save the cropped image
#                         cropped_image.save(output_path, "PNG")
    return img_sizes
55/16:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):
    
    img_sizes = []
    
    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["other"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        img_sizes.append(cropped_image.size)
                        
#                         # append bounding box id to the cropped image name
#                         output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
#                         # Save the cropped image
#                         cropped_image.save(output_path, "PNG")
    return img_sizes
55/17:
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/all_data/typed'

temp = crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
55/18: temp[0:10]
55/19: len(temp)
55/20:
def calculate_aspect_ratios(directory):
    aspect_ratios = []
    for filename in os.listdir(directory):
        with Image.open(os.path.join(directory, filename)) as img:
            width, height = img.size
            aspect_ratios.append(width / height)
    return aspect_ratios

directory = 'data/all_data/typed'  # Replace with your image directory
aspect_ratios = calculate_aspect_ratios(directory)

# Calculate average aspect ratio
average_aspect_ratio = np.mean(aspect_ratios)

# Calculate median aspect ratio
median_aspect_ratio = np.median(aspect_ratios)

# Plot the distribution of aspect ratios
plt.hist(aspect_ratios, bins=50)
plt.xlabel('Aspect Ratio (Width/Height)')
plt.ylabel('Number of Images')
plt.show()

print(f"Average aspect ratio: {average_aspect_ratio}")
print(f"Median aspect ratio: {median_aspect_ratio}")
55/21: np.mean(temp[0:10])
55/22: np.mean(temp[0:10], axis=1)
55/23: np.mean(temp[0:10], axis=0)
55/24:
np.mean(temp, axis=0)

# Looking at the below
55/25: np.median(temp, axis=0)
55/26:
image_folder = 'data/FUNSD/dataset/testing_data/images'
annotation_folder = 'data/FUNSD/dataset/testing_data/annotations'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
55/27:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):
    
    img_sizes = []
    
    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["other"]:
                        # Crop the image using the bounding box coordinates
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
#                         img_sizes.append(cropped_image.size)
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
                        # Save the cropped image
                        cropped_image.save(output_path, "PNG")
#     return img_sizes
55/28:
image_folder = 'data/FUNSD/dataset/testing_data/images'
annotation_folder = 'data/FUNSD/dataset/testing_data/annotations'
output_folder = 'data/all_data/typed'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
55/29:
import ipywidgets as widgets
from IPython.display import display, Image, clear_output

def view_images(image_paths_list):
    
    num_images = len(image_paths_list)
    image_index = 0
    
    def display_image(image_index):
        image_path = image_paths_list[image_index]
        with open(image_path, 'rb') as file:
            image_widget.value = file.read()
    
    image_widget = widgets.Image(format='png')
    
    # Image navigation buttons
    prev_button = widgets.Button(description='Previous')
    next_button = widgets.Button(description='Next')
    
    def on_prev_button_clicked(b):
        nonlocal image_index
        image_index = (image_index - 1) % num_images
        display_image(image_index)
    
    def on_next_button_clicked(b):
        nonlocal image_index
        image_index = (image_index + 1) % num_images
        display_image(image_index)
    
    display(image_widget)
    
    # Initial display
    display_image(image_index)
    
    prev_button.on_click(on_prev_button_clicked)
    next_button.on_click(on_next_button_clicked)
    
    # Display navigation buttons
    display(widgets.HBox([prev_button, next_button]))
55/30:
import ipywidgets as widgets
from IPython.display import display, Image, clear_output

def view_images(image_paths_list):
    
    num_images = len(image_paths_list)
    image_index = 0
    
    def display_image(image_index):
        image_path = image_paths_list[image_index]
        with open(image_path, 'rb') as file:
            image_widget.value = file.read()
    
    image_widget = widgets.Image(format='png')
    
    # Image navigation buttons
    prev_button = widgets.Button(description='Previous')
    next_button = widgets.Button(description='Next')
    
    def on_prev_button_clicked(b):
        nonlocal image_index
        image_index = (image_index - 1) % num_images
        display_image(image_index)
    
    def on_next_button_clicked(b):
        nonlocal image_index
        image_index = (image_index + 1) % num_images
        display_image(image_index)
    
    display(image_widget)
    
    # Initial display
    display_image(image_index)
    
    prev_button.on_click(on_prev_button_clicked)
    next_button.on_click(on_next_button_clicked)
    
    # Display navigation buttons
    display(widgets.HBox([prev_button, next_button]))
55/31:
funsd_img_paths = os.listdir("data/all_data/typed/")

view_images(funsd_img_paths)
55/32:
funsd_img_paths = os.path.join("data/all_data/typed/", os.listdir("data/all_data/typed/"))

view_images(funsd_img_paths)
55/33:
funsd_img_paths = []

for file_name in os.listdir("data/all_data/typed/"):
    funsd_img_paths.append(os.path.join("data/all_data/typed/", file_name))

view_images(funsd_img_paths)
55/34:
# Transform to preprocess images
class FUNSDTransform:
    
    transforms_FUNSD = transforms.Compose([
        transforms.Resize([])
    ])
    
    def __init__(self):
        pass
    
    def __call__(self):
        pass
55/35:
# Transform to preprocess images
class FUNSDTransform:
    
    transforms_FUNSD = transforms.Compose([
        transforms.Resize([50, ])
    ])
    
    def __init__(self):
        pass
    
    def __call__(self):
        pass
55/36:
# Transform to preprocess images
class FUNSDTransform:
    
    transforms_FUNSD = transforms.Compose([
        transforms.Resize([50, ])
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        t = FUNSDTransform.transforms_FUNSD()(image)
        
        return t
55/37: ! mkdir data/FUNSD/temp
55/38:
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/FUNSD/temp/'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
55/39:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):
    
    img_sizes = []
    
    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["other"]:
                        # Crop the image using the bounding box coordinates (the coordinate system is Cartesian)
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        # img_sizes.append(cropped_image.size)
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
                        # Save the cropped image
                        cropped_image.save(output_path, "PNG")
    # return img_sizes
55/40:
def crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder):
    
    img_sizes = []
    
    # Iterate over files in the annotations folder
    for annotation_file in os.listdir(annotation_folder):
        annotation_path = os.path.join(annotation_folder, annotation_file)
        
        image_file = os.path.splitext(annotation_file)[0] + ".png"
        image_path = os.path.join(image_folder, image_file)        
        
        # Open the image
        image = Image.open(image_path)
        
        if os.path.isfile(annotation_path):
            # Load bounding box data from JSON file
            with open(annotation_path, 'r') as f:
                raw = json.load(f)
                
                for item in raw["form"]:
                    box = item["box"]
                    label = item["label"]
                    idx = item["id"]
                    
                    if label not in ["other"]:
                        # Crop the image using the bounding box coordinates (the coordinate system is Cartesian)
                        cropped_image = image.crop((box[0], box[1],
                                                    box[2], box[3]))
                        
                        # img_sizes.append(cropped_image.size)
                        
                        # append bounding box id to the cropped image name
                        output_path = os.path.join(output_folder, image_file[:-4]+"-"+str(idx)+".png")
                        # Save the cropped image
                        cropped_image.save(output_path, "PNG")
    # return img_sizes
55/41:
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/FUNSD/temp/'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
55/42:
import os
import json
import re
import sys
import shutil

from PIL import Image, UnidentifiedImageError
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine
import torchvision.transforms.functional as TF

import magic

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
55/43:
image_folder = 'data/FUNSD/dataset/training_data/images'
annotation_folder = 'data/FUNSD/dataset/training_data/annotations'
output_folder = 'data/FUNSD/temp'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
55/44: ! ls data/FUNSD/ | wc -l
55/45: ! ls data/FUNSD/temp | wc -l
55/46: ! ls data/FUNSD/temp | wc -l
55/47:
image_folder = 'data/FUNSD/dataset/testing_data/images'
annotation_folder = 'data/FUNSD/dataset/testing_data/annotations'
output_folder = 'data/FUNSD/temp'

crop_images_with_bounding_boxes(image_folder, annotation_folder, output_folder)
55/48: ! ls data/FUNSD/temp | wc -l
55/49:
funsd_img_paths = []

for file_name in os.listdir("data/FUNSD/temp"):
    funsd_img_paths.append(os.path.join("data/FUNSD/temp", file_name))

view_images(funsd_img_paths)
55/50:
# Transform to preprocess images
class FUNSDTransform:
    
    transforms_FUNSD = transforms.Compose([
        transforms.Resize([50, ])
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        t = FUNSDTransform.transforms_FUNSD(image)
        
        return t
55/51:
import os
import json
import re
import sys
import shutil

from PIL import Image, UnidentifiedImageError
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine, adjust_sharpness
import torchvision.transforms.functional as TF

import magic

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
55/52:
# Transform to preprocess images
class FUNSDTransform:
    
    transforms_FUNSD = transforms.Compose([
        transforms.Resize([50, ]),
        adjust_sharpness(sharpness_factor=2)
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        t = FUNSDTransform.transforms_FUNSD(image)
        
        return t
55/53:
# Transform to preprocess images
class FUNSDTransform:
    
    transforms_FUNSD = transforms.Compose([
        transforms.Resize([50, ]),
        adjust_sharpness(img, sharpness_factor=2)
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        t = FUNSDTransform.transforms_FUNSD(image)
        
        return t
55/54:
# Transform to preprocess images
class FUNSDTransform:
    
    transforms_FUNSD = transforms.Compose([
        transforms.Resize([50, ]),
        transforms.Lambda(lambda img: adjust_sharpness(img, sharpness_factor=2.0))
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        t = FUNSDTransform.transforms_FUNSD(image)
        
        return t
55/55:
output_dir = "data/all_preprocessed_data/typed/"

for img_path in funsd_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = FUNSDTransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + "FUNSD" + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
55/56:
# Transform to preprocess images
class FUNSDTransform:
    
    transforms_FUNSD = transforms.Compose([
        transforms.Resize([50, ]),
        transforms.Lambda(lambda img: adjust_sharpness(img, sharpness_factor=2.0)),
        transforms.ToTensor()
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        t = FUNSDTransform.transforms_FUNSD(image)
        
        return t
55/57:
output_dir = "data/all_preprocessed_data/typed/"

for img_path in funsd_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = FUNSDTransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + "FUNSD" + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
55/58: ! ls data/all_preprocessed_data/typed/ | wc -l
55/59:
temp_img_paths = []

for file_name in os.listdir("data/all_preprocessed_data/typed"):
    temp_img_paths.append(os.path.join("data/all_preprocessed_data/typed", file_name))

view_images(temp_img_paths)
55/60:
# Transform to preprocess images
class FUNSDTransform:
    
    transforms_FUNSD = transforms.Compose([
        transforms.Resize([50, ]),
        transforms.Lambda(lambda img: adjust_sharpness(img, sharpness_factor=5.0)),
        transforms.ToTensor()
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        t = FUNSDTransform.transforms_FUNSD(image)
        
        return t
55/61:
output_dir = "data/all_preprocessed_data/typed/"

for img_path in funsd_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = FUNSDTransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + "FUNSD" + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
55/62: ! ls data/all_preprocessed_data/typed/ | wc -l
55/63:
temp_img_paths = []

for file_name in os.listdir("data/all_preprocessed_data/typed"):
    temp_img_paths.append(os.path.join("data/all_preprocessed_data/typed", file_name))

view_images(temp_img_paths)
55/64:
# Transform to preprocess images
class FUNSDTransform:
    
    transforms_FUNSD = transforms.Compose([
        transforms.Resize([50, ]),
        transforms.Lambda(lambda img: adjust_sharpness(img, sharpness_factor=2.0)),
        transforms.ToTensor()
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        t = FUNSDTransform.transforms_FUNSD(image)
        
        return t
55/65:
output_dir = "data/all_preprocessed_data/typed/"

for img_path in funsd_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = FUNSDTransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + "FUNSD" + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
55/66: ! ls data/all_preprocessed_data/typed/ | wc -l
55/67:
temp_img_paths = []

for file_name in os.listdir("data/all_preprocessed_data/typed"):
    temp_img_paths.append(os.path.join("data/all_preprocessed_data/typed", file_name))

view_images(temp_img_paths)
55/68:
# Util function to collect image paths from subdirectories
def get_image_paths(root_dir, data_levels=None, file_exts=None):
    """
    root_dir (str): Root directory to start from
    data_levels (list): Subdirectories to search for
    file_exts (tuple): File extensions to look for
    """
    
    image_paths = []
    
    if(data_levels == None or file_exts == None):
        # just collect the image paths (no filters)
        for root, _, files in os.walk(os.path.join(root_dir)):
            for file in files:
                if file.endswith(file_exts):
                    image_paths.append(os.path.join(root, file))
        
    else:
        for data_level in data_levels:
            for root, _, files in os.walk(os.path.join(root_dir, data_level)):
                for file in files:
                    if file.endswith(file_exts):
                        image_paths.append(os.path.join(root, file))
                        
    return image_paths
55/69:
root_dir = "data/SROIE2019/0325updated.task1train(626p)/"
file_exts = (".jpg")
sroie_img_paths = []

sroie_img_paths = get_image_paths(root_dir, None, file_exts)
55/70: len(sroie_img_paths)
55/71: sroie_img_paths[0]
55/72: ! mkdir data/SROIE2019/temp
55/73:
for img_path in sroie_img_paths:
    
    img_id = img
55/74:
import os
import json
import re
import sys
import shutil
import csv

from PIL import Image, UnidentifiedImageError
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine, adjust_sharpness

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
55/75:
for img_path in sroie_img_paths[0:1]:
    
    img_id = img_path.split("/")[-1][:-4]
    annotation_file = root_dir + img_id + ".txt"
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            print(row)
55/76:
for img_path in sroie_img_paths[0:1]:
    
    img_id = img_path.split("/")[-1][:-4]
    annotation_file = root_dir + img_id + ".txt"
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            print(row[0:8])
55/77:
for img_path in sroie_img_paths[0:1]:
    
    img_id = img_path.split("/")[-1][:-4]
    annotation_file = root_dir + img_id + ".txt"
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            print(tuple(row[0:8]))
55/78:
for img_path in sroie_img_paths[0:1]:
    
    img_id = img_path.split("/")[-1][:-4]
    annotation_file = root_dir + img_id + ".txt"
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            x1, y1, x2, y2, x3, y3, x4, y4 = tuple(row[0:8])
            print(x1, y1, x2, y2, x3, y3, x4, y4)
55/79:
output_path = "data/SROIE2019/temp/"

for img_path in sroie_img_paths[0:1]:
    
    img_id = img_path.split("/")[-1][:-4]
    annotation_file = root_dir + img_id + ".txt"
    
    # Open the image
    image = Image.open(img_path)
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            x1, y1, x2, y2, x3, y3, x4, y4 = tuple(row[0:8])
            
            left = min(x1, x2, x3, x4)
            right = max(x1, x2, x3, x4)
            upper = min(y1, y2, y3, y4)
            lower = max(y1, y2, y3, y4)
            
            cropped_image = image.crop((left, upper, right, lower))
            
            cropped_image.save(output_path, "PNG")
55/80:
output_path = "data/SROIE2019/temp/"

for img_path in sroie_img_paths[0:1]:
    
    img_id = img_path.split("/")[-1][:-4]
    annotation_file = root_dir + img_id + ".txt"
    
    # Open the image
    image = Image.open(img_path)
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            x1, y1, x2, y2, x3, y3, x4, y4 = tuple(row[0:8])
            
            left = int(min(x1, x2, x3, x4))
            right = int(max(x1, x2, x3, x4))
            upper = int(min(y1, y2, y3, y4))
            lower = int(max(y1, y2, y3, y4))
            
            cropped_image = image.crop((left, upper, right, lower))
            
            cropped_image.save(output_path, "PNG")
55/81:
output_path = "data/SROIE2019/temp/"

for img_path in sroie_img_paths[0:1]:
    
    count = 0
    
    img_id = img_path.split("/")[-1][:-4]
    annotation_file = root_dir + img_id + ".txt"
    
    # Open the image
    image = Image.open(img_path)
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            x1, y1, x2, y2, x3, y3, x4, y4 = tuple(row[0:8])
            
            left = int(min(x1, x2, x3, x4))
            right = int(max(x1, x2, x3, x4))
            upper = int(min(y1, y2, y3, y4))
            lower = int(max(y1, y2, y3, y4))
            
            cropped_image = image.crop((left, upper, right, lower))
            output_name = img_id + str(count)
            
            cropped_image.save(output_path+output_name+".png", "PNG")
55/82:
output_path = "data/SROIE2019/temp/"

for img_path in sroie_img_paths[0:1]:
    
    count = 0
    
    img_id = img_path.split("/")[-1][:-4]
    annotation_file = root_dir + img_id + ".txt"
    
    # Open the image
    image = Image.open(img_path)
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            x1, y1, x2, y2, x3, y3, x4, y4 = tuple(row[0:8])
            
            left = int(min(x1, x2, x3, x4))
            right = int(max(x1, x2, x3, x4))
            upper = int(min(y1, y2, y3, y4))
            lower = int(max(y1, y2, y3, y4))
            
            cropped_image = image.crop((left, upper, right, lower))
            output_name = img_id + "-" + str(count)
            
            cropped_image.save(output_path+output_name+".png", "PNG")
55/83:
output_path = "data/SROIE2019/temp/"

for img_path in sroie_img_paths[0:1]:
    
    count = 0
    
    img_id = img_path.split("/")[-1][:-4]
    annotation_file = root_dir + img_id + ".txt"
    
    # Open the image
    image = Image.open(img_path)
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            x1, y1, x2, y2, x3, y3, x4, y4 = tuple(row[0:8])
            
            left = int(min(x1, x2, x3, x4))
            right = int(max(x1, x2, x3, x4))
            upper = int(min(y1, y2, y3, y4))
            lower = int(max(y1, y2, y3, y4))
            
            cropped_image = image.crop((left, upper, right, lower))
            output_name = img_id + "-" + str(count)
            
            cropped_image.save(output_path+output_name+".png", "PNG")
            
            count += 1
55/84:
temp_img_paths = []

for file_name in os.listdir("data/SROIE2019/temp"):
    temp_img_paths.append(os.path.join("data/SROIE2019/temp", file_name))

view_images(temp_img_paths)
55/85:
directory = 'data/SROIE2019/temp'  # Replace with your image directory
aspect_ratios = calculate_aspect_ratios(directory)

# Calculate average aspect ratio
average_aspect_ratio = np.mean(aspect_ratios)

# Calculate median aspect ratio
median_aspect_ratio = np.median(aspect_ratios)

# Plot the distribution of aspect ratios
plt.hist(aspect_ratios, bins=50)
plt.xlabel('Aspect Ratio (Width/Height)')
plt.ylabel('Number of Images')
plt.show()

print(f"Average aspect ratio: {average_aspect_ratio}")
print(f"Median aspect ratio: {median_aspect_ratio}")
55/86:
output_path = "data/SROIE2019/temp/"

for img_path in sroie_img_paths:
    
    count = 0
    
    img_id = img_path.split("/")[-1][:-4]
    annotation_file = root_dir + img_id + ".txt"
    
    # Open the image
    image = Image.open(img_path)
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            x1, y1, x2, y2, x3, y3, x4, y4 = tuple(row[0:8])
            
            left = int(min(x1, x2, x3, x4))
            right = int(max(x1, x2, x3, x4))
            upper = int(min(y1, y2, y3, y4))
            lower = int(max(y1, y2, y3, y4))
            
            cropped_image = image.crop((left, upper, right, lower))
            output_name = img_id + "-" + str(count)
            
            cropped_image.save(output_path+output_name+".png", "PNG")
            
            count += 1
55/87:
output_path = "data/SROIE2019/temp/"

for img_path in sroie_img_paths:
    
    count = 0
    
    img_id = img_path.split("/")[-1][:-4]
    annotation_file = root_dir + img_id + ".txt"
    
    # Open the image
    image = Image.open(img_path)
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            x1, y1, x2, y2, x3, y3, x4, y4 = tuple(row[0:8])
            
            left = int(min(x1, x2, x3, x4))
            right = int(max(x1, x2, x3, x4))
            upper = int(min(y1, y2, y3, y4))
            lower = int(max(y1, y2, y3, y4))
            
            print(img_id)
            
            cropped_image = image.crop((left, upper, right, lower))
            output_name = img_id + "-" + str(count)
            
            cropped_image.save(output_path+output_name+".png", "PNG")
            
            count += 1
55/88:
output_path = "data/SROIE2019/temp/"

for img_path in sroie_img_paths:
    
    count = 0
    
    img_id = img_path.split("/")[-1][:-4]
    annotation_file = root_dir + img_id + ".txt"
    
    # Open the image
    image = Image.open(img_path)
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            x1, y1, x2, y2, x3, y3, x4, y4 = tuple(row[0:8])
            
            left = int(min(x1, x2, x3, x4))
            right = int(max(x1, x2, x3, x4))
            upper = int(min(y1, y2, y3, y4))
            lower = int(max(y1, y2, y3, y4))
            
            print(img_id, count)
            
            cropped_image = image.crop((left, upper, right, lower))
            output_name = img_id + "-" + str(count)
            
            cropped_image.save(output_path+output_name+".png", "PNG")
            
            count += 1
55/89:
output_path = "data/SROIE2019/temp/"

for img_path in sroie_img_paths:
    
    count = 0
    
    img_id = img_path.split("/")[-1][:-4]
    annotation_file = root_dir + img_id + ".txt"
    
    # Open the image
    image = Image.open(img_path)
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            x1, y1, x2, y2, x3, y3, x4, y4 = tuple(row[0:8])
            
            left = int(min(x1, x2, x3, x4))
            right = int(max(x1, x2, x3, x4))
            upper = int(min(y1, y2, y3, y4))
            lower = int(max(y1, y2, y3, y4))
            
            print(img_id, count, (left, upper, right, lower))
            
            cropped_image = image.crop((left, upper, right, lower))
            output_name = img_id + "-" + str(count)
            
            cropped_image.save(output_path+output_name+".png", "PNG")
            
            count += 1
55/90:
output_path = "data/SROIE2019/temp/"

for img_path in sroie_img_paths:
    
    count = 0
    
    img_id = img_path.split("/")[-1][:-4]
    annotation_file = root_dir + img_id + ".txt"
    
    # Open the image
    image = Image.open(img_path)
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            x1, y1, x2, y2, x3, y3, x4, y4 = tuple(row[0:8])
            
            print(x1, x2, x3, x4)
            
            left = int(min(x1, x2, x3, x4))
            right = int(max(x1, x2, x3, x4))
            upper = int(min(y1, y2, y3, y4))
            lower = int(max(y1, y2, y3, y4))
            
            print(img_id, count, (left, upper, right, lower))
            
            cropped_image = image.crop((left, upper, right, lower))
            output_name = img_id + "-" + str(count)
            
            cropped_image.save(output_path+output_name+".png", "PNG")
            
            count += 1
55/91: min(x1, x2, x3, x4)
55/92:
output_path = "data/SROIE2019/temp/"

for img_path in sroie_img_paths:
    
    count = 0
    
    img_id = img_path.split("/")[-1][:-4]
    annotation_file = root_dir + img_id + ".txt"
    
    # Open the image
    image = Image.open(img_path)
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            x1, y1, x2, y2, x3, y3, x4, y4 = tuple([int(x) for x in row[0:8]])
                        
            left = min(x1, x2, x3, x4)
            right = max(x1, x2, x3, x4)
            upper = min(y1, y2, y3, y4)
            lower = max(y1, y2, y3, y4)
                        
            cropped_image = image.crop((left, upper, right, lower))
            output_name = img_id + "-" + str(count)
            
            cropped_image.save(output_path+output_name+".png", "PNG")
            
            count += 1
55/93:
def calculate_aspect_ratios(directory):
    aspect_ratios = []
    for filename in os.walk(directory):
        with Image.open(os.path.join(directory, filename)) as img:
            width, height = img.size
            aspect_ratios.append(width / height)
    return aspect_ratios

directory = 'data/FUNSD/temp'  # Replace with your image directory
aspect_ratios = calculate_aspect_ratios(directory)

# Calculate average aspect ratio
average_aspect_ratio = np.mean(aspect_ratios)

# Calculate median aspect ratio
median_aspect_ratio = np.median(aspect_ratios)

# Plot the distribution of aspect ratios
plt.hist(aspect_ratios, bins=50)
plt.xlabel('Aspect Ratio (Width/Height)')
plt.ylabel('Number of Images')
plt.show()

print(f"Average aspect ratio: {average_aspect_ratio}")
print(f"Median aspect ratio: {median_aspect_ratio}")
55/94:
def calculate_aspect_ratios(directory):
    aspect_ratios = []
    for root, _ , file in os.walk(directory):
        with Image.open(os.path.join(directory, file)) as img:
            width, height = img.size
            aspect_ratios.append(width / height)
    return aspect_ratios

directory = 'data/FUNSD/temp'  # Replace with your image directory
aspect_ratios = calculate_aspect_ratios(directory)

# Calculate average aspect ratio
average_aspect_ratio = np.mean(aspect_ratios)

# Calculate median aspect ratio
median_aspect_ratio = np.median(aspect_ratios)

# Plot the distribution of aspect ratios
plt.hist(aspect_ratios, bins=50)
plt.xlabel('Aspect Ratio (Width/Height)')
plt.ylabel('Number of Images')
plt.show()

print(f"Average aspect ratio: {average_aspect_ratio}")
print(f"Median aspect ratio: {median_aspect_ratio}")
55/95:
def calculate_aspect_ratios(directory):
    aspect_ratios = []
    for root, _ , files in os.walk(directory):
        for file in files:
            with Image.open(os.path.join(directory, file)) as img:
                width, height = img.size
                aspect_ratios.append(width / height)
    return aspect_ratios

directory = 'data/FUNSD/temp'  # Replace with your image directory
aspect_ratios = calculate_aspect_ratios(directory)

# Calculate average aspect ratio
average_aspect_ratio = np.mean(aspect_ratios)

# Calculate median aspect ratio
median_aspect_ratio = np.median(aspect_ratios)

# Plot the distribution of aspect ratios
plt.hist(aspect_ratios, bins=50)
plt.xlabel('Aspect Ratio (Width/Height)')
plt.ylabel('Number of Images')
plt.show()

print(f"Average aspect ratio: {average_aspect_ratio}")
print(f"Median aspect ratio: {median_aspect_ratio}")
55/96:
def calculate_aspect_ratios(directory):
    aspect_ratios = []
    for root, _ , files in os.walk(directory):
        for file in files:
            with Image.open(os.path.join(root, file)) as img:
                width, height = img.size
                aspect_ratios.append(width / height)
    return aspect_ratios

directory = 'data/FUNSD/temp'  # Replace with your image directory
aspect_ratios = calculate_aspect_ratios(directory)

# Calculate average aspect ratio
average_aspect_ratio = np.mean(aspect_ratios)

# Calculate median aspect ratio
median_aspect_ratio = np.median(aspect_ratios)

# Plot the distribution of aspect ratios
plt.hist(aspect_ratios, bins=50)
plt.xlabel('Aspect Ratio (Width/Height)')
plt.ylabel('Number of Images')
plt.show()

print(f"Average aspect ratio: {average_aspect_ratio}")
print(f"Median aspect ratio: {median_aspect_ratio}")
55/97: ! ls data/all_preprocessed_data/typed/ | wc -l
55/98:
#  This took way too long to finish

output_path = "data/SROIE2019/temp/"

for img_path in sroie_img_paths:
    
    count = 0
    
    img_id = img_path.split("/")[-1][:-4]
    
    if(img_id.contains("(")):
        continue
    
    annotation_file = root_dir + img_id + ".txt"
    
    # Open the image
    image = Image.open(img_path)
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            x1, y1, x2, y2, x3, y3, x4, y4 = tuple([int(x) for x in row[0:8]])
                        
            left = min(x1, x2, x3, x4)
            right = max(x1, x2, x3, x4)
            upper = min(y1, y2, y3, y4)
            lower = max(y1, y2, y3, y4)
                        
            cropped_image = image.crop((left, upper, right, lower))
            output_name = img_id + "-" + str(count)
            
            cropped_image.save(output_path+output_name+".png", "PNG")
            
            count += 1
55/99:
#  This took way too long to finish

output_path = "data/SROIE2019/temp/"

for img_path in sroie_img_paths:
    
    count = 0
    
    img_id = img_path.split("/")[-1][:-4]
    
    if("(" in img_id):
        continue
    
    annotation_file = root_dir + img_id + ".txt"
    
    # Open the image
    image = Image.open(img_path)
    
    with open(annotation_file, "r") as ann:
        reader = csv.reader(ann)
        
        for row in reader:
            x1, y1, x2, y2, x3, y3, x4, y4 = tuple([int(x) for x in row[0:8]])
                        
            left = min(x1, x2, x3, x4)
            right = max(x1, x2, x3, x4)
            upper = min(y1, y2, y3, y4)
            lower = max(y1, y2, y3, y4)
                        
            cropped_image = image.crop((left, upper, right, lower))
            output_name = img_id + "-" + str(count)
            
            cropped_image.save(output_path+output_name+".png", "PNG")
            
            count += 1
55/100:
# check aspect ratio of images from CRAFT folder

CRAFT_directory = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"
aspect_ratios = calculate_aspect_ratios(CRAFT_directory)

# Calculate average aspect ratio
average_aspect_ratio = np.mean(aspect_ratios)

# Calculate median aspect ratio
median_aspect_ratio = np.median(aspect_ratios)

# Plot the distribution of aspect ratios
plt.hist(aspect_ratios, bins=50)
plt.xlabel('Aspect Ratio (Width/Height)')
plt.ylabel('Number of Images')
plt.show()

print(f"Average aspect ratio: {average_aspect_ratio}")
print(f"Median aspect ratio: {median_aspect_ratio}")
55/101:
temp_img_paths = []

for file_name in os.listdir("data/SROIE2019/temp"):
    temp_img_paths.append(os.path.join("data/SROIE2019/temp", file_name))

view_images(temp_img_paths)
55/102:
directory = 'data/SROIE2019/temp'  # Replace with your image directory
aspect_ratios = calculate_aspect_ratios(directory)

# Calculate average aspect ratio
average_aspect_ratio = np.mean(aspect_ratios)

# Calculate median aspect ratio
median_aspect_ratio = np.median(aspect_ratios)

# Plot the distribution of aspect ratios
plt.hist(aspect_ratios, bins=50)
plt.xlabel('Aspect Ratio (Width/Height)')
plt.ylabel('Number of Images')
plt.show()

print(f"Average aspect ratio: {average_aspect_ratio}")
print(f"Median aspect ratio: {median_aspect_ratio}")
55/103:
# Define the custom transform
class SROIETransform:
    
    # Define the custom transforms pipeline for longer images
    transforms_sroie_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
    ])
    
    # Define the custom transforms pipeline for shorter images
    transforms_sroie_words = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        width, height = image.size
        aspect_ratio = width/height
        
        if(aspect_ratio >= 5): 
            t = SROIETransform.transforms_sroie_lines(image)
        else:
            t = SROIETransform.transforms_sroie_words(image)
        
        return t
55/104:
# Define the custom transform
class SROIETransform:
    
    # Define the custom transforms pipeline for longer images
    transforms_sroie_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 1.5)),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
    ])
    
    # Define the custom transforms pipeline for shorter images
    transforms_sroie_words = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 1.5)),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        width, height = image.size
        aspect_ratio = width/height
        
        if(aspect_ratio >= 5): 
            t = SROIETransform.transforms_sroie_lines(image)
        else:
            t = SROIETransform.transforms_sroie_words(image)
        
        return t
55/105:
import os
import json
import re
import sys
import shutil
import csv
import glob

from PIL import Image, UnidentifiedImageError
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine, adjust_sharpness

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
55/106:
output_dir = "data/all_preprocessed_data/typed/"
directory = 'data/SROIE2019/temp'  

for img_path in os.listdir(directory):
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = SROIETransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + "SROIE" + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
55/107:
output_dir = "data/all_preprocessed_data/typed/"
directory = 'data/SROIE2019/temp/'  

for img_path in os.listdir(directory):
    count = 0
    
    try:
        # Open image
        img = Image.open(directory+img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = SROIETransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + "SROIE" + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
55/108:
# Define the custom transform
class SROIETransform:
    
    # Define the custom transforms pipeline for longer images
    transforms_sroie_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
    ])
    
    # Define the custom transforms pipeline for shorter images
    transforms_sroie_words = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 2.0)),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        width, height = image.size
        aspect_ratio = width/height
        
        if(aspect_ratio >= 5): 
            t = SROIETransform.transforms_sroie_lines(image)
        else:
            t = SROIETransform.transforms_sroie_words(image)
        
        return t
55/109:
output_dir = "data/all_preprocessed_data/typed/"
directory = 'data/SROIE2019/temp/'  

for img_path in os.listdir(directory):
    count = 0
    
    try:
        # Open image
        img = Image.open(directory+img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = SROIETransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + "SROIE" + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
55/110:
output_dir = "data/all_preprocessed_data/typed/"
directory = 'data/SROIE2019/temp/'  

for img_path in os.listdir(directory):
    count = 0
    
    try:
        # Open image
        img = Image.open(directory+img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = SROIETransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    except RuntimeError:
        # to exclude images smaller than the kernel used for blurring.
        continue
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + "SROIE" + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
55/111:
temp_img_paths = []
root_dir = "data/all_preprocessed_data/typed/"

g = glob.glob(root_dir+"*SROIE*")

count = 0

for file_name in g:
    count += 1
    temp_img_paths.append(os.path.join("data/SROIE2019/temp", file_name))

view_images(temp_img_paths)
55/112:
temp_img_paths = []
root_dir = "data/all_preprocessed_data/typed/"

g = glob.glob(root_dir+"*SROIE*")

count = 0

for file_name in g:
    count += 1
    temp_img_paths.append(file_name)

view_images(temp_img_paths)
55/113:
# Define the custom transform
class SROIETransform:
    
    # Define the custom transforms pipeline for longer images
    transforms_sroie_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.3, 0.3)),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
    ])
    
    # Define the custom transforms pipeline for shorter images
    transforms_sroie_words = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.3, 0.3)),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        width, height = image.size
        aspect_ratio = width/height
        
        if(aspect_ratio >= 5): 
            t = SROIETransform.transforms_sroie_lines(image)
        else:
            t = SROIETransform.transforms_sroie_words(image)
        
        return t
55/114:
# Define the custom transform
class SROIETransform:
    
    # Define the custom transforms pipeline for longer images
    transforms_sroie_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.3, 0.3)),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
    ])
    
    # Define the custom transforms pipeline for shorter images
    transforms_sroie_words = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.3, 0.3)),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        width, height = image.size
        aspect_ratio = width/height
        
        if(aspect_ratio >= 5): 
            t = SROIETransform.transforms_sroie_lines(image)
        else:
            t = SROIETransform.transforms_sroie_words(image)
        
        return t
55/115:
output_dir = "data/all_preprocessed_data/typed/"
directory = 'data/SROIE2019/temp/'  

for img_path in os.listdir(directory):
    count = 0
    
    try:
        # Open image
        img = Image.open(directory+img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = SROIETransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    except RuntimeError:
        # to exclude images smaller than the kernel used for blurring.
        continue
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + "SROIE" + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
55/116:
temp_img_paths = []
root_dir = "data/all_preprocessed_data/typed/"

g = glob.glob(root_dir+"*SROIE*")

count = 0

for file_name in g:
    count += 1
    temp_img_paths.append(file_name)

view_images(temp_img_paths)
55/117:
temp_img_paths = []
root_dir = "data/all_preprocessed_data/typed/"

g = glob.glob(root_dir+"*SROIE*")

count = 0

for file_name in g:
    count += 1
    temp_img_paths.append(file_name)

view_images(temp_img_paths)
55/118:
# Define the custom transform
class SROIETransform:
    
    # Define the custom transforms pipeline for longer images
    transforms_sroie_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 0.1)),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
    ])
    
    # Define the custom transforms pipeline for shorter images
    transforms_sroie_words = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 0.1)),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        width, height = image.size
        aspect_ratio = width/height
        
        if(aspect_ratio >= 5): 
            t = SROIETransform.transforms_sroie_lines(image)
        else:
            t = SROIETransform.transforms_sroie_words(image)
        
        return t
55/119:
output_dir = "data/all_preprocessed_data/typed/"
directory = 'data/SROIE2019/temp/'  

for img_path in os.listdir(directory):
    count = 0
    
    try:
        # Open image
        img = Image.open(directory+img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = SROIETransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    except RuntimeError:
        # to exclude images smaller than the kernel used for blurring.
        continue
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        output_file_path = output_dir + "SROIE" + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
55/120:
temp_img_paths = []
root_dir = "data/all_preprocessed_data/typed/"

g = glob.glob(root_dir+"*SROIE*")

count = 0

for file_name in g:
    count += 1
    temp_img_paths.append(file_name)

view_images(temp_img_paths)
55/121:
sys.path.insert(0, 'data/MSCOCO/coco-text/')

import coco_text
55/122:
sys.path.insert(0, 'data/MSCOCO/coco-text/')

import coco_text
55/123: ct = coco_text.COCO_Text('data/MSCOCO/cocotext.v2.json')
55/124: ct.info()
55/125:
imgs_typed = ct.getImgIds(imgIds=ct.train, 
                    catIds=[('class','machine printed')])
len(imgs_typed)
55/126:
imgs_hw = ct.getImgIds(imgIds=ct.train, 
                    catIds=[('class','handwritten')])
len(imgs_hw)
55/127:
dataDir='data/MSCOCO/'
dataType='train2014'
55/128: pylab.rcParams['figure.figsize'] = (10.0, 8.0)
55/129:
# pick one at random
img = ct.loadImgs(imgs_typed[np.random.randint(0,len(imgs_typed))])[0]
55/130:
I = io.imread('%s/%s/%s'%(dataDir,dataType,img['file_name']))
print('/images/%s/%s'%(dataType,img['file_name']))
plt.figure()
plt.imshow(I)
55/131:
# load and display text annotations
plt.imshow(I)
annIds = ct.getAnnIds(imgIds=img['id'])
anns = ct.loadAnns(annIds)
ct.showAnns(anns)
55/132:
for ann in anns:
    print(ann['class'], "-> ", ann['bbox'])
55/133: ann
55/134:
# load and display text annotations
plt.imshow(I)
annIds = ct.getAnnIds(imgIds=img['id'])
anns = ct.loadAnns(annIds)
ct.showAnns(anns)
55/135: img
55/136: annIds
55/137:
for ann in anns:
    print(ann['class'], "-> ", ann['bbox'])
55/138: ann
55/139:
# Get all images IDs with machine printed text
imgs_typed = ct.getImgIds(imgIds=ct.train, 
                    catIds=[('class','machine printed')])
len(imgs_typed)
55/140:
# Get all images IDs with machine printed text
imgs_typed = ct.getImgIds(imgIds=ct.train, 
                    catIds=[('class','handwritten')])
len(imgs_typed)
55/141:
def COCO_crop_images_with_bounding_boxes(image_path, bbox, output_folder, idx):
             
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[0]+bbox[2], bbox[1]+bbox[3]))
                        
    # print(image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    # append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    # Save the cropped image
    cropped_image.save(output_path, "JPEG")
55/142: ! mkdir data/MSCOCO/temp
55/143:
# Get all images using the image IDs
output_folder = "data/MSCOCO/temp/"

# Crop machine printed text from images using bounding box coordinates
# and save the cropped images in the "typed" folder
for img_id in range(len(imgs_typed)):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            COCO_crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder, ann['id'])
56/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
56/2:
dataset_path = "data/all_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 100)),
#     blur, thinning, resize and pad (255 -> white) to same size

    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/3:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 100)),
#     blur, thinning, resize and pad (255 -> white) to same size

    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/4:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
#     transforms.Resize([100, ]),
    transforms.CenterCrop((100, 100)),
#     blur, thinning, resize and pad (255 -> white) to same size

    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
57/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
57/2:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
#     transforms.Resize([100, ]),
    transforms.CenterCrop((100, 100)),
#     blur, thinning, resize and pad (255 -> white) to same size

    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/5:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
#     transforms.Resize([100, ]),
    transforms.CenterCrop((100, 100)),
#     blur, thinning, resize and pad (255 -> white) to same size

    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/6:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
#     transforms.Resize([100, ]),
    transforms.CenterCrop((100, 100)),
#     blur, thinning, resize and pad (255 -> white) to same size

    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/7: # consider image thinning as a preprocessing step
56/8:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
56/9:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
56/10:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/11:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/12:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/13:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            pass
#             counter += 1
#             # Check if the counter reaches the patience limit
#             if counter >= patience:
#                 print('Early stopping triggered...')
#             break

print("\n=== Training complete! ===\n")
57/3:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
#     transforms.Resize([100, ]),
    transforms.CenterCrop((100, 150)),
#     blur, thinning, resize and pad (255 -> white) to same size

    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
58/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, SubsetRandomSampler
58/2:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
58/3:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
58/4:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
58/5:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.RandomCrop((100, 100)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
58/6:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
58/7:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
58/8:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
58/9:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
58/10: test_correct/len(test_loader.dataset)
58/11: y_true_cpu, y_pred_cpu
58/12:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, int(len(test_loader.dataset)/4), figsize=(12, 3))
    for i in range(4):
        for j in range(int(len(test_loader.dataset)/4)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            img_no += 1
    plt.show()
58/13:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.RandomCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
58/14:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
58/15:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
58/16:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
58/17:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
58/18: test_correct/len(test_loader.dataset)
58/19: y_true_cpu, y_pred_cpu
58/20:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.RandomCrop((100, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
58/21:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
58/22:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
58/23:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
58/24:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
58/25: test_correct/len(test_loader.dataset)
58/26: y_true_cpu, y_pred_cpu
58/27:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
58/28:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
58/29:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
58/30:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
58/31: test_correct/len(test_loader.dataset)
58/32: y_true_cpu, y_pred_cpu
58/33:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, int(len(test_loader.dataset)/4), figsize=(12, 3))
    for i in range(4):
        for j in range(int(len(test_loader.dataset)/4)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            img_no += 1
    plt.show()
58/34:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, int(len(test_loader.dataset)/4)+1, figsize=(12, 3))
    for i in range(4):
        for j in range(int(len(test_loader.dataset)/4)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            img_no += 1
    plt.show()
58/35:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, int(len(test_loader.dataset)/4)+1, figsize=(12, 3))
    for i in range(4):
        for j in range(int(len(test_loader.dataset)/4)+1):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            img_no += 1
    plt.show()
58/36:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(5, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(4):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            img_no += 1
    plt.show()
58/37:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(5, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(5):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            img_no += 1
    plt.show()
58/38:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.RandomCrop((100, 100)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
58/39:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
58/40:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
58/41:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
58/42:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
58/43: test_correct/len(test_loader.dataset)
58/44: y_true_cpu, y_pred_cpu
58/45:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.RandomCrop((100, 180)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
58/46:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
58/47:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
58/48:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
58/49:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
58/50: test_correct/len(test_loader.dataset)
58/51: y_true_cpu, y_pred_cpu
58/52:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
58/53:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
58/54:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
58/55:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
58/56:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
58/57: test_correct/len(test_loader.dataset)
58/58: y_true_cpu, y_pred_cpu
58/59:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
58/60:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
58/61:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
58/62:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
58/63:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
58/64:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
58/65:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
58/66: test_correct/len(test_loader.dataset)
58/67: y_true_cpu, y_pred_cpu
58/68:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(5, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(5):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            img_no += 1
    plt.show()
58/69:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
58/70:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.RandomCrop((100, 100)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
58/71:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
58/72:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
58/73:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
58/74:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
58/75: test_correct/len(test_loader.dataset)
58/76: y_true_cpu, y_pred_cpu
58/77:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.RandomCrop((100, 150)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
58/78:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
58/79:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
58/80:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
58/81:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
58/82: test_correct/len(test_loader.dataset)
58/83: y_true_cpu, y_pred_cpu
56/14:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([224, ]),
    transforms.CenterCrop((224, 300)),
    
    transforms.Pad((0, 0, max(0, 300 - img.width), max(0, 224 - img.height)), fill=(255, 255, 255)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/15:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([224, ]),
    transforms.CenterCrop((224, 300)),
    
#     transforms.Pad((0, 0, max(0, 300 - img.width), max(0, 224 - img.height)), fill=(255, 255, 255)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/16:
# Define the percentage of samples for the validation set
val_split = 0.1  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/17:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
#     transforms.CenterCrop((224, 300)),
    
#     transforms.Pad((0, 0, max(0, 300 - img.width), max(0, 224 - img.height)), fill=(255, 255, 255)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/18:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
56/19:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
56/20:
# Define the percentage of samples for the validation set
val_split = 0.1  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/21:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/22:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.RandomCrop((100, 300)),
    
#     transforms.Pad((0, 0, max(0, 300 - img.width), max(0, 224 - img.height)), fill=(255, 255, 255)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/23:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
56/24:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
56/25:
# Define the percentage of samples for the validation set
val_split = 0.1  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/26:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/27:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.RandomCrop((100, 200), padding=),
    
#     transforms.Lambda.Pad(),
    transforms.Lambda(
        lambda img: transforms.functional.pad((0, 0, max(0, 300 - img.width), max(0, 224 - img.height)), 
                                              fill=(255, 255, 255)))
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/28:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.RandomCrop((100, 200)),
    
#     transforms.Lambda.Pad(),
    transforms.Lambda(
        lambda img: transforms.functional.pad((0, 0, max(0, 300 - img.width), max(0, 224 - img.height)), 
                                              fill=(255, 255, 255)))
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/29:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.RandomCrop((100, 200)),
    
#     transforms.Lambda.Pad(),
    transforms.Lambda(
        lambda img: transforms.functional.pad((0, 0, max(0, 300 - img.width), max(0, 224 - img.height)), 
                                              fill=(255, 255, 255))),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/30:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.RandomCrop((100, 200)),
    
#     transforms.Lambda.Pad(),
    transforms.Lambda(
        lambda img: transforms.functional.pad((0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/31:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
56/32:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
56/33:
# Define the percentage of samples for the validation set
val_split = 0.1  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/34:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/35:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: transforms.functional.pad((0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.RandomCrop((100, 200)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/36:
# Define the percentage of samples for the validation set
val_split = 0.1  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/37:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/38:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
56/39:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad((0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.RandomCrop((100, 200)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/40:
# Define the percentage of samples for the validation set
val_split = 0.1  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/41:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/42:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.RandomCrop((100, 200)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/43:
# Define the percentage of samples for the validation set
val_split = 0.1  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/44:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/45:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.RandomCrop((100, 200)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/46:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
56/47:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
56/48:
# Define the percentage of samples for the validation set
val_split = 0.1  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/49:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/50:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/51:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            pass
#             counter += 1
#             # Check if the counter reaches the patience limit
#             if counter >= patience:
#                 print('Early stopping triggered...')
#             break

print("\n=== Training complete! ===\n")
56/52:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/53:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
56/54:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/vgg16_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
56/55:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.RandomCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/56:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/57:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/58:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/59:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/60: test_correct/len(test_loader.dataset)
56/61: y_true_cpu, y_pred_cpu
55/144:
# Get all images IDs with machine printed text
imgs_typed = ct.getImgIds(imgIds=ct.train, 
                    catIds=[('class','machine printed')])
len(imgs_typed)
55/145:
# Get all images using the image IDs
output_folder = "data/MSCOCO/temp/"

# Crop machine printed text from images using bounding box coordinates
# and save the cropped images in the "typed" folder
for img_id in range(len(imgs_typed)):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            COCO_crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder, ann['id'])
56/62:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(4, int(len(test_loader.dataset)/4), figsize=(12, 3))
    for i in range(4):
        for j in range(int(len(test_loader.dataset)/4)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            img_no += 1
    plt.show()
56/63:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(5, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(5):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            img_no += 1
    plt.show()
56/64:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
56/65:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            img_no += 1
    plt.show()
56/66:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            # Create a Rectangle patch
            rect = patches.Rectangle((50, 100), 40, 30, linewidth=1, edgecolor='r', facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/67:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 40, 30, linewidth=1, edgecolor='r', facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/68:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 100, 150, linewidth=1, edgecolor='r', facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/69:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 150, 100, linewidth=1, edgecolor='r', facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/70:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=1, edgecolor='r', facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/71:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 100, linewidth=1, edgecolor='r', facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/72:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no])
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 100, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/73:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 100, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/74:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/75:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/76:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/77:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/78:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/79:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/80: test_correct/len(test_loader.dataset)
56/81:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.RandomCrop((100, 200)),
#     blur, thinning, resize and pad (255 -> white) to same size

    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/82:
# Define the percentage of samples for the validation set
val_split = 0.1  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/83:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/84:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/85:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)

# Load the pre-trained VGG16 model
model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = model.classifier[6].in_features
model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg16_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
55/146:
def COCO_crop_images_with_bounding_boxes(image_path, bbox, output_folder, idx):
             
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[0]+bbox[2], bbox[1]+bbox[3]))
    width, height = cropped_image.size
    
    if(width < 50 and height < 50):
        return 1
                        
    # print(image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    # append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    
    # Save the cropped image
#     cropped_image.save(output_path, "JPEG")
55/147: # ! mkdir data/MSCOCO/temp
55/148:
# Get all images using the image IDs
output_folder = "data/MSCOCO/temp/"
count = 0

# Crop machine printed text from images using bounding box coordinates
# and save the cropped images in the "typed" folder
for img_id in range(len(imgs_typed)):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            count += COCO_crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder, ann['id'])
55/149:
def COCO_crop_images_with_bounding_boxes(image_path, bbox, output_folder, idx):
             
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[0]+bbox[2], bbox[1]+bbox[3]))
    width, height = cropped_image.size
    
    if(width < 50 and height < 50):
        return 0
    return 1
                        
    # print(image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    # append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    
    # Save the cropped image
#     cropped_image.save(output_path, "JPEG")
55/150: # ! mkdir data/MSCOCO/temp
55/151:
# Get all images using the image IDs
output_folder = "data/MSCOCO/temp/"
count = 0

# Crop machine printed text from images using bounding box coordinates
# and save the cropped images in the "typed" folder
for img_id in range(len(imgs_typed)):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            count += COCO_crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder, ann['id'])
56/86:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.RandomCrop((100, 200)),
#     blur, thinning, resize and pad (255 -> white) to same size

    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/87:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.RandomCrop((100, 200)),
#     blur, thinning, resize and pad (255 -> white) to same size

    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/88:
# Define the percentage of samples for the validation set
val_split = 0.1  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/89:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/90:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/91:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# Load pretrained MobileNetV2 model and reset final fully connected layer.
model = models.mobilenet_v2(pretrained=True)
model.classifier[1] = nn.Linear(model.last_channel, num_classes)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/mobilenetv2_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/92:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# Load pretrained MobileNetV2 model and reset final fully connected layer.
model = models.mobilenet_v2(pretrained=True)
model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/mobilenetv2_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
55/152:
# Get all images using the image IDs
output_folder = "data/MSCOCO/temp/"
count = 0

# Crop machine printed text from images using bounding box coordinates
# and save the cropped images in the "typed" folder
for img_id in range(len(imgs_typed)):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            count += COCO_crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder, ann['id'])
56/93:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# Load pretrained MobileNetV2 model and reset final fully connected layer.
model = models.mobilenet_v2(pretrained=True)
model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/mobilenetv2_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/94:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/95:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# Load pretrained MobileNetV2 model and reset final fully connected layer.
model = models.mobilenet_v2(pretrained=True)
model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/mobilenetv2_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/96:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/97:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/98:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/99:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)


# Load pre-trained DenseNet model
model = models.densenet121(pretrained=True)

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/mobilenetv2_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/100:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)


# Load pre-trained DenseNet model
model = models.densenet121(pretrained=True)

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/mobilenetv2_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/101:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.RandomCrop((100, 200)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/102:
# Define the percentage of samples for the validation set
val_split = 0.1  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/103:
# Define the percentage of samples for the validation set
val_split = 0.2  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/104:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/105:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/106:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)


# Load pre-trained DenseNet model
model = models.densenet121(pretrained=True)

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/mobilenetv2_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/107:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.RandomCrop((100, 200)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/108:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/109:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/110:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/111:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)


# Load pre-trained DenseNet model
model = models.densenet121(pretrained=True)

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/mobilenetv2_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/112:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.RandomCrop((100, 200)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/113:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/114:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)


# Load pre-trained DenseNet model
model = models.densenet121(pretrained=True)

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/mobilenetv2_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/115:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.RandomCrop((100, 200)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/116:
# Define the percentage of samples for the validation set
val_split = 0.2  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/117:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/118:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/119:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)


# Load pre-trained DenseNet model
model = models.densenet121(pretrained=True)

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/mobilenetv2_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/120:
# Define the percentage of samples for the validation set
val_split = 0.2  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/121:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/122:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/123:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)


# Load pre-trained DenseNet model
model = models.densenet121(pretrained=True)

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/mobilenetv2_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/124:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)


# Load pre-trained DenseNet model
model = models.densenet121(pretrained=True)

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
55/153: count
55/154:
img_paths = []

for root, _, file_name in os.walk("/projectnb/sparkgrp/kabilanm/goodfilescraft/):
    funsd_img_paths.append(os.path.join("/projectnb/sparkgrp/kabilanm/goodfilescraft/", file_name))

view_images(img_paths)
55/155:
img_paths = []

for root, _, file_name in os.walk("/projectnb/sparkgrp/kabilanm/goodfilescraft/"):
    funsd_img_paths.append(os.path.join("/projectnb/sparkgrp/kabilanm/goodfilescraft/", file_name))

view_images(img_paths)
55/156:
# Talk to Kevin H about fonts
# RPM fusion
56/125:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
56/126:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")


# VGG 16
# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)

# DenseNet

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
56/127:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")


# VGG 16
# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)

# DenseNet
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
model.classifier = nn.Linear(num_ftrs, 1)  # 2 classes for binary classification

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
56/128:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

best_model = models.densenet121(pretrained=False)


# VGG 16
# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)

# DenseNet
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
model.classifier = nn.Linear(num_ftrs, 1)  # 2 classes for binary classification

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
56/129:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(pretrained=True)
# DenseNet
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
model.classifier = nn.Linear(num_ftrs, 1)  # 2 classes for binary classification

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
56/130:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(pretrained=True)
# DenseNet
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
model.classifier = nn.Linear(num_ftrs, 2)  # 2 classes for binary classification

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
56/131:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(pretrained=False)
# DenseNet
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model, vgg16_1_epoch
best_model = best_model.to(device)
56/132:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(pretrained=False)
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
56/133:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.RandomCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/134:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/135:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/136:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/137:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/138: test_correct/len(test_loader.dataset)
56/139: y_true_cpu, y_pred_cpu
56/140:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/141:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/142:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/143:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/144:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/145:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/146: test_correct/len(test_loader.dataset)
56/147: y_true_cpu, y_pred_cpu
56/148:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/149:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.RandomCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/150:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/151:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/152:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/153:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/154: test_correct/len(test_loader.dataset)
56/155: y_true_cpu, y_pred_cpu
56/156:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/157:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/158:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/159:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/160:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/161:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/162:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/163:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/164:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/165: test_correct/len(test_loader.dataset)
56/166: y_true_cpu, y_pred_cpu
56/167:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/168:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 250)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/169:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/170:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/171:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/172:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/173: test_correct/len(test_loader.dataset)
56/174: y_true_cpu, y_pred_cpu
56/175:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/176:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/177:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/178:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/179:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/180: test_correct/len(test_loader.dataset)
56/181: y_true_cpu, y_pred_cpu
56/182:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/183:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 300 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.RandomCrop((100, 300)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/184:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
56/185:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
56/186:
# Define the percentage of samples for the validation set
val_split = 0.2  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/187:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/188:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/189:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 000)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/190:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/191:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/192:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/193:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/194:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/195:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/196:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/197: test_correct/len(test_loader.dataset)
56/198: y_true_cpu, y_pred_cpu
56/199:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/200:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/201:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/202:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/203:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/204:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/205: test_correct/len(test_loader.dataset)
56/206: y_true_cpu, y_pred_cpu
56/207:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 300 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.RandomCrop((100, 300)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/208:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
56/209:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
56/210:
# Define the percentage of samples for the validation set
val_split = 0.2  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/211:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/212:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show((out * 255).astype(np.uint8))
    break  # Only display one batch of transformed images
56/213:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/214:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow((images[i]).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/215:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/216:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/217:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)


# Load pre-trained DenseNet model
model = models.densenet121(pretrained=True)

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/218:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)


# Load pre-trained DenseNet model
model = models.densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/219:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)


# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
58/84:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
58/85:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
58/86:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
58/87:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
58/88:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
58/89:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
58/90:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
58/91: test_correct/len(test_loader.dataset)
58/92: y_true_cpu, y_pred_cpu
58/93:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
58/94:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
58/95:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
58/96:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
58/97:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
58/98:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
58/99:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
58/100:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
58/101:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
58/102:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
58/103:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
58/104:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
58/105:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
58/106:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
58/107:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
58/108:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
58/109:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
58/110: test_correct/len(test_loader.dataset)
58/111: y_true_cpu, y_pred_cpu
58/112:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/220:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
56/221:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/222:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/223:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/224:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/225:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/226: test_correct/len(test_loader.dataset)
56/227: y_true_cpu, y_pred_cpu
56/228:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
56/229:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/230:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/231:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/232:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/233:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/234: test_correct/len(test_loader.dataset)
56/235: y_true_cpu, y_pred_cpu
56/236:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/237:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/238:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.RandomCrop((100, 300)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/239:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
56/240:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
56/241:
# Define the percentage of samples for the validation set
val_split = 0.2  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/242:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/243:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.RandomCrop((100, 200)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/244:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
56/245:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
56/246:
# Define the percentage of samples for the validation set
val_split = 0.2  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/247:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/248:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/249:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/250:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/251:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/252:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/253:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/254:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CentreCrop((100, 200)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/255:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
55/157:
def COCO_crop_images_with_bounding_boxes(image_path, bbox, output_folder, idx):       
    # Open the image
    image = Image.open(image_path)

    # Crop the image using the bounding box coordinates
    cropped_image = image.crop((bbox[0], bbox[1],
                                bbox[0]+bbox[2], bbox[1]+bbox[3]))
    width, height = cropped_image.size
           
    # Append bounding box id to the cropped image name
    output_path = os.path.join(output_folder, image_path.split("/")[-1][:-4] + "-"+str(idx)+".jpg")
    
    # Drop very small images
    if(width < 50 and height < 50):
        return
    
    # Save the cropped image
    cropped_image.save(output_path, "JPEG")
55/158: # ! mkdir data/MSCOCO/temp
55/159:
# Get all images using the image IDs
# output_folder = "data/MSCOCO/temp/"
output_folder = "data/all_preprocessed_data/typed/"
count = 0

# Crop machine printed text from images using bounding box coordinates
# and save the cropped images in the "typed" folder
for img_id in range(len(imgs_typed)):
    img = ct.loadImgs(imgs_typed[img_id])[0]
    image_path = "data/MSCOCO/train2014/"+img['file_name']
    
    # Get annotations in the image
    annIds = ct.getAnnIds(imgIds=img['id'])
    anns = ct.loadAnns(annIds)
    
    for ann in anns:
        if(ann['class'] == "machine printed"):
            COCO_crop_images_with_bounding_boxes(image_path, ann['bbox'], output_folder, ann['id'])
56/256:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
56/257:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
56/258:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
56/259:
# Define the percentage of samples for the validation set
val_split = 0.2  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/260:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/261:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/262:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/263:
# Define the percentage of samples for the validation set
val_split = 0.2  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
56/264:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
56/265:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
56/266:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 10

patience = 2
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/267:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
56/268: torch.save(model.state_dict(), 'model/densenet_best_model.pth')
56/269:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
56/270:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/271:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/272:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/273:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/274:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/275: test_correct/len(test_loader.dataset)
56/276: y_true_cpu, y_pred_cpu
56/277:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/278:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 15

patience = 3
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/279:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
56/280:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/281:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/282:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/283:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/284:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/285: test_correct/len(test_loader.dataset)
56/286: y_true_cpu, y_pred_cpu
56/287:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/288:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 8
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/289:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
56/290:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
56/291:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/292:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/293:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/294:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/295:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/296: test_correct/len(test_loader.dataset)
56/297: y_true_cpu, y_pred_cpu
56/298:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/299:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 8
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/300: # y_true_cpu, y_pred_cpu
56/301:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
56/302:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/303:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/304:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/305:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/306:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/307: test_correct/len(test_loader.dataset)
56/308: # y_true_cpu, y_pred_cpu
56/309:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/310:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
56/311:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 8
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
56/312:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
56/313:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
56/314:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
56/315:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
56/316:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
56/317:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
56/318:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
56/319: test_correct/len(test_loader.dataset)
56/320: # y_true_cpu, y_pred_cpu
56/321:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
59/1:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
59/2:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
59/3:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
59/4:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
59/5:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
59/6:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
59/7:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
59/8:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
59/9:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
59/10:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
59/11:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
59/12: test_correct/len(test_loader.dataset)
59/13: # y_true_cpu, y_pred_cpu
59/14:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
55/160: ! pip install uno
55/161:
import uno
import pathlib
from com.sun.star.beans import PropertyValue
import os
 
def create_instance():
    localContext = uno.getComponentContext()
    resolver = localContext.ServiceManager.createInstanceWithContext(
        "com.sun.star.bridge.UnoUrlResolver", localContext )
    ctx = resolver.resolve("uno:socket,host=localhost,port=2002;urp;StarOffice.ComponentContext" )
    smgr = ctx.ServiceManager
    desktop = smgr.createInstanceWithContext("com.sun.star.frame.Desktop",ctx)
 
    return desktop
 
def make_prop(name, val):
    return PropertyValue(Name=name, Value=val)
 
def file_url(path):
    path = os.path.abspath(path)
    return pathlib.Path(path).as_uri()
 
def export_pdf():
    desktop = create_instance()
 
    loadArgs = [
        make_prop("UpdateDocMode", 1),
        make_prop("Hidden", True)
    ]
 
    url = file_url("file.odt")
 
    print(url)
 
    print("Loading...")
 
    doc = desktop.loadComponentFromURL(url, "_blank", 0, loadArgs)
 
    print("Loaded...")
 
    filterProps = [
        make_prop("IsSkipEmptyPages", False),
    ]
    filterProps = uno.Any("[]com.sun.star.beans.PropertyValue", filterProps)
 
    saveArgs = [
        make_prop("FilterName", "writer_pdf_Export"),
        make_prop("FilterData", filterProps),
    ]
 
    pdfName = file_url("file.pdf")
 
    print("Saving PDF", pdfName)
 
    doc.storeToURL(pdfName, saveArgs)
 
    doc.dispose()

export_pdf()
55/162: ! pip uninstall uno
55/163: ! pip uninstall uno -y
55/164: ! pip uninstall uno -y
55/165: ! pip install unotools
55/166:
import uno
import pathlib
from com.sun.star.beans import PropertyValue
import os
 
def create_instance():
    localContext = uno.getComponentContext()
    resolver = localContext.ServiceManager.createInstanceWithContext(
        "com.sun.star.bridge.UnoUrlResolver", localContext )
    ctx = resolver.resolve("uno:socket,host=localhost,port=2002;urp;StarOffice.ComponentContext" )
    smgr = ctx.ServiceManager
    desktop = smgr.createInstanceWithContext("com.sun.star.frame.Desktop",ctx)
 
    return desktop
 
def make_prop(name, val):
    return PropertyValue(Name=name, Value=val)
 
def file_url(path):
    path = os.path.abspath(path)
    return pathlib.Path(path).as_uri()
 
def export_pdf():
    desktop = create_instance()
 
    loadArgs = [
        make_prop("UpdateDocMode", 1),
        make_prop("Hidden", True)
    ]
 
    url = file_url("file.odt")
 
    print(url)
 
    print("Loading...")
 
    doc = desktop.loadComponentFromURL(url, "_blank", 0, loadArgs)
 
    print("Loaded...")
 
    filterProps = [
        make_prop("IsSkipEmptyPages", False),
    ]
    filterProps = uno.Any("[]com.sun.star.beans.PropertyValue", filterProps)
 
    saveArgs = [
        make_prop("FilterName", "writer_pdf_Export"),
        make_prop("FilterData", filterProps),
    ]
 
    pdfName = file_url("file.pdf")
 
    print("Saving PDF", pdfName)
 
    doc.storeToURL(pdfName, saveArgs)
 
    doc.dispose()

export_pdf()
60/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
60/2:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
60/3:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
60/4:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
60/5:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
60/6:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
60/7:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
60/8:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
60/9:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
60/10: test_correct/len(test_loader.dataset)
60/11:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
63/1:
import os
import json
import re
import sys
import shutil
import csv
import glob

from PIL import Image, UnidentifiedImageError
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine, adjust_sharpness

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
from craft_text_detector import Craft
63/2:
import os
import json
import re
import sys
import shutil
import csv
import glob

from PIL import Image, UnidentifiedImageError
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine, adjust_sharpness

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
from craft_text_detector import Craft
63/3: # gdown https://drive.google.com/uc?id=1A27rofqn8ZTfj3L8wbzlY-0n4AL2nnh2
63/4: ! unzip -q data/synthetic-font-images.zip -d data/synthetic_font_data/
63/5:
output_dir_craft = "data/synthetic_font_data/output"
work_dir = "data/synthetic_font_data/images/"

# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft, 
              export_extra = False, 
              text_threshold = .7, 
              link_threshold = .4, 
              crop_type="poly", 
              low_text = .3, 
              cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
    
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
63/6:
import os
import json
import re
import sys
import shutil
import csv
import glob

from PIL import Image, UnidentifiedImageError
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine, adjust_sharpness

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
from craft_text_detector import Craft
from tqdm import tqdm
63/7:
output_dir_craft = "data/synthetic_font_data/output"
work_dir = "data/synthetic_font_data/images/"

# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft, 
              export_extra = False, 
              text_threshold = .7, 
              link_threshold = .4, 
              crop_type="poly", 
              low_text = .3, 
              cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
    
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
63/8:
output_dir_craft = "data/synthetic_font_data/output"
workdir = "data/synthetic_font_data/images/"

# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft, 
              export_extra = False, 
              text_threshold = .7, 
              link_threshold = .4, 
              crop_type="poly", 
              low_text = .3, 
              cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
    
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
63/9:
dir_craft = "data/synthetic_font_data/output"

for root, _ , files in os.walk(dir_craft):
    for file in files:
        print(file)
63/10:
dir_craft = "data/synthetic_font_data/output"

for root, _ , files in os.walk(dir_craft):
    for file in files:
        print(os.path.join/root, file)
63/11:
dir_craft = "data/synthetic_font_data/output"

for root, _ , files in os.walk(dir_craft):
    for file in files:
        print(os.path.join(root), file)
63/12:
dir_craft = "data/synthetic_font_data/output"

for root, _ , files in os.walk(dir_craft):
    for file in files:
        print(os.path.join(root, file))
63/13:
dir_craft = "data/synthetic_font_data/output"
synthetic_img_paths = []

for root, _ , files in os.walk(dir_craft):
    for file in files:
        synthetic_img_paths.append(os.path.join(root, file))
63/14:
# Define the custom transform
class SyntheticImagesTransform:
    
    # Define the custom transforms pipeline for longer images
    transforms_syn_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 0.1)),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
    ])
    
    # Define the custom transforms pipeline for shorter images
    transforms_syn_words = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 0.1)),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        width, height = image.size
        aspect_ratio = width/height
        
        if(aspect_ratio >= 5): 
            t = SROIETransform.transforms_sroie_lines(image)
        else:
            t = SROIETransform.transforms_sroie_words(image)
        
        return t
63/15:
# Define the custom transform
class SyntheticImagesTransform:
    
    # Define the custom transforms pipeline for longer images
    transforms_syn_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 0.1)),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
    ])
    
    # Define the custom transforms pipeline for shorter images
    transforms_syn_words = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 0.1)),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        width, height = image.size
        aspect_ratio = width/height
        
        if(aspect_ratio >= 5): 
            t = SyntheticImagesTransform.transforms_syn_lines(image)
        else:
            t = SyntheticImagesTransform.transforms_syn_words(image)
        
        return t
63/16:
# Define the custom transform
class SyntheticImagesTransform:
    
    # Define the custom transforms pipeline for longer images
    transforms_syn_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 0.1)),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        transforms.Lambda(lambda crops: torch.stack([crop for crop in crops]))
    ])
    
    # Define the custom transforms pipeline for shorter images
    transforms_syn_words = transforms.Compose([
        transforms.ToTensor(),
        transforms.GaussianBlur((3, 3), sigma=(0.1, 0.1)),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        width, height = image.size
        aspect_ratio = width/height
        
        if(aspect_ratio >= 5): 
            t = SyntheticImagesTransform.transforms_syn_lines(image)
        else:
            t = SyntheticImagesTransform.transforms_syn_words(image)
        
        return t
63/17: len(synthetic_img_paths)
63/18: synthetic_img_paths[0]
63/19:
output_dir = "data/all_preprocessed_data/typed/"
directory = 'data/synthetic_font_data/output/'  

for img_path in synthetic_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = SyntheticImagesTransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    except RuntimeError:
        # to exclude images smaller than the kernel used for blurring.
        continue
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        temp = img_path.split("/")[-2]
        output_file_path = output_dir + "synthetic" + temp + "-" + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
62/1:
#Imports and installs
import transformers
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
import torch
import os
from PIL import Image
import torch
import torch.nn as nn
import torchvision
from torchvision import datasets
from tqdm import tqdm
import pandas as pd

import numpy as np
import json

import warnings

import trocr

from taxonerd import TaxoNERD
61/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
61/2:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     blur, thinning, resize and pad (255 -> white) to same size

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
61/3:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
61/4:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
61/5:
# Define the percentage of samples for the validation set
val_split = 0.2  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
61/6:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
61/7:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
61/8:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 50
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
61/9:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 50
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
65/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
65/2:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   thinning

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
65/3:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
65/4:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
65/5:
# Define the percentage of samples for the validation set
val_split = 0.2  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
65/6:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
65/7:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
65/8:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 50
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
66/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
66/2:
best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
66/3:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
66/4:
best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
67/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
67/2:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
67/3:
best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
67/4:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
67/5:
best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
67/6:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
67/7:
best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
67/8:
# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
device
67/9:
best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
67/10:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
67/11:
best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth'))
best_model = best_model.to(device)
67/12:
best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth'))
best_model = best_model.to(device)
67/13:
# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
device
67/14:
best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth'))
best_model = best_model.to(device)
67/15:
best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth'))
best_model = best_model.to(torch.device('cpu'))
67/16:
best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth'))
best_model = best_model.to(device)
65/9:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
65/10:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
65/11:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=16)
65/12:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
65/13:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
65/14:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
65/15: test_correct/len(test_loader.dataset)
65/16: # y_true_cpu, y_pred_cpu
65/17:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%len(test_loader.dataset)])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
65/18:
#  Image 8 misclassified most probably due to COCO-Text
len(test_loader.dataset)
65/19:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%16])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
65/20:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 50
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
65/21:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
65/22:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
65/23:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
65/24:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
65/25: test_correct/len(test_loader.dataset)
65/26: # y_true_cpu, y_pred_cpu
65/27:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%16])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
65/28:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
65/29:
#  Image 8 misclassified most probably due to COCO-Text
len(test_loader.dataset)
65/30:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows)+1, figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
65/31:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 50
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
65/32:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   thinning

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
65/33:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
65/34:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
65/35:
# Define the percentage of samples for the validation set
val_split = 0.2  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
65/36:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
65/37:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
65/38:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 50
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
65/39:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
65/40:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
65/41:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
65/42:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
65/43:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
65/44:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
65/45: test_correct/len(test_loader.dataset)
65/46: # y_true_cpu, y_pred_cpu
65/47:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows)+1, figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
65/48:
#  Image 8 misclassified most probably due to COCO-Text
len(test_loader.dataset)
65/49:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
65/50:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
65/51: test_correct/len(test_loader.dataset)
65/52: # y_true_cpu, y_pred_cpu
65/53:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
65/54: len(train_loader)
65/55: len(train_loader.dataset)
65/56: len(train_loader)
65/57:
# Define the percentage of samples for the validation set
val_split = 0.2  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
65/58:
# Define the percentage of samples for the validation set
val_split = 0.2  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
65/59:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
65/60:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 300 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 300)),
#   thinning

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
65/61:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
65/62:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
65/63:
# Define the percentage of samples for the validation set
val_split = 0.2  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
65/64:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
65/65:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
65/66: len(train_loader)
65/67:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 50
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
            break

print("\n=== Training complete! ===\n")
65/68:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
65/69:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
65/70:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
65/71:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
65/72:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
65/73:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
65/74: test_correct/len(test_loader.dataset)
65/75: # y_true_cpu, y_pred_cpu
65/76:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 5

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
65/77:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
65/78:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 300)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
65/79:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
65/80:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
65/81:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
65/82:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
65/83: test_correct/len(test_loader.dataset)
65/84: # y_true_cpu, y_pred_cpu
65/85:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
65/86:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   thinning

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
65/87:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
65/88:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
65/89:
# Define the percentage of samples for the validation set
val_split = 0.2  # 10% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
65/90:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
65/91:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
65/92: len(train_loader)
68/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
68/2:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
68/3:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # vgg16_best_model
best_model = best_model.to(device)
68/4:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
68/5:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
68/6:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
68/7:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
68/8:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
68/9: test_correct/len(test_loader.dataset)
68/10:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
68/11:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by dilation is operated on the grayscale image)

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/12:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
68/13:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
68/14:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 16

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
68/15:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/16:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
68/17: len(train_loader)
68/18:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20

patience = 50
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            pass
#             counter += 1
#             # Check if the counter reaches the patience limit
#             if counter >= patience:
#                 print('Early stopping triggered...')
#             break

print("\n=== Training complete! ===\n")
68/19:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
68/20:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/21:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
68/22: len(train_loader)
68/23:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20

patience = 50
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            pass
#             counter += 1
#             # Check if the counter reaches the patience limit
#             if counter >= patience:
#                 print('Early stopping triggered...')
#                 break

print("\n=== Training complete! ===\n")
70/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
70/2:
# Move the model to the device (CPU or GPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cpu')
device
70/3:
best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth'))
best_model = best_model.to(device)
70/4:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
70/5:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
70/6:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
70/7:
best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth'))
best_model = best_model.to(device)
68/24: torch.save(model.state_dict(), 'model/densenet_last_epoch_model.pth')
68/25:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
68/26:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
68/27:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
68/28:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
68/29:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
68/30:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
68/31:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
68/32: test_correct/len(test_loader.dataset)
68/33: # y_true_cpu, y_pred_cpu
68/34:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
68/35:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_last_epoch_model.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
68/36:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
68/37:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
68/38:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
68/39:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
68/40:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
68/41: test_correct/len(test_loader.dataset)
68/42: # y_true_cpu, y_pred_cpu
68/43:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
68/44:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
68/45:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
68/46:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
68/47:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
68/48:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
68/49:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
68/50: test_correct/len(test_loader.dataset)
68/51: # y_true_cpu, y_pred_cpu
68/52:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
68/53:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 7

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
68/54:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6|

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
68/55:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
68/56:
# Define the custom transform
class PartialErosion:
    
    def __init__(self):
        pass
    
    def __call__(self, image):

        # Define structuring element
        selem = morphology.square(3)

        # Define number of iterations
        iterations = 2  # Change this to control the number of erosion iterations

        # Perform erosion for the specified number of iterations
        eroded_image = image
        for i in range(iterations):
            eroded_image = morphology.erosion(eroded_image, selem)
        
        return eroded_image
68/57:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    PartialErosion(),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/58:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
68/59:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
68/60:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
68/61:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/62:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, 

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
68/63:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
68/64:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/65:
# Define the custom transform
class PartialErosion:
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        image = transforms.Grayscale()(image)

        # Define structuring element
        selem = morphology.square(3)

        # Define number of iterations
        iterations = 2  # Change this to control the number of erosion iterations

        # Perform erosion for the specified number of iterations
        eroded_image = image
        for i in range(iterations):
            eroded_image = morphology.erosion(eroded_image, selem)
        
        return eroded_image
68/66:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/67:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/68:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
68/69:
# Define the custom transform
class PartialErosion:
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        # Convert the image to grayscale
        grayscale_image = color.rgb2gray(image)

        # Define structuring element
        selem = morphology.square(3)

        # Define number of iterations
        iterations = 2  # Change this to control the number of erosion iterations

        # Perform erosion for the specified number of iterations
        eroded_image = grayscale_image
        for i in range(iterations):
            eroded_image = morphology.erosion(eroded_image, selem)
        
        return eroded_image
68/70:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/71:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/72:
# Define the custom transform
class PartialErosion:
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        # Convert the image to grayscale
        grayscale_image = color.rgb2gray(image)

        # Define structuring element
        selem = morphology.square(3)

        # Define number of iterations
        iterations = 2  # Change this to control the number of erosion iterations

        # Perform erosion for the specified number of iterations
        eroded_image = grayscale_image
        for i in range(iterations):
            eroded_image = morphology.erosion(eroded_image, selem)
        
        return eroded_image
68/73:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/74:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
68/75:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
68/76:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
68/77:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/78:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/79:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
68/80:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
68/81:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
68/82:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/83:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/84:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
68/85:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
68/86:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
68/87:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/88:
# Define the custom transform
class PartialErosion:
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        # Convert the image to grayscale
        grayscale_image = color.rgb2gray(image)

        # Define structuring element
        selem = morphology.square(3)

        # Define number of iterations
        iterations = 20  # Change this to control the number of erosion iterations

        # Perform erosion for the specified number of iterations
        eroded_image = grayscale_image
        for i in range(iterations):
            eroded_image = morphology.erosion(eroded_image, selem)
        
        return eroded_image
68/89:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/90:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
68/91:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
68/92:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
68/93:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/94:
# Define the custom transform
class PartialErosion:
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        # Convert the image to grayscale
        grayscale_image = color.rgb2gray(image)

        # Define structuring element
        selem = morphology.square(3)

        # Define number of iterations
        iterations = 5  # Change this to control the number of erosion iterations

        # Perform erosion for the specified number of iterations
        eroded_image = grayscale_image
        for i in range(iterations):
            eroded_image = morphology.erosion(eroded_image, selem)
        
        return eroded_image
68/95:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/96:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
68/97:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
68/98:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
68/99:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/100:
# Define the custom transform
class PartialErosion:
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        # Convert the image to grayscale
        grayscale_image = color.rgb2gray(image)

        # Define structuring element
        selem = morphology.square(3)

        # Define number of iterations
        iterations = 7  # Change this to control the number of erosion iterations

        # Perform erosion for the specified number of iterations
        eroded_image = grayscale_image
        for i in range(iterations):
            eroded_image = morphology.erosion(eroded_image, selem)
        
        return eroded_image
68/101:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/102:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
68/103:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
68/104:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
68/105:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/106:
# Define the custom transform
class PartialErosion:
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        # Convert the image to grayscale
        grayscale_image = color.rgb2gray(image)

        # Define structuring element
        selem = morphology.square(1)

        # Define number of iterations
        iterations = 7  # Change this to control the number of erosion iterations

        # Perform erosion for the specified number of iterations
        eroded_image = grayscale_image
        for i in range(iterations):
            eroded_image = morphology.erosion(eroded_image, selem)
        
        return eroded_image
68/107:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/108:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
68/109:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
68/110:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
68/111:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/112:
# Define the custom transform
class PartialErosion:
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        # Convert the image to grayscale
        grayscale_image = color.rgb2gray(image)

        # Define structuring element
        selem = morphology.square(2)

        # Define number of iterations
        iterations = 7  # Change this to control the number of erosion iterations

        # Perform erosion for the specified number of iterations
        eroded_image = grayscale_image
        for i in range(iterations):
            eroded_image = morphology.erosion(eroded_image, selem)
        
        return eroded_image
68/113:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/114:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
68/115:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
68/116:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
68/117:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/118:
# Define the custom transform
class PartialErosion:
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        # Convert the image to grayscale
        grayscale_image = color.rgb2gray(image)

        # Define structuring element
        selem = morphology.square(2)

        # Define number of iterations
        iterations = 20  # Change this to control the number of erosion iterations

        # Perform erosion for the specified number of iterations
        eroded_image = grayscale_image
        for i in range(iterations):
            eroded_image = morphology.erosion(eroded_image, selem)
        
        return eroded_image
68/119:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/120:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
68/121:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
68/122:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
68/123:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/124:
# Define the custom transform
class PartialErosion:
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        
        # Convert the image to grayscale
        grayscale_image = color.rgb2gray(image)

        # Define structuring element
        selem = morphology.square(2)

        # Define number of iterations
        iterations = 5  # Change this to control the number of erosion iterations

        # Perform erosion for the specified number of iterations
        eroded_image = grayscale_image
        for i in range(iterations):
            eroded_image = morphology.erosion(eroded_image, selem)
        
        return eroded_image
68/125:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/126:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
68/127:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
68/128:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
68/129:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/130: len(train_loader)
68/131:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20

patience = 50
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            pass
#             counter += 1
#             # Check if the counter reaches the patience limit
#             if counter >= patience:
#                 print('Early stopping triggered...')
#                 break

print("\n=== Training complete! ===\n")
68/132:
class PartialErosion:
    def __init__(self, iterations=1, selem=square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        if isinstance(image, np.ndarray) and len(image.shape) == 3:
            result = np.zeros_like(image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(image.shape[2]):
                temp_image = image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
68/133:
class PartialErosion:
    def __init__(self, iterations=1, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        if isinstance(image, np.ndarray) and len(image.shape) == 3:
            result = np.zeros_like(image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(image.shape[2]):
                temp_image = image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
68/134:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/135:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
68/136:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
68/137:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
68/138:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/139:
class PartialErosion:
    def __init__(self, iterations=1, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
68/140:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/141:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
68/142:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
68/143:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
68/144:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/145:
class PartialErosion:
    def __init__(self, iterations=1, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
68/146:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/147:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
68/148:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
68/149:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
68/150:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
68/151:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20

patience = 50
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            pass
#             counter += 1
#             # Check if the counter reaches the patience limit
#             if counter >= patience:
#                 print('Early stopping triggered...')
#                 break

print("\n=== Training complete! ===\n")
68/152:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
68/153:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5], std=[0.5]),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
68/154:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
68/155:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
68/156:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
68/157:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
68/158: test_correct/len(test_loader.dataset)
68/159: # y_true_cpu, y_pred_cpu
68/160:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
68/161:
#  Image 8 misclassified most probably due to COCO-Text
len(test_loader.dataset)
68/162:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=5),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
68/163:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
68/164:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
68/165:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
68/166:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=5),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/167:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
68/168:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
68/169:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
68/170:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
68/171:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
68/172:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
68/173: test_correct/len(test_loader.dataset)
68/174: # y_true_cpu, y_pred_cpu
68/175:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
68/176:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
68/177:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
68/178:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
68/179:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
68/180:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
68/181: test_correct/len(test_loader.dataset)
68/182: # y_true_cpu, y_pred_cpu
68/183:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
68/184:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
68/185:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
68/186:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
68/187:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
68/188:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
68/189:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
68/190: test_correct/len(test_loader.dataset)
68/191: # y_true_cpu, y_pred_cpu
68/192:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
69/1:
# Define the custom transform
class SyntheticImagesTransform:
    
    # Define the custom transforms pipeline for longer images
    transforms_syn_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        
        transforms.Lambda(lambda orig_img: torch.stack([transforms.RandomCrop((100, 300))(orig_img) for _ in range(5)]))
    ])
    
    # Define the custom transforms pipeline for shorter images
    transforms_syn_words = transforms.Compose([
        transforms.ToTensor(),
#         transforms.GaussianBlur((3, 3), sigma=(0.1, 0.1)),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        width, height = image.size
        aspect_ratio = width/height
        
        if(aspect_ratio >= 5): 
            t = SyntheticImagesTransform.transforms_syn_lines(image)
        else:
            t = SyntheticImagesTransform.transforms_syn_words(image)
        
        return t
69/2:
output_dir = "data/all_preprocessed_data/typed/"
directory = 'data/synthetic_font_data/output/'  

for img_path in synthetic_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = SyntheticImagesTransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    except RuntimeError:
        # to exclude images smaller than the kernel used for blurring.
        continue
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
#         img_name = img_path.split("/")[-1][:-4]
#         temp = img_path.split("/")[-2]
#         output_file_path = output_dir + "synthetic" + temp + "-" + img_name + str(count) +'.jpg'
        
#         if os.path.exists(output_file_path):
#             continue
#         # Save the image
#         tensor_to_image.save(output_file_path)
#         count += 1
69/3:
import os
import json
import re
import sys
import shutil
import csv
import glob

from PIL import Image, UnidentifiedImageError
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine, adjust_sharpness

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
from craft_text_detector import Craft
from tqdm import tqdm
69/4:
# Define the custom transform
class SyntheticImagesTransform:
    
    # Define the custom transforms pipeline for longer images
    transforms_syn_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        
        transforms.Lambda(lambda orig_img: torch.stack([transforms.RandomCrop((100, 300))(orig_img) for _ in range(5)]))
    ])
    
    # Define the custom transforms pipeline for shorter images
    transforms_syn_words = transforms.Compose([
        transforms.ToTensor(),
#         transforms.GaussianBlur((3, 3), sigma=(0.1, 0.1)),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        width, height = image.size
        aspect_ratio = width/height
        
        if(aspect_ratio >= 5): 
            t = SyntheticImagesTransform.transforms_syn_lines(image)
        else:
            t = SyntheticImagesTransform.transforms_syn_words(image)
        
        return t
69/5:
dir_craft = "data/synthetic_font_data/output"
synthetic_img_paths = []

for root, _ , files in os.walk(dir_craft):
    for file in files:
        synthetic_img_paths.append(os.path.join(root, file))
69/6: len(synthetic_img_paths)
69/7: synthetic_img_paths[0]
69/8:
output_dir = "data/all_preprocessed_data/typed/"
directory = 'data/synthetic_font_data/output/'  

for img_path in synthetic_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = SyntheticImagesTransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    except RuntimeError:
        # to exclude images smaller than the kernel used for blurring.
        continue
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
#         img_name = img_path.split("/")[-1][:-4]
#         temp = img_path.split("/")[-2]
#         output_file_path = output_dir + "synthetic" + temp + "-" + img_name + str(count) +'.jpg'
        
#         if os.path.exists(output_file_path):
#             continue
#         # Save the image
#         tensor_to_image.save(output_file_path)
#         count += 1
69/9:
# Define the custom transform
class SyntheticImagesTransform:
    
    # Define the custom transforms pipeline for longer images
    transforms_syn_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize([100, ]),
        transforms.FiveCrop((100, 300)),
        
        transforms.Lambda(lambda orig_img: torch.stack([transforms.RandomCrop(size=(100, 300))(orig_img) for _ in range(5)]))
    ])
    
    # Define the custom transforms pipeline for shorter images
    transforms_syn_words = transforms.Compose([
        transforms.ToTensor(),
#         transforms.GaussianBlur((3, 3), sigma=(0.1, 0.1)),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        width, height = image.size
        aspect_ratio = width/height
        
        if(aspect_ratio >= 5): 
            t = SyntheticImagesTransform.transforms_syn_lines(image)
        else:
            t = SyntheticImagesTransform.transforms_syn_words(image)
        
        return t
69/10:
output_dir = "data/all_preprocessed_data/typed/"
directory = 'data/synthetic_font_data/output/'  

for img_path in synthetic_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = SyntheticImagesTransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    except RuntimeError:
        # to exclude images smaller than the kernel used for blurring.
        continue
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
#         img_name = img_path.split("/")[-1][:-4]
#         temp = img_path.split("/")[-2]
#         output_file_path = output_dir + "synthetic" + temp + "-" + img_name + str(count) +'.jpg'
        
#         if os.path.exists(output_file_path):
#             continue
#         # Save the image
#         tensor_to_image.save(output_file_path)
#         count += 1
69/11:
# Define the custom transform
class SyntheticImagesTransform:
    
    # Define the custom transforms pipeline for longer images
    transforms_syn_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize([100, ]),
#         transforms.FiveCrop((100, 300)),
        
        transforms.Lambda(lambda orig_img: torch.stack([transforms.RandomCrop(size=(100, 300))(orig_img) for _ in range(5)]))
    ])
    
    # Define the custom transforms pipeline for shorter images
    transforms_syn_words = transforms.Compose([
        transforms.ToTensor(),
#         transforms.GaussianBlur((3, 3), sigma=(0.1, 0.1)),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        width, height = image.size
        aspect_ratio = width/height
        
        if(aspect_ratio >= 5): 
            t = SyntheticImagesTransform.transforms_syn_lines(image)
        else:
            t = SyntheticImagesTransform.transforms_syn_words(image)
        
        return t
69/12:
output_dir = "data/all_preprocessed_data/typed/"
directory = 'data/synthetic_font_data/output/'  

for img_path in synthetic_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = SyntheticImagesTransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    except RuntimeError:
        # to exclude images smaller than the kernel used for blurring.
        continue
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
#         img_name = img_path.split("/")[-1][:-4]
#         temp = img_path.split("/")[-2]
#         output_file_path = output_dir + "synthetic" + temp + "-" + img_name + str(count) +'.jpg'
        
#         if os.path.exists(output_file_path):
#             continue
#         # Save the image
#         tensor_to_image.save(output_file_path)
#         count += 1
69/13:
# output_dir = "data/all_preprocessed_data/typed/"
output_dir = "./"
directory = 'data/synthetic_font_data/output/'  

for img_path in synthetic_img_paths[0:2]:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = SyntheticImagesTransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    except RuntimeError:
        # to exclude images smaller than the kernel used for blurring.
        continue
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        temp = img_path.split("/")[-2]
        output_file_path = output_dir + "synthetic" + temp + "-" + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
69/14:
# Define the custom transform
class SyntheticImagesTransform:
    
    # Define the custom transforms pipeline for longer images
    transforms_syn_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize([100, ]),        
        transforms.Lambda(lambda orig_img: 
                          torch.stack([transforms.RandomCrop(size=(100, 300))(orig_img) 
                          for _ in range(5)]))
    ])
    
    # Define the custom transforms pipeline for shorter images
    transforms_syn_words = transforms.Compose([
        transforms.ToTensor(),
#         transforms.GaussianBlur((3, 3), sigma=(0.1, 0.1)),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        width, height = image.size
        aspect_ratio = width/height
        
        if(aspect_ratio >= 5): 
            t = SyntheticImagesTransform.transforms_syn_lines(image)
        else:
            t = SyntheticImagesTransform.transforms_syn_words(image)
        
        return t
69/15:
# Batch 2
! gdown https://drive.google.com/uc?id=1sms0wqQFXo6Jlhs0dcyLjRGrhNEavt6F
! mkdir data/synthetic_font_data_2
! unzip -q data/synthetic-font-images-2.zip -d data/synthetic_font_data_2/
69/16:
# Batch 2
! gdown https://drive.google.com/uc?id=1sms0wqQFXo6Jlhs0dcyLjRGrhNEavt6F
# ! mkdir data/synthetic_font_data_2
! unzip -q data/synthetic-font-images-2.zip -d data/synthetic_font_data_2/
69/17:
# Batch 2
! gdown https://drive.google.com/uc?id=1kI1MMWj4qGB7Ex0RP7uX8lJfIi9t5EMi
# ! mkdir data/synthetic_font_data_2
! unzip -q data/synthetic-font-images-2.zip -d data/synthetic_font_data_2/
69/18:
# Batch 2
! gdown --folder data/ https://drive.google.com/uc?id=1kI1MMWj4qGB7Ex0RP7uX8lJfIi9t5EMi
# ! mkdir data/synthetic_font_data_2
! unzip -q data/synthetic-font-images-2.zip -d data/synthetic_font_data_2/
69/19:
# Batch 2
! gdown --folder=data/ https://drive.google.com/uc?id=1kI1MMWj4qGB7Ex0RP7uX8lJfIi9t5EMi
# ! mkdir data/synthetic_font_data_2
! unzip -q data/synthetic-font-images-2.zip -d data/synthetic_font_data_2/
69/20:
# Batch 2
! gdown --O data/ https://drive.google.com/uc?id=1kI1MMWj4qGB7Ex0RP7uX8lJfIi9t5EMi
# ! mkdir data/synthetic_font_data_2
! unzip -q data/synthetic-font-images-2.zip -d data/synthetic_font_data_2/
69/21:
# Batch 2
! gdown -O data/ https://drive.google.com/uc?id=1kI1MMWj4qGB7Ex0RP7uX8lJfIi9t5EMi
# ! mkdir data/synthetic_font_data_2
! unzip -q data/synthetic-font-images-2.zip -d data/synthetic_font_data_2/
69/22:
# Batch 2
! gdown -O data/ https://drive.google.com/uc?id=1kI1MMWj4qGB7Ex0RP7uX8lJfIi9t5EMi
# ! mkdir data/synthetic_font_data_2
! unzip -q data/synthetic-font-images-2.zip -d data/synthetic_font_data_2/
69/23:
output_dir_craft = "data/synthetic_font_data_2/output"
workdir = "data/synthetic_font_data_2/images/"

# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft, 
              export_extra = False, 
              text_threshold = .7, 
              link_threshold = .4, 
              crop_type="poly", 
              low_text = .3, 
              cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
    
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
69/24:
output_dir_craft = "data/synthetic_font_data_2/output"
workdir = "data/synthetic_font_data_2/images/"

# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft, 
              export_extra = False, 
              text_threshold = .7, 
              link_threshold = .4, 
              crop_type="poly", 
              low_text = .3, 
              cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
    
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
73/1:
import os
import json
import re
import sys
import shutil
import csv
import glob

from PIL import Image, UnidentifiedImageError
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine, adjust_sharpness

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
from craft_text_detector import Craft
from tqdm import tqdm
73/2:
output_dir_craft = "data/synthetic_font_data_2/output"
workdir = "data/synthetic_font_data_2/images/"

# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft, 
              export_extra = False, 
              text_threshold = .7, 
              link_threshold = .4, 
              crop_type="poly", 
              low_text = .3, 
              cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
    
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
73/3:
dir_craft = "data/synthetic_font_data_2/output"
synthetic_img_paths = []

for root, _ , files in os.walk(dir_craft):
    for file in files:
        synthetic_img_paths.append(os.path.join(root, file))
73/4: len(synthetic_img_paths)
73/5: synthetic_img_paths[0]
73/6:
# Define the custom transform
class SyntheticImagesTransform:
    
    # Define the custom transforms pipeline for longer images
    transforms_syn_lines = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize([100, ]),        
        transforms.Lambda(lambda orig_img: 
                          torch.stack([transforms.RandomCrop(size=(100, 300))(orig_img) 
                          for _ in range(5)]))
    ])
    
    # Define the custom transforms pipeline for shorter images
    transforms_syn_words = transforms.Compose([
        transforms.ToTensor(),
#         transforms.GaussianBlur((3, 3), sigma=(0.1, 0.1)),
        transforms.Resize([100, ]),
    ])
    
    def __init__(self):
        pass
    
    def __call__(self, image):
        width, height = image.size
        aspect_ratio = width/height
        
        if(aspect_ratio >= 5): 
            t = SyntheticImagesTransform.transforms_syn_lines(image)
        else:
            t = SyntheticImagesTransform.transforms_syn_words(image)
        
        return t
73/7:
output_dir = "data/all_preprocessed_data/typed/"
# output_dir = "./"
directory = 'data/synthetic_font_data_2/output/'  

for img_path in synthetic_img_paths:
    count = 0
    
    try:
        # Open image
        img = Image.open(img_path)
    except UnidentifiedImageError:
        continue
    
    # Apply transform
    try:
        t = SyntheticImagesTransform()(img)
    except ValueError:
        t = IAMTransform(True)(img)
    except RuntimeError:
        # to exclude images smaller than the kernel used for blurring.
        continue
    
    for tensor in t:
        tensor_to_image = transforms.ToPILImage()(tensor)
        img_name = img_path.split("/")[-1][:-4]
        temp = img_path.split("/")[-2]
        output_file_path = output_dir + "synthetic" + temp + "-" + img_name + str(count) +'.jpg'
        
        if os.path.exists(output_file_path):
            continue
        # Save the image
        tensor_to_image.save(output_file_path)
        count += 1
74/1:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
74/2:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
74/3:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
74/4:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
74/5:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
74/6:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
74/7:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
74/8:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
74/9:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
74/10: len(train_loader)
74/11:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20

patience = 50
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
75/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
75/2:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
75/3:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
75/4:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
75/5:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
75/6:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
75/7:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
75/8:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
75/9: len(train_loader)
75/10:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20

patience = 50
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
75/11:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
75/12:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/13:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/14:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/15:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/16:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/17: test_correct/len(test_loader.dataset)
75/18: # y_true_cpu, y_pred_cpu
75/19:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/20: torch.save(model.state_dict(), 'model/densenet_last_epoch_model.pth')
75/21:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_last_epoch_model.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
75/22:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/23:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/24:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/25:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/26:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/27: test_correct/len(test_loader.dataset)
75/28: # y_true_cpu, y_pred_cpu
75/29:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/30:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/31:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/32:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/33:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/34:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/35:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=3),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/36:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/37:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/38:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/39:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/40: test_correct/len(test_loader.dataset)
75/41: # y_true_cpu, y_pred_cpu
75/42:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/43:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/44:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/45:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/46:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/47:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/48: test_correct/len(test_loader.dataset)
75/49: # y_true_cpu, y_pred_cpu
75/50:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 3  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20

patience = 50
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
75/51:
# Define the percentage of samples for the validation set
val_split = 0.9  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
75/52:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
75/53:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
75/54: len(train_loader)
75/55:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 3  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 20

patience = 50
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
75/56: torch.save(model.state_dict(), 'model/densenet_last_epoch_model.pth')
75/57:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_last_epoch_model.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
75/58:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/59:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/60:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/61:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/62:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/63: test_correct/len(test_loader.dataset)
75/64: # y_true_cpu, y_pred_cpu
75/65:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/66:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
75/67:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
75/68:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
75/69: len(train_loader)
75/70:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 3  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
76/1:
import os
import json
import re
import sys
import shutil
import csv
import glob

from PIL import Image, UnidentifiedImageError
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine, adjust_sharpness

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
from craft_text_detector import Craft
from tqdm import tqdm
75/71:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter
75/72:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler

! pip install tensorboard jupyter-tensorboard
! jupyter nbextension enable --py jupyter_tensorboard
from torch.utils.tensorboard import SummaryWriter
75/73:
# ! jupyter nbextension enable --py jupyter_tensorboard
%load_ext tensorboard
%tensorboard --log-dir logs
75/74:
# ! jupyter nbextension enable --py jupyter_tensorboard
# %load_ext tensorboard
%tensorboard --logdir logs
75/75:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler

# ! pip install tensorboard jupyter-tensorboard
from torch.utils.tensorboard import SummaryWriter
75/76:
# ! jupyter nbextension enable --py jupyter_tensorboard
# %load_ext tensorboard
%tensorboard --logdir logs
75/77:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
75/78:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
75/79:
# Convert transforms to JSON string
transforms_json_train = json.dumps(preprocess.transforms, indent=4)
print(transforms_json_train)
75/80:
# Convert transforms to JSON string
# transforms_json_train = json.dumps(preprocess.transforms, indent=4)
print(preprocess.transforms)
75/81:
# Convert transforms to JSON string
transforms_json_train = json.dumps(preprocess.transforms, indent=4)
print(preprocess.transforms)
75/82:
# Convert transforms to JSON string
# transforms_json_train = json.dumps(preprocess.transforms, indent=4)
print(preprocess.transforms)
75/83:
# Convert transforms to JSON string
transforms_json_train = json.dumps(preprocess.transforms, indent=4)
print(preprocess.transforms)
75/84:
## Convert transforms to JSON string

# Extract relevant information from transforms
transform_info = []
for t in preprocess.transforms:
    transform_info.append({
        'name': t.__class__.__name__,
        'args': t.__dict__.get('__dict__'),
        'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
    })

# Convert transform information to JSON string
transform_json = json.dumps(transform_info, indent=4)

print(transform_json)
75/85:
# Convert transforms to JSON string
transforms_json_train = json.dumps(preprocess.transforms, indent=4)
print(str(preprocess.transforms))

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
75/86:
# Convert transforms to JSON string
# transforms_json_train = json.dumps(preprocess.transforms, indent=4)
print(str(preprocess.transforms))

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
75/87:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(str(preprocess.transforms))

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
75/88:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
75/89:
# Define the percentage of samples for the validation set
val_split = 0.01  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
75/90:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
75/91:
# Define the percentage of samples for the validation set
val_split = 0.99  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
75/92:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
75/93:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
75/94: len(train_loader)
75/95:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 3  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer.as_default():
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)

            # Write the parameters used for training
            writer.add_hparams(
                {'num_epochs': num_epochs, 'batch_size': batch_size, 'learning_rate': lr}
            )
            
            writer.add_text("train_preprocess", transforms_json_train)
            
            writer.close()

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
75/96:
# Define the percentage of samples for the validation set
val_split = 0.99  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices[0:100])

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
75/97:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
75/98:
len(train_loader)
len(val_loader)
75/99:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 3  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer.as_default():
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)

            # Write the parameters used for training
            writer.add_hparams(
                {'num_epochs': num_epochs, 'batch_size': batch_size, 'learning_rate': lr}
            )
            
            writer.add_text("train_preprocess", transforms_json_train)
            
            writer.close()

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
75/100:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 3  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)

            # Write the parameters used for training
            writer.add_hparams(
                {'num_epochs': num_epochs, 'batch_size': batch_size, 'learning_rate': lr}
            )
            
            writer.add_text("train_preprocess", transforms_json_train)
            
            writer.close()

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
75/101:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 3  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)

            # Write the parameters used for training
            writer.add_hparams(
                {
                    'num_epochs': num_epochs, 
                     'batch_size': batch_size, 
#                      'learning_rate': lr
                }
            )
            
            writer.add_text("train_preprocess", transforms_json_train)
            
            writer.close()

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
75/102:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 3  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = {
                    'num_epochs': num_epochs, 
                     'batch_size': batch_size, 
#                      'learning_rate': lr
                }
            )
            
            writer.add_text("train_preprocess", transforms_json_train)
            
            writer.close()

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
75/103:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 3  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)
            
            writer.close()

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
75/104:
# ! jupyter nbextension enable --py jupyter_tensorboard
# %load_ext tensorboard
75/105: %tensorboard --logdir logs
75/106: !tensorboard --logdir logs
75/107: !tensorboard --logdir logs
75/108: %tensorboard --logdir logs
75/109: %tensorboard --logdir logs
75/110: # %tensorboard --logdir logs
75/111:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
75/112:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
75/113:
print(len(train_loader),
len(val_loader))
75/114:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 3  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)
            
            writer.close()

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
75/115:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 4  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)
            
            writer.close()

        # Save the best model based on validation loss and early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_not_best_model.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
75/116:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_not_best_model.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
75/117:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/118:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/119:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/120:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/121:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/122: test_correct/len(test_loader.dataset)
75/123: # y_true_cpu, y_pred_cpu
75/124:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/125:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/126:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/127:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/128:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/129:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/130: test_correct/len(test_loader.dataset)
75/131: # y_true_cpu, y_pred_cpu
75/132:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/133:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
75/134:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
75/135:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
75/136:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
75/137:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
75/138:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
75/139:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(2)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
75/140:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
75/141:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
75/142:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
75/143:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
75/144:
# Define the percentage of samples for the validation set
val_split = 0.2  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
75/145:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
75/146:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
75/147:
print(len(train_loader),
len(val_loader))
75/148: # %tensorboard --logdir logs
75/149: torch.save(model.state_dict(), 'model/densenet_4_layers_unlocked.pth')
75/150:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 3  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)
            
            writer.close()

        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_3_layers_unlocked.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
75/151:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_3_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
75/152:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/153:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/154:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/155:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/156:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/157: test_correct/len(test_loader.dataset)
75/158: # y_true_cpu, y_pred_cpu
75/159:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/num_rows), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/num_rows)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/160:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/161:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
75/162:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
75/163:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
75/164:
print(len(train_loader),
len(val_loader))
75/165: # %tensorboard --logdir logs
75/166:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 5  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)
            
            writer.close()

        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_3_layers_unlocked.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
75/167:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 5  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)
            
            writer.close()

        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_5_layers_unlocked.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
75/168:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_5_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
75/169:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/170:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/171:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/172:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/173:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/174: test_correct/len(test_loader.dataset)
75/175: # y_true_cpu, y_pred_cpu
75/176:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/177:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_5_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
75/178:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/179:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/180:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/181:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/182:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/183: test_correct/len(test_loader.dataset)
75/184: # y_true_cpu, y_pred_cpu
75/185:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/186:
#  Image 8 misclassified most probably due to COCO-Text
len(test_loader.dataset)
75/187:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_5_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
75/188:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/189:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/190:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/191:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/192:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/193: test_correct/len(test_loader.dataset)
75/194: # y_true_cpu, y_pred_cpu
75/195:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/196:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 4  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_5_layers_unlocked.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
75/197:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_4_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
75/198:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/199:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/200:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/201:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/202:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/203: test_correct/len(test_loader.dataset)
75/204: # y_true_cpu, y_pred_cpu
75/205:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/206:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/207:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/208:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/209:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/210:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/211: test_correct/len(test_loader.dataset)
75/212: # y_true_cpu, y_pred_cpu
75/213:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/214:
#  Image 8 misclassified most probably due to COCO-Text
len(test_loader.dataset)
75/215:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_3_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
75/216:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/217:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/218:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/219:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/220:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/221: test_correct/len(test_loader.dataset)
75/222: # y_true_cpu, y_pred_cpu
75/223:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/224:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_4_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
75/225:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     transforms.RandomHorizontalFlip(),
#     transforms.RandomVerticalFlip(),
#     transforms.RandomRotation(degrees=90),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
# #     transforms.Normalize(mean=[0.5], std=[0.5]),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    
    PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/226:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/227:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/228:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/229:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/230: test_correct/len(test_loader.dataset)
75/231: # y_true_cpu, y_pred_cpu
75/232:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/233:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
75/234:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_4_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
75/235:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/236:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/237:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/238:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/239:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/240: test_correct/len(test_loader.dataset)
75/241: # y_true_cpu, y_pred_cpu
75/242:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/243:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_3_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
75/244:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/245:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/246:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/247:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/248:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/249: test_correct/len(test_loader.dataset)
75/250: # y_true_cpu, y_pred_cpu
75/251:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/252:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_5_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
75/253:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
75/254:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
75/255:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
75/256:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
75/257:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
75/258: test_correct/len(test_loader.dataset)
75/259: # y_true_cpu, y_pred_cpu
75/260:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
75/261:
#  Image 8 misclassified most probably due to COCO-Text
len(test_loader.dataset)
75/262:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
75/263:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
75/264:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
    
    def __str__(self,):
        return " iterations="+str(self.iterations)+", selem="+str(self.selem)
75/265:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
75/266:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
75/267:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
    
    def __str__(self):
        return f'iterations={self.iterations}+", selem={self.selem}'
75/268:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
75/269:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
75/270:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
    
    def __repr__(self):
        return f'iterations={self.iterations}+", selem={self.selem}'
75/271:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
75/272:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
75/273:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
75/274:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
    
    def __repr__(self):
        return f'PartialErosion(iterations={self.iterations}+", selem={self.selem})'
75/275:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
75/276:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
75/277:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
    
    def __repr__(self):
        return f'PartialErosion(iterations={self.iterations}, selem={self.selem})'
75/278:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
75/279:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
75/280:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
    
    def __str__(self):
        return f'PartialErosion(iterations={self.iterations}, selem={self.selem})'
75/281:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
75/282:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
75/283:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
    
    def __repr__(self):
        return f'PartialErosion(iterations={self.iterations}, selem={self.selem})'
75/284:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
75/285:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
75/286:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
    
    def __repr__(self):
        return f'PartialErosion(iterations={self.iterations}, selem={self.selem})'
75/287:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
75/288:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
75/289:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
75/290:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
75/291:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
75/292:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
75/293:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
75/294:
print(len(train_loader),
len(val_loader))
75/295: # %tensorboard --logdir logs
75/296:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 7  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_7_layers_unlocked.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad
from torch.utils.data import DataLoader, SubsetRandomSampler

# ! pip install tensorboard jupyter-tensorboard
from torch.utils.tensorboard import SummaryWriter
77/2:
# ! jupyter nbextension enable --py jupyter_tensorboard
# %load_ext tensorboard
77/3:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
    
    def __repr__(self):
        return f'PartialErosion(iterations={self.iterations}, selem={self.selem})'
77/4:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/5:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/6:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/7:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/8:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/9:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/10:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
77/11:
print(len(train_loader),
len(val_loader))
77/12: # %tensorboard --logdir logs
77/13:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_7_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/14:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/15:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/16:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/17:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/18:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/19: test_correct/len(test_loader.dataset)
77/20: # y_true_cpu, y_pred_cpu
77/21:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/22:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_5_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/23:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/24:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/25:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/26:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/27:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/28: test_correct/len(test_loader.dataset)
77/29: # y_true_cpu, y_pred_cpu
77/30:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/31:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 5  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_5_layers_unlocked_run_2.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/32:
class Skeletonize:
    def __init__(self):
        pass
    
    def __call__(self, image):
        # Convert image tensor to numpy array
        np_image = F.to_pil_image(image)
        np_image = np_image.convert("L")  # Convert to grayscale

        # Perform skeletonization
        skeleton_np = skeletonize(np_image)

        # Convert skeletonized image back to tensor
        skeleton_image = F.to_tensor(skeleton_np)

        return skeleton_image
77/33:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/34:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/35:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/36:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/37:
class Skeletonize:
    def __init__(self):
        pass
    
    def __call__(self, image):
        # Convert image tensor to numpy array
#         np_image = F.to_pil_image(image)
        np_image = image
        np_image = np_image.convert("L")  # Convert to grayscale

        # Perform skeletonization
        skeleton_np = skeletonize(np_image)

        # Convert skeletonized image back to tensor
        skeleton_image = F.to_tensor(skeleton_np)

        return skeleton_image
77/38:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/39:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/40:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/41:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/42:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/43:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/44:
class Skeletonize:
    def __init__(self):
        pass
    
    def __call__(self, image):
        # Convert image tensor to numpy array
#         np_image = F.to_pil_image(image)
        np_image = image
        np_image = np_image.convert("L")  # Convert to grayscale

        # Perform skeletonization
        skeleton_np = morphology.skeletonize(np_image)

        # Convert skeletonized image back to tensor
        skeleton_image = F.to_tensor(skeleton_np)

        return skeleton_image
77/45:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/46:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/47:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/48:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/49:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/50:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/51:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image
from torch.utils.data import DataLoader, SubsetRandomSampler

# ! pip install tensorboard jupyter-tensorboard
from torch.utils.tensorboard import SummaryWriter
77/52:
class Skeletonize:
    def __init__(self):
        pass
    
    def __call__(self, image):
        # Convert image tensor to numpy array
        np_image = to_pil_image(image)
        np_image = np_image.convert("L")  # Convert to grayscale

        # Perform skeletonization
        skeleton_np = morphology.skeletonize(np_image)

        # Convert skeletonized image back to tensor
        skeleton_image = F.to_tensor(skeleton_np)

        return skeleton_image
77/53:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/54:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/55:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/56:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/57:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/58:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/59:
class Skeletonize:
    def __init__(self):
        pass
    
    def __call__(self, image):
#         # Convert image tensor to numpy array
#         np_image = to_pil_image(image)
#         np_image = np_image.convert("L")  # Convert to grayscale

#         # Perform skeletonization
#         skeleton_np = morphology.skeletonize(np_image)

#         # Convert skeletonized image back to tensor
#         skeleton_image = F.to_tensor(skeleton_np)

#         return skeleton_image

        # Convert PIL image to ndarray
        np_image = to_tensor(image).numpy().squeeze(0)
        np_image = img_as_ubyte(np_image)

        # Perform skeletonization
        skeleton_np = skeletonize(np_image)

        # Convert skeletonized image back to tensor
        skeleton_image = torch.from_numpy(skeleton_np).unsqueeze(0).float()

        return skeleton_image
77/60:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/61:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/62:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/63:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/64:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/65:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/66:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

# ! pip install tensorboard jupyter-tensorboard
from torch.utils.tensorboard import SummaryWriter
77/67:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/68:
import numpy as np
from PIL import Image
from skimage import color
from skimage.morphology import skeletonize

# Load RGB image using PIL
image_pil = Image.open('image.jpg')

# Convert PIL image to grayscale
image_gray = image_pil.convert('L')

# Convert grayscale image to numpy array
image_np = np.array(image_gray)

# Perform skeletonization on grayscale image
skeleton = skeletonize(image_np)
77/69:
import numpy as np
from PIL import Image
from skimage import color
from skimage.morphology import skeletonize

# Load RGB image using PIL
image_pil = Image.open('data/test_data/typed/00000.png')

# Convert PIL image to grayscale
image_gray = image_pil.convert('L')

# Convert grayscale image to numpy array
image_np = np.array(image_gray)

# Perform skeletonization on grayscale image
skeleton = skeletonize(image_np)
77/70:
class Skeletonize:
    def __init__(self):
        pass
    
    def __call__(self, image):
        # Convert image tensor to numpy array
        np_image = to_pil_image(image)
        np_image = np_image.convert("L")  # Convert to grayscale

        # Perform skeletonization
        skeleton_np = morphology.skeletonize(np_image)

        # Convert skeletonized image back to tensor
        skeleton_image = to_tensor(skeleton_np)

        return skeleton_image
77/71:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/72:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/73:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/74:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/75:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/76:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/77:
class Skeletonize:
    def __init__(self):
        pass
    
    def __call__(self, image):
        # Convert image tensor to numpy array
#         np_image = to_pil_image(image)
        np_image = image.convert("L")  # Convert to grayscale

        # Perform skeletonization
        skeleton_np = morphology.skeletonize(np_image)

        # Convert skeletonized image back to tensor
        skeleton_image = to_tensor(skeleton_np)

        return skeleton_image
77/78:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/79:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/80:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/81:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/82:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/83:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/84:
class Skeletonize:
    def __init__(self):
        pass
    
    def __call__(self, image):
        # Convert image tensor to numpy array
        np_image = to_pil_image(image)
        np_image = np_image.convert("L")  # Convert to grayscale

        # Perform skeletonization
        skeleton_np = morphology.skeletonize(np_image)

        # Convert skeletonized image back to tensor
        skeleton_image = to_tensor(skeleton_np)

        return skeleton_image
77/85:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/86:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/87:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/88:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/89:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/90:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/91:
class Skeletonize:
    def __init__(self):
        pass
    
    def __call__(self, image):
        # Convert image tensor to numpy array
#         np_image = to_pil_image(image)
        np_image = image.convert("L")  # Convert to grayscale

        # Perform skeletonization
        skeleton_np = morphology.skeletonize(np_image)

        # Convert skeletonized image back to tensor
        skeleton_image = to_tensor(skeleton_np)

        return skeleton_image
77/92:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/93:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/94:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/95:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/96:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/97:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/98:
class Skeletonize:
    def __init__(self):
        pass
    
    def __call__(self, image):
        # Convert image tensor to numpy array
#         np_image = to_pil_image(image)
        np_image = image.convert("L")  # Convert to grayscale

        # Perform skeletonization
        skeleton_np = morphology.skeletonize(np.array(np_image))

        # Convert skeletonized image back to tensor
        skeleton_image = to_tensor(skeleton_np)

        return skeleton_image
77/99:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/100:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/101:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/102:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/103:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/104:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/105:
class Skeletonize:
    def __init__(self):
        pass
    
    def __call__(self, image):
        # Convert image tensor to numpy array
#         np_image = to_pil_image(image)
        np_image = image.convert("L")  # Convert to grayscale

        # Perform skeletonization
        skeleton_np = morphology.skeletonize(np.array(np_image))

        # Convert skeletonized image back to tensor
#         skeleton_image = to_tensor(skeleton_np)

        return skeleton_np
77/106:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/107:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/108:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/109:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/110:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/111:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/112:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/113:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/114:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/115:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/116:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/117:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/118:
class Skeletonize:
    def __init__(self):
        pass
    
    def __call__(self, image):
        # Convert image tensor to numpy array
#         np_image = to_pil_image(image)
        np_image = image.convert("L")  # Convert to grayscale

        # Perform skeletonization
        skeleton_np = morphology.skeletonize(np.array(np_image).astype(np.float32))

        # Convert skeletonized image back to tensor
#         skeleton_image = to_tensor(skeleton_np)

        return skeleton_np
77/119:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/120:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/121:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/122:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/123:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/124:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/125:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/126:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/127:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/128:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/129:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/130:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/131:
class Skeletonize:
    def __init__(self):
        pass
    
    def __call__(self, image):
        # Convert image tensor to numpy array
#         np_image = to_pil_image(image)
        image = image.convert("L")  # Convert to grayscale
        np_image = np.array(image).astype(np.float32)
        
        # Normalize the image to the range [0, 1]
        np_image /= 255.0

        # Perform skeletonization
        skeleton_np = morphology.skeletonize(np_image)

        # Convert skeletonized image back to PIL image
        skeleton_pil = Image.fromarray((skeleton_np * 255).astype(np.uint8))

        # Convert skeletonized image back to tensor
#         skeleton_image = to_tensor(skeleton_np)

        return skeleton_np
77/132:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/133:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/134:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/135:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/136:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/137:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/138:
class Skeletonize:
    def __init__(self):
        pass
    
    def __call__(self, image):
        # Convert image tensor to numpy array
#         np_image = to_pil_image(image)
        image = image.convert("L")  # Convert to grayscale
        np_image = np.array(image).astype(np.float32)
        
        # Normalize the image to the range [0, 1]
        np_image /= 255.0

        # Perform skeletonization
        skeleton_np = morphology.skeletonize(np_image)

        # Convert skeletonized image back to PIL image
        skeleton_pil = Image.fromarray((skeleton_np * 255).astype(np.uint8))

        # Convert skeletonized image back to tensor
#         skeleton_image = to_tensor(skeleton_np)

        return skeleton_pil
77/139:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/140:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/141:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/142:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/143:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/144:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/145:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
    Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/146:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/147:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/148:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/149:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/150:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/151:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/152:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
77/153:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/154:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/155:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/156:
print(len(train_loader),
len(val_loader))
77/157: # %tensorboard --logdir logs
77/158:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 5  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_5_layers_unlocked_skeleton.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/159:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/160:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/161:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/162:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/163:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/164:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/165:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/166:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
77/167:
print(len(train_loader),
len(val_loader))
77/168:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

num_layers_to_unfreeze = 7  # Specify the number of layers to unfreeze

for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_7_layers_unlocked.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/169:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_7_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/170:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/171:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/172:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/173:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/174:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/175: test_correct/len(test_loader.dataset)
77/176: # y_true_cpu, y_pred_cpu
77/177:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/178:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Get the parameters of the model
parameters = model.parameters()

# Specify the number of layers in the begining to unfreeze
num_layers_to_unfreeze_begin = 5

# Specify the number of layers in the end to unfreeze
num_layers_to_unfreeze_end = 5 


for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)
77/179:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Get the parameters of the model
parameters = model.parameters()

# Specify the number of layers in the begining to unfreeze
num_layers_to_unfreeze_begin = 5

# Specify the number of layers in the end to unfreeze
num_layers_to_unfreeze_end = 5



# Counter to keep track of the number of unlocked layers
unlocked_layers = 0

# Loop over the parameters and set requires_grad=True for the desired number of layers
for param in parameters:
    param.requires_grad = True
    unlocked_layers += 1
    if unlocked_layers == num_layers_to_unlock:
        break


for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

model
77/180:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Get the parameters of the model
parameters = model.parameters()

# Specify the number of layers in the begining to unfreeze
num_layers_to_unfreeze_begin = 5

# Specify the number of layers in the end to unfreeze
num_layers_to_unfreeze_end = 5



# Counter to keep track of the number of unlocked layers
unlocked_layers = 0

# Loop over the parameters and set requires_grad=True for the desired number of layers
for param in parameters:
    param.requires_grad = True
    unlocked_layers += 1
    if unlocked_layers == num_layers_to_unfreeze_end:
        break


for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

model
77/181:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Get the parameters of the model
parameters = model.parameters()

# Specify the number of layers in the begining to unfreeze
num_layers_to_unfreeze_begin = 5

# Specify the number of layers in the end to unfreeze
num_layers_to_unfreeze_end = 5



# Counter to keep track of the number of unlocked layers
unlocked_layers = 0

# Loop over the parameters and set requires_grad=True for the desired number of layers
for param in parameters:
    param.requires_grad = True
    unlocked_layers += 1
    if unlocked_layers == num_layers_to_unfreeze_end:
        break


for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

model
77/182:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Get the parameters of the model
parameters = model.parameters()

# Specify the number of layers in the begining to unfreeze
num_layers_to_unfreeze_begin = 5

# Specify the number of layers in the end to unfreeze
num_layers_to_unfreeze_end = 5



# Counter to keep track of the number of unlocked layers
unlocked_layers = 0

# Loop over the parameters and set requires_grad=True for the desired number of layers
for param in parameters:
    param.requires_grad = True
    unlocked_layers += 1
    if unlocked_layers == num_layers_to_unfreeze_end:
        break


for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False

# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# model
77/183:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_7_layers_unlocked.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/184:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_5_5_layers_unlocked.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/185:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Get the parameters of the model
parameters = model.parameters()

# Specify the number of layers in the begining to unfreeze
num_layers_to_unfreeze_begin = 5

# Specify the number of layers in the end to unfreeze
num_layers_to_unfreeze_end = 5

# Unlock/unfreeze the last few layers by freezing all the remaining layers
for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False


# Now we unfreeze a few layers in the beginning to help us capture low level features
# Counter to keep track of the number of unlocked layers
unlocked_layers = 0

# Loop over the parameters and set requires_grad=True for the desired number of layers
for param in parameters:
    param.requires_grad = True
    unlocked_layers += 1
    if unlocked_layers == num_layers_to_unfreeze_end:
        break
    
    
# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# model
77/186:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_5_5_layers_unlocked.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/187:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_5_5_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/188:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/189:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/190:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/191:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/192:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/193: test_correct/len(test_loader.dataset)
77/194: # y_true_cpu, y_pred_cpu
77/195:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/196:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/197:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/198:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/199:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/200:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/201: test_correct/len(test_loader.dataset)
77/202: # y_true_cpu, y_pred_cpu
77/203:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/204:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
77/205:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Get the parameters of the model
parameters = model.parameters()

# Specify the number of layers in the begining to unfreeze
num_layers_to_unfreeze_begin = 10

# Specify the number of layers in the end to unfreeze
num_layers_to_unfreeze_end = 5

# Unlock/unfreeze the last few layers by freezing all the remaining layers
for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False


# Now we unfreeze a few layers in the beginning to help us capture low level features
# Counter to keep track of the number of unlocked layers
unlocked_layers = 0

# Loop over the parameters and set requires_grad=True for the desired number of layers
for param in parameters:
    param.requires_grad = True
    unlocked_layers += 1
    if unlocked_layers == num_layers_to_unfreeze_end:
        break
    
    
# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# model
77/206:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
77/207:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_10_5_layers_unlocked.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/208:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_5_5_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/209:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/210:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/211:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/212:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/213:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/214:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/215:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/216: test_correct/len(test_loader.dataset)
77/217: # y_true_cpu, y_pred_cpu
77/218:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/219:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/220:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/221:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/222:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/223:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/224:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/225: test_correct/len(test_loader.dataset)
77/226: # y_true_cpu, y_pred_cpu
77/227:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/228:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/229:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/230:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/231:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/232:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/233:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/234: test_correct/len(test_loader.dataset)
77/235: # y_true_cpu, y_pred_cpu
77/236:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/237:
#  Image 8 misclassified most probably due to COCO-Text
len(test_loader.dataset)
77/238:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/239:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/240:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/241:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/242:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/243:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/244: test_correct/len(test_loader.dataset)
77/245: # y_true_cpu, y_pred_cpu
77/246:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/247:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/248:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/249:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/250:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/251:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/252:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/253:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
77/254:
print(len(train_loader),
len(val_loader))
77/255: # %tensorboard --logdir logs
77/256:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Get the parameters of the model
parameters = model.parameters()

# Specify the number of layers in the begining to unfreeze
num_layers_to_unfreeze_begin = 10

# Specify the number of layers in the end to unfreeze
num_layers_to_unfreeze_end = 5

# Unlock/unfreeze the last few layers by freezing all the remaining layers
for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False


# Now we unfreeze a few layers in the beginning to help us capture low level features
# Counter to keep track of the number of unlocked layers
unlocked_layers = 0

# Loop over the parameters and set requires_grad=True for the desired number of layers
for param in parameters:
    param.requires_grad = True
    unlocked_layers += 1
    if unlocked_layers == num_layers_to_unfreeze_end:
        break
    
    
# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# model
77/257:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
77/258:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_10_5_layers_unlocked_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/259:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/260:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/261:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/262:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/263:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/264:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/265:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/266:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/267:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/268:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/269: test_correct/len(test_loader.dataset)
77/270: # y_true_cpu, y_pred_cpu
77/271:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/272:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/273:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/274:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/275:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/276:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/277: test_correct/len(test_loader.dataset)
77/278: # y_true_cpu, y_pred_cpu
77/279:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/280:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/281:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/282:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/283:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/284:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/285: test_correct/len(test_loader.dataset)
77/286: # y_true_cpu, y_pred_cpu
77/287:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/288:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Get the parameters of the model
parameters = model.parameters()

# Specify the number of layers in the begining to unfreeze
num_layers_to_unfreeze_begin = 10

# Specify the number of layers in the end to unfreeze
num_layers_to_unfreeze_end = 5

# Unlock/unfreeze the last few layers by freezing all the remaining layers
for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False


# Now we unfreeze a few layers in the beginning to help us capture low level features
# Counter to keep track of the number of unlocked layers
unlocked_layers = 0

# Loop over the parameters and set requires_grad=True for the desired number of layers
for param in parameters:
    param.requires_grad = True
    unlocked_layers += 1
    if unlocked_layers == num_layers_to_unfreeze_end:
        break
    
    
# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)

model = model.to(device)

# model
77/289:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
77/290:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_10_5_layers_unlocked_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/291:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Get the parameters of the model
parameters = model.parameters()

# Specify the number of layers in the begining to unfreeze
num_layers_to_unfreeze_begin = 25

# Specify the number of layers in the end to unfreeze
num_layers_to_unfreeze_end = 10

# Unlock/unfreeze the last few layers by freezing all the remaining layers
for param in model.features[:-num_layers_to_unfreeze].parameters():
    param.requires_grad = False


# Now we unfreeze a few layers in the beginning to help us capture low level features
# Counter to keep track of the number of unlocked layers
unlocked_layers = 0

# Loop over the parameters and set requires_grad=True for the desired number of layers
for param in parameters:
    param.requires_grad = True
    unlocked_layers += 1
    if unlocked_layers == num_layers_to_unfreeze_end:
        break
    
    
# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)

model = model.to(device)

# model
77/292:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
77/293:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# Load pre-trained DenseNet model
model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# Get the parameters of the model
parameters = model.parameters()

# Specify the number of layers in the begining to unfreeze
num_layers_to_unfreeze_begin = 25

# Specify the number of layers in the end to unfreeze
num_layers_to_unfreeze_end = 10

# Unlock/unfreeze the last few layers by freezing all the remaining layers
for param in model.features[:-num_layers_to_unfreeze_end].parameters():
    param.requires_grad = False


# Now we unfreeze a few layers in the beginning to help us capture low level features
# Counter to keep track of the number of unlocked layers
unlocked_layers = 0

# Loop over the parameters and set requires_grad=True for the desired number of layers
for param in parameters:
    param.requires_grad = True
    unlocked_layers += 1
    if unlocked_layers == num_layers_to_unfreeze_begin:
        break
    
    
# Replace the last layer for binary classification
model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)

model = model.to(device)

# model
77/294:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
77/295:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_10_5_layers_unlocked_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/296:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_25_10_layers_unlocked_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/297:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# VGG model from scratch
model = VGG16Binary(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)

model = model.to(device)

# model
77/298:
class VGG16Binary(nn.Module):
    def __init__(self, num_classes=1):
        super(VGG16Binary, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, num_classes),
            nn.Sigmoid()  # Apply sigmoid activation for binary classification
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
77/299:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# VGG model from scratch
model = VGG16Binary(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)

model = model.to(device)

# model
77/300:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
77/301:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_25_10_layers_unlocked_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/302:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# VGG model from scratch
input_shape = (3, 100, 200)  # Example input shape (channels, height, width)

model = VGG16Binary(input_shape=input_shape, num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)

model = model.to(device)

# model
77/303:
class VGG16Binary(nn.Module):
    def __init__(self, input_shape, num_classes=1):
        super(VGG16Binary, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(input_shape[0], 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.classifier = nn.Sequential(
            nn.Linear(512 * (input_shape[1] // 32) * (input_shape[2] // 32), 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, num_classes),
            nn.Sigmoid()  # Apply sigmoid activation for binary classification
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
77/304:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# VGG model from scratch
input_shape = (3, 100, 200)  # Example input shape (channels, height, width)

model = VGG16Binary(input_shape=input_shape, num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)

model = model.to(device)

# model
77/305:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
77/306:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_25_10_layers_unlocked_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/307:
class VGG16Binary(nn.Module):
    def __init__(self, input_shape, num_classes=1):
        super(VGG16Binary, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(input_shape[0], 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.classifier = nn.Sequential(
            nn.Linear(512 * 6 * 12, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, num_classes),
            nn.Sigmoid()  # Apply sigmoid activation for binary classification
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
77/308:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# VGG model from scratch
input_shape = (3, 100, 200)  # input shape (channels, height, width)

model = VGG16Binary(input_shape=input_shape, num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)

model = model.to(device)

# model
77/309:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
77/310:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_vgg_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/311:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# VGG model from scratch
input_shape = (3, 100, 200)  # input shape (channels, height, width)

model = VGG16Binary(input_shape=input_shape, num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# model
77/312:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/313:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/314:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/315:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/316:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/317:
# Define the percentage of samples for the validation set
val_split = 0.3  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/318:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/319:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
77/320:
print(len(train_loader),
len(val_loader))
77/321: # %tensorboard --logdir logs
77/322:
class VGG16Binary(nn.Module):
    def __init__(self, input_shape, num_classes=1):
        super(VGG16Binary, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(input_shape[0], 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.classifier = nn.Sequential(
            nn.Linear(512 * 6 * 12, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, num_classes),
            nn.Sigmoid()  # Apply sigmoid activation for binary classification
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
77/323:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# VGG model from scratch
input_shape = (3, 100, 200)  # input shape (channels, height, width)

model = VGG16Binary(input_shape=input_shape, num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# model
77/324:
class VGG16Binary(nn.Module):
    def __init__(self, input_shape, num_classes=1):
        super(VGG16Binary, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(input_shape[0], 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.classifier = nn.Sequential(
            nn.Linear(512 * 6 * 12, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, num_classes),
#             nn.Sigmoid()  # Apply sigmoid activation for binary classification
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
77/325:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# VGG model from scratch
input_shape = (3, 100, 200)  # input shape (channels, height, width)

model = VGG16Binary(input_shape=input_shape, num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)

# model
77/326:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
77/327:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_vgg_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/328:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_vgg_from_scratch_no_erosion.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/329:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/330:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

best_model.load_state_dict(torch.load('model/densenet_vgg_from_scratch_no_erosion.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/331:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_vgg_from_scratch_no_erosion.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/332:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/333:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/334:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/335:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/336:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/337: test_correct/len(test_loader.dataset)
77/338: # y_true_cpu, y_pred_cpu
77/339:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/340:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/341:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/342:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/343:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/344: test_correct/len(test_loader.dataset)
77/345: # y_true_cpu, y_pred_cpu
77/346:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/347:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 300)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/348:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/349:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/350:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/351:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/352:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 247)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/353:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/354:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/355:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/356:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/357:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/358:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/359:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/360:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/361: test_correct/len(test_loader.dataset)
77/362:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/363:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(5)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
    
    def __repr__(self):
        return f'PartialErosion(iterations={self.iterations}, selem={self.selem})'
77/364:
# class Skeletonize:
#     def __init__(self):
#         pass
    
#     def __call__(self, image):
#         # Convert image tensor to numpy array
# #         np_image = to_pil_image(image)
#         image = image.convert("L")  # Convert to grayscale
#         np_image = np.array(image).astype(np.float32)
        
#         # Normalize the image to the range [0, 1]
#         np_image /= 255.0

#         # Perform skeletonization
#         skeleton_np = morphology.skeletonize(np_image)

#         # Convert skeletonized image back to PIL image
#         skeleton_pil = Image.fromarray((skeleton_np * 255).astype(np.uint8))

#         # Convert skeletonized image back to tensor
# #         skeleton_image = to_tensor(skeleton_np)

#         return skeleton_pil
77/365:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/366:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/367:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/368:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/369:
# Define the percentage of samples for the validation set
val_split = 0.3  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/370:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/371:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square2):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
    
    def __repr__(self):
        return f'PartialErosion(iterations={self.iterations}, selem={self.selem})'
77/372:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(2)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
    
    def __repr__(self):
        return f'PartialErosion(iterations={self.iterations}, selem={self.selem})'
77/373:
# class Skeletonize:
#     def __init__(self):
#         pass
    
#     def __call__(self, image):
#         # Convert image tensor to numpy array
# #         np_image = to_pil_image(image)
#         image = image.convert("L")  # Convert to grayscale
#         np_image = np.array(image).astype(np.float32)
        
#         # Normalize the image to the range [0, 1]
#         np_image /= 255.0

#         # Perform skeletonization
#         skeleton_np = morphology.skeletonize(np_image)

#         # Convert skeletonized image back to PIL image
#         skeleton_pil = Image.fromarray((skeleton_np * 255).astype(np.uint8))

#         # Convert skeletonized image back to tensor
# #         skeleton_image = to_tensor(skeleton_np)

#         return skeleton_pil
77/374:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/375:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/376:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/377:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/378:
# Define the percentage of samples for the validation set
val_split = 0.3  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/379:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/380:
class PartialErosion:
    def __init__(self, iterations=2, selem=morphology.square(3)):
        self.iterations = iterations
        self.selem = selem

    def __call__(self, image):
        
        # Convert the image to a numpy array
        np_image = np.array(image)
        
        if isinstance(np_image, np.ndarray) and len(np_image.shape) == 3:
            result = np.zeros_like(np_image)  # Create an empty array with same shape as input image
            # Apply erosion to each channel independently
            for channel in range(np_image.shape[2]):
                temp_image = np_image[:, :, channel]
                for i in range(self.iterations):
                    temp_image = morphology.erosion(temp_image, self.selem)
                result[:, :, channel] = temp_image
            return result
        else:
            raise ValueError("Input image must be a 3D numpy array (height x width x channels).")
    
    def __repr__(self):
        return f'PartialErosion(iterations={self.iterations}, selem={self.selem})'
77/381:
# class Skeletonize:
#     def __init__(self):
#         pass
    
#     def __call__(self, image):
#         # Convert image tensor to numpy array
# #         np_image = to_pil_image(image)
#         image = image.convert("L")  # Convert to grayscale
#         np_image = np.array(image).astype(np.float32)
        
#         # Normalize the image to the range [0, 1]
#         np_image /= 255.0

#         # Perform skeletonization
#         skeleton_np = morphology.skeletonize(np_image)

#         # Convert skeletonized image back to PIL image
#         skeleton_pil = Image.fromarray((skeleton_np * 255).astype(np.uint8))

#         # Convert skeletonized image back to tensor
# #         skeleton_image = to_tensor(skeleton_np)

#         return skeleton_pil
77/382:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
77/383:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
77/384:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
77/385:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
77/386:
# Define the percentage of samples for the validation set
val_split = 0.3  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/387:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/388:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
77/389:
print(len(train_loader),
len(val_loader))
77/390:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
77/391:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
77/392:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
77/393:
print(len(train_loader),
len(val_loader))
77/394: # %tensorboard --logdir logs
77/395:
class VGG16Binary(nn.Module):
    def __init__(self, input_shape, num_classes=1):
        super(VGG16Binary, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(input_shape[0], 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.classifier = nn.Sequential(
            nn.Linear(512 * 6 * 12, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, num_classes),
#             nn.Sigmoid()  # Apply sigmoid activation for binary classification
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
77/396:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# VGG model from scratch
input_shape = (3, 100, 200)  # input shape (channels, height, width)

model = VGG16Binary(input_shape=input_shape, num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)

model = model.to(device)

# model
77/397:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
77/398:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.0005,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_vgg_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/399:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_vgg_from_scratch_no_erosion.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/400:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/401:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/402:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/403:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/404:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/405: test_correct/len(test_loader.dataset)
77/406: # y_true_cpu, y_pred_cpu
77/407:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/408:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/409:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/410:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/411:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/412: test_correct/len(test_loader.dataset)
77/413: # y_true_cpu, y_pred_cpu
77/414:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/415:
class VGG16Binary(nn.Module):
    def __init__(self, input_shape, num_classes=1):
        super(VGG16Binary, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(input_shape[0], 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

#             nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.classifier = nn.Sequential(
            nn.Linear(256 * 6 * 12, 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.3),
            nn.Linear(2048, 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.3),
            nn.Linear(2048, num_classes),
#             nn.Sigmoid()  # Apply sigmoid activation for binary classification
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
77/416:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# VGG model from scratch
input_shape = (3, 100, 200)  # input shape (channels, height, width)

model = VGG16Binary(input_shape=input_shape, num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)

model = model.to(device)
77/417:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
77/418:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.0005,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_vgg_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/419:
class VGG16Binary(nn.Module):
    def __init__(self, input_shape, num_classes=1):
        super(VGG16Binary, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(input_shape[0], 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

        )
        self.classifier = nn.Sequential(
            nn.Linear(256 * (input_shape[1] // 8) * (input_shape[2] // 8), 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.3),
            nn.Linear(2048, 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.3),
            nn.Linear(2048, num_classes),
#             nn.Sigmoid()  # Apply sigmoid activation for binary classification
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

    
    #             nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.MaxPool2d(kernel_size=2, stride=2)
77/420:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# VGG model from scratch
input_shape = (3, 100, 200)  # input shape (channels, height, width)

model = VGG16Binary(input_shape=input_shape, num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)
77/421:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
77/422:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.0005,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_vgg_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/423:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.0005,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_vgg_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/424:
class VGG16Binary(nn.Module):
    def __init__(self, input_shape, num_classes=1):
        super(VGG16Binary, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(input_shape[0], 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

        )
        self.classifier = nn.Sequential(
            nn.Linear(256 * (input_shape[1] // 8) * (input_shape[2] // 8), 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.3),
            nn.Linear(2048, 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.3),
            nn.Linear(2048, num_classes),
#             nn.Sigmoid()  # Apply sigmoid activation for binary classification
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

    
#             nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.MaxPool2d(kernel_size=2, stride=2)
77/425:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# VGG model from scratch
input_shape = (3, 100, 200)  # input shape (channels, height, width)

model = VGG16Binary(input_shape=input_shape, num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.classifier[1].parameters(), lr=0.001, momentum=0.9) -> for MobileNetV2
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

model = model.to(device)
77/426:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
77/427:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.0005,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/densenet_vgg_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/428:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.0005,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/429:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/430:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/vgg_from_scratch_no_erosion.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/431:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/432:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/433:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/434:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/435:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/436: test_correct/len(test_loader.dataset)
77/437: # y_true_cpu, y_pred_cpu
77/438:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/439:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/vgg_from_scratch_no_erosion_v2.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
77/440:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/vgg_from_scratch_no_erosion.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/441:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/442:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/443:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/444:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/445:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/446: test_correct/len(test_loader.dataset)
77/447: # y_true_cpu, y_pred_cpu
77/448:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = 6

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/449:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/vgg_from_scratch_no_erosion.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/450:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/451:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/452:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = math.ceil(len(test_loader.dataset) / 5)

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/453:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/454:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/455:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/456:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/457:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/458: test_correct/len(test_loader.dataset)
77/459:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = math.ceil(len(test_loader.dataset) / 5)

    # Plot the images
    fig, axes = plt.subplots(num_rows, int(len(test_loader.dataset)/5), figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/460:
#  Image 8 misclassified most probably due to COCO-Text
len(test_loader.dataset)
77/461:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = math.ceil(len(test_loader.dataset) / 5)

    # Plot the images
    fig, axes = plt.subplots(num_rows, 5, figsize=(12, 3))
    for i in range(num_rows):
        for j in range(int(len(test_loader.dataset)/5)):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/462:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = math.ceil(len(test_loader.dataset) / 5)

    # Plot the images
    fig, axes = plt.subplots(num_rows, 5, figsize=(12, 3))
    for i in range(num_rows):
        for j in range(5):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/463:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = math.ceil(len(test_loader.dataset) / 5)

    # Plot the images
    fig, axes = plt.subplots(num_rows, 5, figsize=(120, 30))
    for i in range(num_rows):
        for j in range(5):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/464:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = math.ceil(len(test_loader.dataset) / 5)

    # Plot the images
    fig, axes = plt.subplots(num_rows, 5, figsize=(120, 30))
    for i in range(num_rows):
        for j in range(5):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/465:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)
    
    num_rows = math.ceil(len(test_loader.dataset) / 5)

    # Plot the images
    fig, axes = plt.subplots(num_rows, 5, figsize=(12, 3))
    for i in range(num_rows):
        for j in range(5):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/466:
#  Image 8 misclassified most probably due to COCO-Text
len(test_loader.batch_size)
77/467:
#  Image 8 misclassified most probably due to COCO-Text
test_loader.batch_size
77/468:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(12, 3))
    for i in range(num_rows):
        for j in range(4):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/469:
img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(24, 6))
    for i in range(num_rows):
        for j in range(4):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/470:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(24, 6))
    for i in range(num_rows):
        for j in range(4):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/471:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(24, 24))
    for i in range(num_rows):
        for j in range(4):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/472:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/473:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/474:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/475:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
77/476:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/477:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/478: test_correct/len(test_loader.dataset)
77/479: # y_true_cpu, y_pred_cpu
77/480:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 195, 95, linewidth=1, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/481:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=3, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/482:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/483:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/484:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/vgg_from_scratch_no_erosion_v2.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/485:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/486:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/487:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/488:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/489:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/490: test_correct/len(test_loader.dataset)
77/491: # y_true_cpu, y_pred_cpu
77/492:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/493:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/494:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/495:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/496:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/497:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/498:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/499:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/500: test_correct/len(test_loader.dataset)
77/501: # y_true_cpu, y_pred_cpu
77/502:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/503:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/504:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/505:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/506:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/507:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/508: test_correct/len(test_loader.dataset)
77/509: # y_true_cpu, y_pred_cpu
77/510:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/511:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/512:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/513:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/514:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/515:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/516: test_correct/len(test_loader.dataset)
77/517: # y_true_cpu, y_pred_cpu
77/518:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/519:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/520:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/521:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/522:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/523:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/524: test_correct/len(test_loader.dataset)
77/525: # y_true_cpu, y_pred_cpu
77/526:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/527:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/528:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/529:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/530:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/531:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/532:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/533: test_correct/len(test_loader.dataset)
77/534: # y_true_cpu, y_pred_cpu
77/535:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/536:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/537:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/538:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/539:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/540:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/541: test_correct/len(test_loader.dataset)
77/542: # y_true_cpu, y_pred_cpu
77/543:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/544:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/545:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/546:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/547:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/548:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/549:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/550: test_correct/len(test_loader.dataset)
77/551: # y_true_cpu, y_pred_cpu
77/552:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            axes[i][j].imshow(images[img_no%32])
            axes[i][j].axis('off')
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/553:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/554:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_5_5_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/555:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/556:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/557:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/558:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/559:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/560: test_correct/len(test_loader.dataset)
77/561: # y_true_cpu, y_pred_cpu
77/562:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/563:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/vgg_from_scratch_no_erosion_v2.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/564:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/vgg_from_scratch_no_erosion_v2.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/565:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/566:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/567:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/568:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/569:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/570: test_correct/len(test_loader.dataset)
77/571: # y_true_cpu, y_pred_cpu
77/572:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/573:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/574:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/575:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/576:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/577:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/578: test_correct/len(test_loader.dataset)
77/579: # y_true_cpu, y_pred_cpu
77/580:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/581:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/582:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/583:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/584:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/585:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/586: test_correct/len(test_loader.dataset)
77/587: # y_true_cpu, y_pred_cpu
77/588:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/589:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/590:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/591:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/592:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/593:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/594: test_correct/len(test_loader.dataset)
77/595: # y_true_cpu, y_pred_cpu
77/596:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/597:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 250)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/598:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/599:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/600:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/601:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/602:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/603:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/604:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/605:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/606: test_correct/len(test_loader.dataset)
77/607:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/608:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/609:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/610:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/611:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/612: test_correct/len(test_loader.dataset)
77/613: # y_true_cpu, y_pred_cpu
77/614:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/615:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=3),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/616:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/617:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/618:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/619:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/620: test_correct/len(test_loader.dataset)
77/621: # y_true_cpu, y_pred_cpu
77/622:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/623:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=1, selem=morphology.square(11)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/624:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/625:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/626:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/627:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/628: test_correct/len(test_loader.dataset)
77/629:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=1, selem=morphology.square(9)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/630:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/631:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/632:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/633:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/634: test_correct/len(test_loader.dataset)
77/635:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=1, selem=morphology.square(5)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/636:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/637:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/638:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/639:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/640: test_correct/len(test_loader.dataset)
77/641:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=1, selem=morphology.square(4)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/642:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/643:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/644:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/645:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/646: test_correct/len(test_loader.dataset)
77/647:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=1, selem=morphology.square(2)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/648:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/649:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/650:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/651:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/652: test_correct/len(test_loader.dataset)
77/653:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2, selem=morphology.square(2)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/654:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/655:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/656:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/657:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/658: test_correct/len(test_loader.dataset)
77/659: # y_true_cpu, y_pred_cpu
77/660:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/661:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/662:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2, selem=morphology.square(2)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/663:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/664:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/665:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/666:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/667: test_correct/len(test_loader.dataset)
77/668:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=3, selem=morphology.square(2)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/669:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/670:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/671:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/672:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/673: test_correct/len(test_loader.dataset)
77/674: # y_true_cpu, y_pred_cpu
77/675:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=3, selem=morphology.square(1)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/676:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/677:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/678:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=6, selem=morphology.square(1)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/679:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/680:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/681:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=6, selem=morphology.square(2)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/682:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/683:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/684:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/685:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/686: test_correct/len(test_loader.dataset)
77/687: # y_true_cpu, y_pred_cpu
77/688:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/689:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2, selem=morphology.square(3)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/690:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/691:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/692:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/693:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/694: test_correct/len(test_loader.dataset)
77/695: # y_true_cpu, y_pred_cpu
77/696:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/697:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/698:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2, selem=morphology.square(3)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/699:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/700:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/701:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/702:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/703: test_correct/len(test_loader.dataset)
77/704: # y_true_cpu, y_pred_cpu
77/705:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/706:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/707:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2, selem=morphology.square(3)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/708:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/709:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/710:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/711:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/712: test_correct/len(test_loader.dataset)
77/713: # y_true_cpu, y_pred_cpu
77/714:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/715:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_5_5_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/716:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2, selem=morphology.square(3)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/717:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/718:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/719:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/720:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/721: test_correct/len(test_loader.dataset)
77/722: # y_true_cpu, y_pred_cpu
77/723:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/724:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/725:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/726:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/727:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2, selem=morphology.square(3)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/728:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/729:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/730:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/731:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/732: test_correct/len(test_loader.dataset)
77/733: # y_true_cpu, y_pred_cpu
77/734:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/735: test_loader.batch_size
79/1:
# densenet_10_5_layers_unlocked.pth
# vgg_from_scratch_w_erosion_v2.pth
# densenet_25_10_layers_unlocked_no_erosion.pth
77/736:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked.pth')) # densenet_last_epoch_model
best_model = best_model.to(device)
77/737:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2, selem=morphology.square(3)),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/738:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/739:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/740:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/741:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/742: test_correct/len(test_loader.dataset)
77/743: # y_true_cpu, y_pred_cpu
77/744:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/745: test_loader.batch_size
77/746:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=3),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/747:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/748:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/749:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/750:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/751: test_correct/len(test_loader.dataset)
77/752:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=4),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/753:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/754:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/755:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/756:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/757: test_correct/len(test_loader.dataset)
77/758: # y_true_cpu, y_pred_cpu
77/759:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/760:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=6),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/761:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/762:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/763:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/764:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/765: test_correct/len(test_loader.dataset)
77/766:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=1),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/767:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/768:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/769:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/770:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/771: test_correct/len(test_loader.dataset)
77/772: # y_true_cpu, y_pred_cpu
77/773:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
77/774:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=0),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/775:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/776:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/777:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/778:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/779: test_correct/len(test_loader.dataset)
77/780:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
77/781:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
77/782:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
77/783:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
77/784:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
77/785: test_correct/len(test_loader.dataset)
77/786:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
86/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

# ! pip install tensorboard jupyter-tensorboard
from torch.utils.tensorboard import SummaryWriter
86/2:
# vgg_from_scratch_w_erosion_v2.pth

best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked.pth'))
best_model = best_model.to(device)
86/3:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
86/4:
# vgg_from_scratch_w_erosion_v2.pth

best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked.pth'))
best_model = best_model.to(device)
86/5:
score_dict = {}
score_dict
86/6:
score_dict = {}
type(score_dict)
86/7:
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),
    transforms.Grayscale(num_output_channels=3),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
86/8:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

from utils import *
86/9:
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),
    transforms.Grayscale(num_output_channels=3),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
86/10:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

from utils import utils
86/11:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

from utils import utils
86/12:
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),
    transforms.Grayscale(num_output_channels=3),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
86/13:
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    utils.PartialErosion(iterations=2),
    transforms.ToTensor(),
    transforms.Grayscale(num_output_channels=3),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
86/14:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

from utils.utils import PartialErosion
86/15:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

from utils.utils import *
86/16:
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    utils.PartialErosion(iterations=2),
    transforms.ToTensor(),
    transforms.Grayscale(num_output_channels=3),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
88/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

# ! pip install tensorboard jupyter-tensorboard
from torch.utils.tensorboard import SummaryWriter
88/2:
# ! jupyter nbextension enable --py jupyter_tensorboard
# %load_ext tensorboard
88/3:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
88/4:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

# ! pip install tensorboard jupyter-tensorboard
from torch.utils.tensorboard import SummaryWriter

from utils.utils import *
88/5:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.ToTensor(),
#     transforms.Normalize([0.5], [0.5])
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
88/6:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
88/7:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
88/8:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
88/9:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
88/10:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
88/11:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
88/12:
print(len(train_loader),
len(val_loader))
88/13:
# CRNN model

class InterleavedCRNN(nn.Module):
    def __init__(self, num_classes):
        super(InterleavedCRNN, self).__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn1 = nn.GRU(input_size=64*50, hidden_size=32, num_layers=1, bidirectional=True, dropout=0.5)

        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn2 = nn.GRU(input_size=128*12, hidden_size=64, num_layers=1, bidirectional=True, dropout=0.5)
        
        self.fc = nn.Linear(64 * 2, num_classes)

    def forward(self, x):
        # pass through first conv block
        x = self.conv1(x)
        
        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)
        
        # pass through first RNN block
        x, _ = self.rnn1(x)
        
        # reshape for conv block
        x = x.view(b, w, c, h).permute(0, 2, 3, 1).contiguous()
        
        # pass through second conv block
        x = self.conv2(x)
        
        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)
        
        # pass through second RNN block
        x, _ = self.rnn2(x)
        
        # pass through FC layer
        x = self.fc(x)
        
        return x
88/14:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/15:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/16:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/17:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/18:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
    transforms.Grayscale(),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
88/19:
# Define the percentage of samples for the validation set
val_split = 0.15  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
88/20:
# Define the percentage of samples for the validation set
val_split = 0.20  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
88/21:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
88/22:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
88/23:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/24:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/25:
# CRNN model

class InterleavedCRNN(nn.Module):
    def __init__(self, num_classes):
        super(InterleavedCRNN, self).__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn1 = nn.GRU(input_size=64*50, hidden_size=32, num_layers=1, bidirectional=True, dropout=0.5)

        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn2 = nn.GRU(input_size=128*12, hidden_size=64, num_layers=1, bidirectional=True, dropout=0.5)
        
        self.fc = nn.Linear(64 * 2, num_classes)

    def forward(self, x):
        # pass through first conv block
        x = self.conv1(x)
        
        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)
        
        # pass through first RNN block
        x, _ = self.rnn1(x)
        
#         # reshape for conv block
#         x = x.view(b, w, c, h).permute(0, 2, 3, 1).contiguous()
        
        # reshape for conv block
        x = x.view(b, w, 64, h).permute(0, 2, 3, 1).contiguous()

        
        # pass through second conv block
        x = self.conv2(x)
        
        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)
        
        # pass through second RNN block
        x, _ = self.rnn2(x)
        
        # pass through FC layer
        x = self.fc(x)
        
        return x
88/26:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/27:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/28:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/29:
# CRNN model

class InterleavedCRNN(nn.Module):
    def __init__(self, num_classes):
        super(InterleavedCRNN, self).__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn1 = nn.GRU(input_size=64*50, hidden_size=32, num_layers=1, bidirectional=True, dropout=0.5)

        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn2 = nn.GRU(input_size=128*12, hidden_size=64, num_layers=1, bidirectional=True, dropout=0.5)
        
        self.fc = nn.Linear(64 * 2, num_classes)

    def forward(self, x):
        # pass through first conv block
        x = self.conv1(x)

        # reshape for RNN
        b, c, h, w = x.size()
        print(f'After conv1: {x.shape}')  # Add this to check the shape
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through first RNN block
        x, _ = self.rnn1(x)
        print(f'After rnn1: {x.shape}')  # Add this to check the shape

        # reshape for conv block
        x = x.view(b, w, c, h).permute(0, 2, 3, 1).contiguous()
        print(f'After reshape: {x.shape}')  # Add this to check the shape

        # pass through second conv block
        x = self.conv2(x)
        print(f'After conv2: {x.shape}')  # Add this to check the shape

        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through second RNN block
        x, _ = self.rnn2(x)
        print(f'After rnn2: {x.shape}')  # Add this to check the shape

        # pass through FC layer
        x = self.fc(x)
        print(f'After fc: {x.shape}')  # Add this to check the shape

        return x
88/30:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/31:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/32:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/33:
# CRNN model

class InterleavedCRNN(nn.Module):
    def __init__(self, num_classes):
        super(InterleavedCRNN, self).__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn1 = nn.GRU(input_size=64*50, hidden_size=32, num_layers=1, bidirectional=True, dropout=0.5)

        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn2 = nn.GRU(input_size=128*12, hidden_size=64, num_layers=1, bidirectional=True, dropout=0.5)
        
        self.fc = nn.Linear(64 * 2, num_classes)

    def forward(self, x):
        # pass through first conv block
        x = self.conv1(x)

        # reshape for RNN
        b, c, h, w = x.size()
        print(f'After conv1: {x.shape}')  # Add this to check the shape
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through first RNN block
        x, _ = self.rnn1(x)
        print(f'After rnn1: {x.shape}')  # Add this to check the shape

        # reshape for conv block
        # x = x.view(b, w, c, h).permute(0, 2, 3, 1).contiguous()
        x = x.view(b, w, c, 1).permute(0, 2, 3, 1).contiguous()
        print(f'After reshape: {x.shape}')  # Add this to check the shape

        # pass through second conv block
        x = self.conv2(x)
        print(f'After conv2: {x.shape}')  # Add this to check the shape

        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through second RNN block
        x, _ = self.rnn2(x)
        print(f'After rnn2: {x.shape}')  # Add this to check the shape

        # pass through FC layer
        x = self.fc(x)
        print(f'After fc: {x.shape}')  # Add this to check the shape

        return x
88/34:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/35:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/36:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/37:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 2, num_classes)

    def forward(self, x):
        x = self.conv(x)
        x = x.squeeze(2).permute(2, 0, 1)  # Reshape for RNN
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/38:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/39:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/40:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/41:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 2, num_classes)

    def forward(self, x):
        x = self.conv(x)
        
        b, c, h, w = x.size()
        print(f'After conv: {x.shape}')  # Add this to check the shape
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)
        
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/42:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/43:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/44:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/45:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.squeeze(2).permute(2, 0, 1)  # Reshape for RNN
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x

# Rest of the code remains the same
88/46:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/47:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/48:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/49:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.squeeze(2).permute(0, 2, 1)  # Reshape for RNN
        x = x.permute(2, 0, 1)  # Adjusted permutation
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/50:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/51:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/52:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/53:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/54:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/55:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/56:
# CRNN model

class InterleavedCRNN(nn.Module):
    def __init__(self, num_classes):
        super(InterleavedCRNN, self).__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn1 = nn.GRU(input_size=64*50, hidden_size=32, num_layers=1, bidirectional=True, dropout=0.5)

        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn2 = nn.GRU(input_size=128*12, hidden_size=64, num_layers=1, bidirectional=True, dropout=0.5)
        
        self.fc = nn.Linear(64 * 2, num_classes)

    def forward(self, x):
        # pass through first conv block
        x = self.conv1(x)

        # reshape for RNN
        b, c, h, w = x.size()
        print(f'After conv1: {x.shape}')  # Add this to check the shape
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through first RNN block
        x, _ = self.rnn1(x)
        print(f'After rnn1: {x.shape}')  # Add this to check the shape

        # reshape for conv block
        # x = x.view(b, w, c, h).permute(0, 2, 3, 1).contiguous()
        x = x.view(b, w, c, h).permute(0, 2, 3, 1).contiguous()
        print(f'After reshape: {x.shape}')  # Add this to check the shape

        # pass through second conv block
        x = self.conv2(x)
        print(f'After conv2: {x.shape}')  # Add this to check the shape

        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through second RNN block
        x, _ = self.rnn2(x)
        print(f'After rnn2: {x.shape}')  # Add this to check the shape

        # pass through FC layer
        x = self.fc(x)
        print(f'After fc: {x.shape}')  # Add this to check the shape

        return x
88/57:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.squeeze(2).permute(0, 2, 1)  # Reshape for RNN
        x = x.permute(2, 0, 1)  # Adjusted permutation
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/58:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/59:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/60:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/61:
# CRNN model

class InterleavedCRNN(nn.Module):
    def __init__(self, num_classes):
        super(InterleavedCRNN, self).__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn1 = nn.GRU(input_size=64*50, hidden_size=32, num_layers=1, bidirectional=True, dropout=0.5)

        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn2 = nn.GRU(input_size=128*12, hidden_size=64, num_layers=1, bidirectional=True, dropout=0.5)
        
        self.fc = nn.Linear(64 * 2, num_classes)

    def forward(self, x):
        # pass through first conv block
        x = self.conv1(x)

        # reshape for RNN
        b, c, h, w = x.size()
        print(f'After conv1: {x.shape}')  # Add this to check the shape
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through first RNN block
        x, _ = self.rnn1(x)
        print(f'After rnn1: {x.shape}')  # Add this to check the shape

        # reshape for conv block
        # x = x.view(b, w, c, h).permute(0, 2, 3, 1).contiguous()
        x = x.view(b, w, c, 1).permute(0, 2, 3, 1).contiguous()
        print(f'After reshape: {x.shape}')  # Add this to check the shape

        # pass through second conv block
        x = self.conv2(x)
        print(f'After conv2: {x.shape}')  # Add this to check the shape

        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through second RNN block
        x, _ = self.rnn2(x)
        print(f'After rnn2: {x.shape}')  # Add this to check the shape

        # pass through FC layer
        x = self.fc(x)
        print(f'After fc: {x.shape}')  # Add this to check the shape

        return x
88/62:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.squeeze(2).permute(0, 2, 1)  # Reshape for RNN
        x = x.permute(2, 0, 1)  # Adjusted permutation
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/63:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/64:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/65:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/66:
# CRNN model

class InterleavedCRNN(nn.Module):
    def __init__(self, num_classes):
        super(InterleavedCRNN, self).__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn1 = nn.GRU(input_size=64*50, hidden_size=32, num_layers=1, bidirectional=True, dropout=0.5)

        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn2 = nn.GRU(input_size=128*12, hidden_size=64, num_layers=1, bidirectional=True, dropout=0.5)
        
        self.fc = nn.Linear(64 * 2, num_classes)

    def forward(self, x):
        # pass through first conv block
        x = self.conv1(x)

        # reshape for RNN
        b, c, h, w = x.size()
        print(f'After conv1: {x.shape}')  # Add this to check the shape
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through first RNN block
        x, _ = self.rnn1(x)
        print(f'After rnn1: {x.shape}')  # Add this to check the shape

        # reshape for conv block
        # x = x.view(b, w, c, h).permute(0, 2, 3, 1).contiguous()
        x = x.view(b, w, 10, 10).permute(0, 2, 3, 1).contiguous()
        print(f'After reshape: {x.shape}')  # Add this to check the shape

        # pass through second conv block
        x = self.conv2(x)
        print(f'After conv2: {x.shape}')  # Add this to check the shape

        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through second RNN block
        x, _ = self.rnn2(x)
        print(f'After rnn2: {x.shape}')  # Add this to check the shape

        # pass through FC layer
        x = self.fc(x)
        print(f'After fc: {x.shape}')  # Add this to check the shape

        return x
88/67:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.squeeze(2).permute(0, 2, 1)  # Reshape for RNN
        x = x.permute(2, 0, 1)  # Adjusted permutation
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/68:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/69:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/70:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/71:
# CRNN model

class InterleavedCRNN(nn.Module):
    def __init__(self, num_classes):
        super(InterleavedCRNN, self).__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn1 = nn.GRU(input_size=64*50, hidden_size=32, num_layers=1, bidirectional=True, dropout=0.5)

        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn2 = nn.GRU(input_size=128*12, hidden_size=64, num_layers=1, bidirectional=True, dropout=0.5)
        
        self.fc = nn.Linear(64 * 2, num_classes)

    def forward(self, x):
        # pass through first conv block
        x = self.conv1(x)

        # reshape for RNN
        b, c, h, w = x.size()
        print(f'After conv1: {x.shape}')  # Add this to check the shape
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through first RNN block
        x, _ = self.rnn1(x)
        print(f'After rnn1: {x.shape}')  # Add this to check the shape

        # reshape for conv block
        # x = x.view(b, w, c, h).permute(0, 2, 3, 1).contiguous()
        # b=64, c=64, w=10, h=10
        x = x.view(b, 10, c, 10).permute(0, 2, 3, 1).contiguous()
        print(f'After reshape: {x.shape}')  # Add this to check the shape

        # pass through second conv block
        x = self.conv2(x)
        print(f'After conv2: {x.shape}')  # Add this to check the shape

        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through second RNN block
        x, _ = self.rnn2(x)
        print(f'After rnn2: {x.shape}')  # Add this to check the shape

        # pass through FC layer
        x = self.fc(x)
        print(f'After fc: {x.shape}')  # Add this to check the shape

        return x
88/72:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.squeeze(2).permute(0, 2, 1)  # Reshape for RNN
        x = x.permute(2, 0, 1)  # Adjusted permutation
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/73:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/74:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/75:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/76:
# CRNN model

class InterleavedCRNN(nn.Module):
    def __init__(self, num_classes):
        super(InterleavedCRNN, self).__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn1 = nn.GRU(input_size=64*50, hidden_size=32, num_layers=1, bidirectional=True, dropout=0.5)

        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn2 = nn.GRU(input_size=640, hidden_size=64, num_layers=1, bidirectional=True, dropout=0.5)
        
        self.fc = nn.Linear(64 * 2, num_classes)

    def forward(self, x):
        # pass through first conv block
        x = self.conv1(x)

        # reshape for RNN
        b, c, h, w = x.size()
        print(f'After conv1: {x.shape}')  # Add this to check the shape
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through first RNN block
        x, _ = self.rnn1(x)
        print(f'After rnn1: {x.shape}')  # Add this to check the shape

        # reshape for conv block
        # x = x.view(b, w, c, h).permute(0, 2, 3, 1).contiguous()
        # b=64, c=64, w=10, h=10
        x = x.view(b, 10, c, 10).permute(0, 2, 3, 1).contiguous()
        print(f'After reshape: {x.shape}')  # Add this to check the shape

        # pass through second conv block
        x = self.conv2(x)
        print(f'After conv2: {x.shape}')  # Add this to check the shape

        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through second RNN block
        x, _ = self.rnn2(x)
        print(f'After rnn2: {x.shape}')  # Add this to check the shape

        # pass through FC layer
        x = self.fc(x)
        print(f'After fc: {x.shape}')  # Add this to check the shape

        return x
88/77:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.squeeze(2).permute(0, 2, 1)  # Reshape for RNN
        x = x.permute(2, 0, 1)  # Adjusted permutation
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/78:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/79:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/80:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/81:
# CRNN model

class InterleavedCRNN(nn.Module):
    def __init__(self, num_classes):
        super(InterleavedCRNN, self).__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn1 = nn.GRU(input_size=64*50, hidden_size=32, num_layers=1, bidirectional=True, dropout=0.5)

        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn2 = nn.GRU(input_size=640, hidden_size=64, num_layers=1, bidirectional=True, dropout=0.5)
        
        self.fc = nn.Linear(64 * 2, num_classes)

    def forward(self, x):
        # pass through first conv block
        x = self.conv1(x)

        # reshape for RNN
        b, c, h, w = x.size()
        print(f'After conv1: {x.shape}')  # Add this to check the shape
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through first RNN block
        x, _ = self.rnn1(x)
        print(f'After rnn1: {x.shape}')  # Add this to check the shape

        # reshape for conv block
        # x = x.view(b, w, c, h).permute(0, 2, 3, 1).contiguous()
        # b=64, c=64, w=10, h=10
        x = x.view(b, 10, c, 10).permute(0, 2, 3, 1).contiguous()
        print(f'After reshape: {x.shape}')  # Add this to check the shape

        # pass through second conv block
        x = self.conv2(x)
        print(f'After conv2: {x.shape}')  # Add this to check the shape

        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through second RNN block
        x, _ = self.rnn2(x)
        print(f'After rnn2: {x.shape}')  # Add this to check the shape
        
        # reshape for fc
        x = x[:, -1, :]

        # pass through FC layer
        x = self.fc(x)
        print(f'After fc: {x.shape}')  # Add this to check the shape

        return x.squeeze()
88/82:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.squeeze(2).permute(0, 2, 1)  # Reshape for RNN
        x = x.permute(2, 0, 1)  # Adjusted permutation
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/83:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/84:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/85:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/86:
# CRNN model

class InterleavedCRNN(nn.Module):
    def __init__(self, num_classes):
        super(InterleavedCRNN, self).__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn1 = nn.GRU(input_size=64*50, hidden_size=32, num_layers=1, bidirectional=True, dropout=0.5)

        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn2 = nn.GRU(input_size=640, hidden_size=64, num_layers=1, bidirectional=True, dropout=0.5)
        
        self.fc = nn.Linear(64 * 2, num_classes)

    def forward(self, x):
        # pass through first conv block
        x = self.conv1(x)

        # reshape for RNN
        b, c, h, w = x.size()
        print(f'After conv1: {x.shape}')  # Add this to check the shape
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through first RNN block
        x, _ = self.rnn1(x)
        print(f'After rnn1: {x.shape}')  # Add this to check the shape

        # reshape for conv block
        # x = x.view(b, w, c, h).permute(0, 2, 3, 1).contiguous()
        # b=64, c=64, w=10, h=10
        x = x.view(b, 10, c, 10).permute(0, 2, 3, 1).contiguous()
        print(f'After reshape: {x.shape}')  # Add this to check the shape

        # pass through second conv block
        x = self.conv2(x)
        print(f'After conv2: {x.shape}')  # Add this to check the shape

        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through second RNN block
        x, _ = self.rnn2(x)
        print(f'After rnn2: {x.shape}')  # Add this to check the shape
        
        # reshape for fc
        x = x[:, -1, :]

        # pass through FC layer
        x = self.fc(x)
        print(f'After fc: {x.shape}')  # Add this to check the shape

        return x
88/87:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.squeeze(2).permute(0, 2, 1)  # Reshape for RNN
        x = x.permute(2, 0, 1)  # Adjusted permutation
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/88:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/89:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/90:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/91:
# CRNN model

class InterleavedCRNN(nn.Module):
    def __init__(self, num_classes):
        super(InterleavedCRNN, self).__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn1 = nn.GRU(input_size=64*50, hidden_size=32, num_layers=1, bidirectional=True, dropout=0.5)

        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn2 = nn.GRU(input_size=640, hidden_size=64, num_layers=1, bidirectional=True, dropout=0.5)
        
        self.fc = nn.Linear(64 * 2, num_classes)

    def forward(self, x):
        # pass through first conv block
        x = self.conv1(x)

        # reshape for RNN
        b, c, h, w = x.size()
#         print(f'After conv1: {x.shape}')  # Add this to check the shape
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through first RNN block
        x, _ = self.rnn1(x)
#         print(f'After rnn1: {x.shape}')  # Add this to check the shape

        # reshape for conv block
        # x = x.view(b, w, c, h).permute(0, 2, 3, 1).contiguous()
        # b=64, c=64, w=10, h=10
        x = x.view(b, 10, c, 10).permute(0, 2, 3, 1).contiguous()
#         print(f'After reshape: {x.shape}')  # Add this to check the shape

        # pass through second conv block
        x = self.conv2(x)
#         print(f'After conv2: {x.shape}')  # Add this to check the shape

        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through second RNN block
        x, _ = self.rnn2(x)
#         print(f'After rnn2: {x.shape}')  # Add this to check the shape
        
        # reshape for fc
        x = x[:, -1, :]

        # pass through FC layer
        x = self.fc(x)
#         print(f'After fc: {x.shape}')  # Add this to check the shape

        return x
88/92:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.squeeze(2).permute(0, 2, 1)  # Reshape for RNN
        x = x.permute(2, 0, 1)  # Adjusted permutation
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/93:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/94:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/95:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/96:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
    transforms.Grayscale(),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
88/97:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
88/98:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
88/99:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
88/100:
# Define the percentage of samples for the validation set
val_split = 0.20  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
88/101:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
88/102:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
88/103:
print(len(train_loader),
len(val_loader))
88/104: # %tensorboard --logdir logs
88/105:
class VGG16Binary(nn.Module):
    def __init__(self, input_shape, num_classes=1):
        super(VGG16Binary, self).__init__()
        self.features = nn.Sequential(
            
            nn.Conv2d(input_shape[0], 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

        )
        self.classifier = nn.Sequential(
            nn.Linear(256 * (input_shape[1] // 8) * (input_shape[2] // 8), 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.3),
            nn.Linear(2048, 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.3),
            nn.Linear(2048, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
88/106:
# CRNN model

class InterleavedCRNN(nn.Module):
    def __init__(self, num_classes):
        super(InterleavedCRNN, self).__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn1 = nn.GRU(input_size=64*50, hidden_size=32, num_layers=1, bidirectional=True, dropout=0.5)

        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn2 = nn.GRU(input_size=640, hidden_size=64, num_layers=1, bidirectional=True, dropout=0.5)
        
        self.fc = nn.Linear(64 * 2, num_classes)

    def forward(self, x):
        # pass through first conv block
        x = self.conv1(x)

        # reshape for RNN
        b, c, h, w = x.size()
#         print(f'After conv1: {x.shape}')  # Add this to check the shape
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through first RNN block
        x, _ = self.rnn1(x)
#         print(f'After rnn1: {x.shape}')  # Add this to check the shape

        # reshape for conv block
        # x = x.view(b, w, c, h).permute(0, 2, 3, 1).contiguous()
        # b=64, c=64, w=10, h=10
        x = x.view(b, 10, c, 10).permute(0, 2, 3, 1).contiguous()
#         print(f'After reshape: {x.shape}')  # Add this to check the shape

        # pass through second conv block
        x = self.conv2(x)
#         print(f'After conv2: {x.shape}')  # Add this to check the shape

        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through second RNN block
        x, _ = self.rnn2(x)
#         print(f'After rnn2: {x.shape}')  # Add this to check the shape
        
        # reshape for fc
        x = x[:, -1, :]

        # pass through FC layer
        x = self.fc(x)
#         print(f'After fc: {x.shape}')  # Add this to check the shape

        return x
88/107:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.squeeze(2).permute(0, 2, 1)  # Reshape for RNN
        x = x.permute(2, 0, 1)  # Adjusted permutation
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/108:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/109:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/110:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/111:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

# CRNN
best_model = InterleavedCRNN(num_classes=1)

best_model.load_state_dict(torch.load('model/crnn_from_scratch_no_erosion.pth'))
best_model = best_model.to(device)
88/112:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),
    transforms.Grayscale(num_output_channels=3),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
88/113:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/114:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/115:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/116:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/117:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/118: test_correct/len(test_loader.dataset)
88/119: # y_true_cpu, y_pred_cpu
88/120:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
88/121:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/122:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

# CRNN
best_model = InterleavedCRNN(num_classes=1)

best_model.load_state_dict(torch.load('model/crnn_from_scratch_no_erosion.pth'))
best_model = best_model.to(device)
88/123:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/124:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/125:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/126:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/127:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/128: test_correct/len(test_loader.dataset)
88/129: # y_true_cpu, y_pred_cpu
88/130:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
88/131:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
    transforms.Grayscale(),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
88/132:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
88/133:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
88/134:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
88/135:
# Define the percentage of samples for the validation set
val_split = 0.20  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
88/136:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
88/137:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
88/138:
print(len(train_loader),
len(val_loader))
88/139: # %tensorboard --logdir logs
88/140:
class VGG16Binary(nn.Module):
    def __init__(self, input_shape, num_classes=1):
        super(VGG16Binary, self).__init__()
        self.features = nn.Sequential(
            
            nn.Conv2d(input_shape[0], 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

        )
        self.classifier = nn.Sequential(
            nn.Linear(256 * (input_shape[1] // 8) * (input_shape[2] // 8), 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.3),
            nn.Linear(2048, 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.3),
            nn.Linear(2048, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
88/141:
# CRNN model

class InterleavedCRNN(nn.Module):
    def __init__(self, num_classes):
        super(InterleavedCRNN, self).__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn1 = nn.GRU(input_size=64*50, hidden_size=32, num_layers=1, bidirectional=True, dropout=0.5)

        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn2 = nn.GRU(input_size=640, hidden_size=64, num_layers=1, bidirectional=True, dropout=0.5)
        
        self.fc = nn.Linear(64 * 2, num_classes)

    def forward(self, x):
        # pass through first conv block
        x = self.conv1(x)

        # reshape for RNN
        b, c, h, w = x.size()
#         print(f'After conv1: {x.shape}')  # Add this to check the shape
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through first RNN block
        x, _ = self.rnn1(x)
#         print(f'After rnn1: {x.shape}')  # Add this to check the shape

        # reshape for conv block
        # x = x.view(b, w, c, h).permute(0, 2, 3, 1).contiguous()
        # b=64, c=64, w=10, h=10
        x = x.view(b, 10, c, 10).permute(0, 2, 3, 1).contiguous()
#         print(f'After reshape: {x.shape}')  # Add this to check the shape

        # pass through second conv block
        x = self.conv2(x)
#         print(f'After conv2: {x.shape}')  # Add this to check the shape

        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through second RNN block
        x, _ = self.rnn2(x)
#         print(f'After rnn2: {x.shape}')  # Add this to check the shape
        
        # reshape for fc
        x = x[:, -1, :]

        # pass through FC layer
        x = self.fc(x)
#         print(f'After fc: {x.shape}')  # Add this to check the shape

        return x
88/142:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.squeeze(2).permute(0, 2, 1)  # Reshape for RNN
        x = x.permute(2, 0, 1)  # Adjusted permutation
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/143:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
model = InterleavedCRNN(num_classes=1)

# model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/144:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/145:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/146:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

# CRNN
best_model = InterleavedCRNN(num_classes=1)

best_model.load_state_dict(torch.load('model/crnn_from_scratch_w_erosion.pth'))
best_model = best_model.to(device)
88/147:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/148:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/149:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/150:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/151:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/152: test_correct/len(test_loader.dataset)
88/153: # y_true_cpu, y_pred_cpu
88/154:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
88/155:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

# CRNN
best_model = InterleavedCRNN(num_classes=1)

best_model.load_state_dict(torch.load('model/crnn_from_scratch_no_erosion.pth'))
best_model = best_model.to(device)
88/156:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/157:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/158:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/159:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/160:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/161: test_correct/len(test_loader.dataset)
88/162: # y_true_cpu, y_pred_cpu
88/163:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
88/164:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/165:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/166:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/167:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/168:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/169: test_correct/len(test_loader.dataset)
88/170: # y_true_cpu, y_pred_cpu
88/171:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
88/172: test_loader.batch_size
88/173:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/174:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/175:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/176:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.squeeze(2).permute(0, 2, 1)  # Reshape for RNN
        x = x.permute(2, 0, 1)  # Adjusted permutation
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/177:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.squeeze(2).permute(0, 2, 1)  # Reshape for RNN
        x = x.permute(2, 0, 1)  # Adjusted permutation
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/178:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/179:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/180:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/181:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.squeeze(2).permute(0, 2, 1)  # Reshape for RNN
        print(x.shape)
        x = x.permute(2, 0, 1)  # Adjusted permutation
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/182:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/183:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/184:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/185:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.squeeze(2).permute(0, 2, 1)  # Reshape for RNN
        print(x.shape)
#         x = x.permute(2, 0, 1)  # Adjusted permutation
#         print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/186:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/187:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/188:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/189:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1)  # Reshape for RNN
        print(x.shape)
#         x = x.permute(2, 0, 1)  # Adjusted permutation
#         print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/190:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/191:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/192:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/193:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1)  # Reshape for RNN
        print(x.shape)
        x = x.permute(2, 0, 1)  # Adjusted permutation
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/194:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/195:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/196:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/197:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1)  # Reshape for RNN
        print(x.shape)
#         x = x.permute(2, 0, 1)  # Adjusted permutation
        x = x.view(b, w, -1)
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/198:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/199:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/200:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/201:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1)  # Reshape for RNN
        print(x.shape)
#         x = x.permute(2, 0, 1)  # Adjusted permutation
        b, c, w, h = x.size()
        x = x.view(b, w, -1)
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/202:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/203:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/204:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/205:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1)  # Reshape for RNN
        print(x.shape)
#         x = x.permute(2, 0, 1)  # Adjusted permutation
        b, c, w, h = x.size()
#         x = x.view(b, w, -1)
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/206:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/207:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/208:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/209:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1)  # Reshape for RNN
        print(x.shape)
        b, c, w, h = x.size()
        x = x.view(w, b, -1)  # merge height and channel dimensions
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/210:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/211:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/212:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/213:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        print(x.shape)
        b, c, w, h = x.size()
        x = x.view(w, b, -1)  # merge height and channel dimensions
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/214:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/215:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/216:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/217:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        print(x.shape)
        b, c, w, h = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/218:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/219:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/220:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/221:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        print(x.shape)
        b, c, w, h = x.size()
        x = x.view(b, w, h, c).permute(1, 0, 2)  # merge height and channel dimensions
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/222:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/223:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/224:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/225:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        print(x.shape)
        b, c, w, h = x.size()
        x = x.view(b, w, h, c)  # merge height and channel dimensions
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/226:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/227:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/228:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/229:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        print(x.shape)
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/230:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/231:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/232:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/233:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=1, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 6, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        print(x.shape)
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/234:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/235:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/236:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/237:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 2, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        print(x.shape)
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        print(x.shape)
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x
88/238:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/239:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/240:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/241:
# Define the CRNN model
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 2, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        print(x.shape)
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/242:
# Define the CRNN model
class CRNN(nn.Module):
    
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 2, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        print(x.shape)
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/243:
# Define the CRNN model
class CRNN(nn.Module):
    
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 2, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        print(x.shape)
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        print(x.shape)
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/244:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/245:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/246:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/247:
# Define the CRNN model
class CRNN(nn.Module):
    
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(256 * 2, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        print(x.shape)
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        print(x.shape)
        _, hidden = self.rnn(x)
        print(x.shape)
        x = hidden[-1]
        print(x.shape)
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/248:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/249:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/250:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/251:
# Define the CRNN model
class CRNN(nn.Module):
    
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.5)
        self.fc = nn.Linear(64*256, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        print(x.shape)
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        print(x.shape)
        _, hidden = self.rnn(x)
        print(x.shape)
        x = hidden[-1]
        print(x.shape)
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/252:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/253:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/254:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/255:
# Define the CRNN model
class CRNN(nn.Module):
    
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.9)
        self.fc = nn.Linear(256, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        print(x.shape)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        print(x.shape)
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        print(x.shape)
        _, hidden = self.rnn(x)
        print(x.shape)
        x = hidden[-1]
        print(x.shape)
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/256:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/257:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/258:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/259:
# Define the CRNN model
class CRNN(nn.Module):
    
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=True, dropout=0.3)
        self.fc = nn.Linear(256, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/260:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/261:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/262:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/block_crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/263:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

# CRNN
best_model = InterleavedCRNN(num_classes=1)

best_model.load_state_dict(torch.load('model/block_crnn_from_scratch_w_erosion.pth'))
best_model = best_model.to(device)
88/264:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

# CRNN
# best_model = InterleavedCRNN(num_classes=1)
best_model = CRNN(num_classes=1)

best_model.load_state_dict(torch.load('model/block_crnn_from_scratch_w_erosion.pth'))
best_model = best_model.to(device)
88/265:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/266:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/267:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/268:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/269:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/270:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/271: test_correct/len(test_loader.dataset)
88/272: # y_true_cpu, y_pred_cpu
88/273:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
88/274:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/275:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/276:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/277:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/278:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/279: test_correct/len(test_loader.dataset)
88/280: # y_true_cpu, y_pred_cpu
88/281:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
88/282:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
#     PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
    transforms.Grayscale(),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
88/283:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
88/284:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
88/285:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
88/286:
# Define the percentage of samples for the validation set
val_split = 0.20  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
88/287:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
88/288:
# Define the CRNN model
class CRNN(nn.Module):
    
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm2d(512),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=False, dropout=0.3)
        self.fc = nn.Linear(256, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/289:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/290:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/291:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/block_crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/292:
# Define the CRNN model
class CRNN(nn.Module):
    
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=512, hidden_size=256, num_layers=4, bidirectional=False, dropout=0.3)
        self.fc = nn.Linear(256, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/293:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/294:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/295:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/block_crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/296:
# Define the CRNN model
class CRNN(nn.Module):
    
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=128, hidden_size=64, num_layers=3, bidirectional=False, dropout=0.3)
        self.fc = nn.Linear(256, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/297:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/298:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/299:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/block_crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/300:
# Define the CRNN model
class CRNN(nn.Module):
    
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=128, hidden_size=64, num_layers=3, bidirectional=False, dropout=0.3)
        self.fc = nn.Linear(64, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/301:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/302:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/303:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/block_crnn_from_scratch_no_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/304:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

# CRNN
# best_model = InterleavedCRNN(num_classes=1)
best_model = CRNN(num_classes=1)

best_model.load_state_dict(torch.load('model/block_crnn_from_scratch_no_erosion.pth'))
best_model = best_model.to(device)
88/305:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/306:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/307:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/308:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/309:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/310: test_correct/len(test_loader.dataset)
88/311: # y_true_cpu, y_pred_cpu
88/312:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
88/313:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
    transforms.Grayscale(),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
88/314:
# Convert transforms to JSON string
transforms_json_train = json.dumps(str(preprocess.transforms), indent=4)
print(transforms_json_train)

# # Extract relevant information from transforms
# transform_info = []
# for t in preprocess.transforms:
#     transform_info.append({
#         'name': t.__class__.__name__,
#         'args': t.__dict__.get('__dict__'),
#         'state_dict': t.state_dict() if hasattr(t, 'state_dict') else None
#     })

# # Convert transform information to JSON string
# transform_json = json.dumps(transform_info, indent=4)

# print(transform_json)
88/315:
! ls data/all_data/handwritten | wc -l
! ls data/all_preprocessed_data/handwritten | wc -l
88/316:
! ls data/all_data/typed | wc -l
! ls data/all_preprocessed_data/typed | wc -l
88/317:
# Define the percentage of samples for the validation set
val_split = 0.20  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nTraining images: ", train_size, "\nValidation images: ", val_size)

# Create indices for train and validation sets
indices = list(range(len(dataset)))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

# Create subset samplers for train and validation sets
train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loaders for train and validation sets
train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
88/318:
# Iterate over the data loader to get a batch of transformed images
for images, labels in train_loader:
    # Convert images to numpy array and transpose dimensions from (batch_size, channels, height, width)
    # to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
#         axes[i].imshow((images[i] * 255).astype(np.uint8))
        axes[i].axis('off')
    plt.show()
    break  # Only display one batch of transformed images
88/319:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
88/320:
print(len(train_loader),
len(val_loader))
88/321: # %tensorboard --logdir logs
88/322:
class VGG16Binary(nn.Module):
    def __init__(self, input_shape, num_classes=1):
        super(VGG16Binary, self).__init__()
        self.features = nn.Sequential(
            
            nn.Conv2d(input_shape[0], 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

        )
        self.classifier = nn.Sequential(
            nn.Linear(256 * (input_shape[1] // 8) * (input_shape[2] // 8), 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.3),
            nn.Linear(2048, 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.3),
            nn.Linear(2048, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
88/323:
# CRNN model

class InterleavedCRNN(nn.Module):
    def __init__(self, num_classes):
        super(InterleavedCRNN, self).__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn1 = nn.GRU(input_size=64*50, hidden_size=32, num_layers=1, bidirectional=True, dropout=0.5)

        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.rnn2 = nn.GRU(input_size=640, hidden_size=64, num_layers=1, bidirectional=True, dropout=0.5)
        
        self.fc = nn.Linear(64 * 2, num_classes)

    def forward(self, x):
        # pass through first conv block
        x = self.conv1(x)

        # reshape for RNN
        b, c, h, w = x.size()
#         print(f'After conv1: {x.shape}')  # Add this to check the shape
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through first RNN block
        x, _ = self.rnn1(x)
#         print(f'After rnn1: {x.shape}')  # Add this to check the shape

        # reshape for conv block
        # x = x.view(b, w, c, h).permute(0, 2, 3, 1).contiguous()
        # b=64, c=64, w=10, h=10
        x = x.view(b, 10, c, 10).permute(0, 2, 3, 1).contiguous()
#         print(f'After reshape: {x.shape}')  # Add this to check the shape

        # pass through second conv block
        x = self.conv2(x)
#         print(f'After conv2: {x.shape}')  # Add this to check the shape

        # reshape for RNN
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).contiguous()
        x = x.view(b, w, -1)

        # pass through second RNN block
        x, _ = self.rnn2(x)
#         print(f'After rnn2: {x.shape}')  # Add this to check the shape
        
        # reshape for fc
        x = x[:, -1, :]

        # pass through FC layer
        x = self.fc(x)
#         print(f'After fc: {x.shape}')  # Add this to check the shape

        return x
88/324:
# Define the CRNN model
class CRNN(nn.Module):
    
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=128, hidden_size=64, num_layers=3, bidirectional=False, dropout=0.3)
        self.fc = nn.Linear(64, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/325:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/326:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/327:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/block_crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/328:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

# CRNN
# best_model = InterleavedCRNN(num_classes=1)
best_model = CRNN(num_classes=1)

best_model.load_state_dict(torch.load('model/block_crnn_from_scratch_w_erosion.pth'))
best_model = best_model.to(device)
88/329:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/330:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/331:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/332:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/333:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/334: test_correct/len(test_loader.dataset)
88/335: # y_true_cpu, y_pred_cpu
88/336:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
88/337:
# Define the CRNN model
class CRNN(nn.Module):
    
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=128, hidden_size=128, num_layers=3, bidirectional=False, dropout=0.3)
        self.fc = nn.Linear(128, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/338:
# Define the CRNN model
class CRNN(nn.Module):
    
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=128, hidden_size=128, num_layers=4, bidirectional=False, dropout=0.3)
        self.fc = nn.Linear(128, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/339:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/340:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/341:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/block_crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/342:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

# CRNN
# best_model = InterleavedCRNN(num_classes=1)
best_model = CRNN(num_classes=1)

best_model.load_state_dict(torch.load('model/block_crnn_from_scratch_w_erosion.pth'))
best_model = best_model.to(device)
88/343:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/344:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/345:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/346:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/347:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/348: test_correct/len(test_loader.dataset)
88/349: # y_true_cpu, y_pred_cpu
88/350:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
88/351:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

# CRNN
# best_model = InterleavedCRNN(num_classes=1)
best_model = CRNN(num_classes=1)

best_model.load_state_dict(torch.load('model/block_crnn_from_scratch_w_erosion.pth'))
best_model = best_model.to(device)
88/352:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/353:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/354:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/355:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/356:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/357: test_correct/len(test_loader.dataset)
88/358:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/359:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/360:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/361:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/362:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/363: test_correct/len(test_loader.dataset)
88/364: # y_true_cpu, y_pred_cpu
88/365:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
88/366: test_loader.batch_size
88/367:
# Define the CRNN model
class CRNN(nn.Module):
    
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=256, hidden_size=256, num_layers=4, bidirectional=False, dropout=0.3)
        self.fc = nn.Linear(256, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/368:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/369:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/370:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/block_crnn_from_scratch_w_erosion.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/371:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/block_crnn_from_scratch_w_erosion_256.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/372:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

# CRNN
# best_model = InterleavedCRNN(num_classes=1)
best_model = CRNN(num_classes=1)

best_model.load_state_dict(torch.load('model/block_crnn_from_scratch_w_erosion_256.pth'))
best_model = best_model.to(device)
88/373:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/374:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/375:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/376:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/377:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/378: test_correct/len(test_loader.dataset)
88/379: # y_true_cpu, y_pred_cpu
88/380:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
88/381:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/382:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/383:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/384:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/385:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/386: test_correct/len(test_loader.dataset)
88/387:
# Define the CRNN model
class CRNN(nn.Module):
    
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=256, hidden_size=256, num_layers=4, bidirectional=False, dropout=0.3)
        self.fc = nn.Linear(256, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/388:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/389:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/390:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/block_crnn_from_scratch_w_erosion_256.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/391:
# Define the CRNN model
class CRNN(nn.Module):
    
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.MaxPool2d(kernel_size=2, stride=2),
#             nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(inplace=True),
#             nn.BatchNorm2d(512),
#             nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.rnn = nn.GRU(input_size=128, hidden_size=256, num_layers=4, bidirectional=False, dropout=0.3)
        self.fc = nn.Linear(256, num_classes)  # Adjusted input size for the linear layer

    def forward(self, x):
        x = self.conv(x)
        x = x.permute(0, 2, 3, 1).contiguous()  # Reshape for RNN
        b, w, h, c = x.size()
        x = x.view(b, -1, c).permute(1, 0, 2)  # merge height and channel dimensions
        _, hidden = self.rnn(x)
        x = hidden[-1]
        x = self.fc(x)
        return x  # Adjusted input size for the linear layer
88/392:
# Set random seed for reproducibility
torch.manual_seed(101)

# Define the number of classes in your specific task
num_classes = 2

# ======================================================================== #

# # Load the pre-trained ResNet model
# model = models.resnet50(weights=True)

# # Modify the first convolutional layer to accept grayscale images
# model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

# # Modify the last fully connected layer to match the number of classes
# num_features = model.fc.in_features
# model.fc = nn.Linear(num_features, num_classes-1)



# # Load the pre-trained VGG16 model
# model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = model.classifier[6].in_features
# model.classifier[6] = torch.nn.Linear(num_features, num_classes-1)



# # Load pretrained MobileNetV2 model and reset final fully connected layer.
# model = models.mobilenet_v2(pretrained=True)
# model.classifier[1] = nn.Linear(model.last_channel, num_classes-1)



# # Load pre-trained DenseNet model
# model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")

# # Get the parameters of the model
# parameters = model.parameters()

# # Specify the number of layers in the begining to unfreeze
# num_layers_to_unfreeze_begin = 25

# # Specify the number of layers in the end to unfreeze
# num_layers_to_unfreeze_end = 10

# # Unlock/unfreeze the last few layers by freezing all the remaining layers
# for param in model.features[:-num_layers_to_unfreeze_end].parameters():
#     param.requires_grad = False


# # Now we unfreeze a few layers in the beginning to help us capture low level features
# # Counter to keep track of the number of unlocked layers
# unlocked_layers = 0

# # Loop over the parameters and set requires_grad=True for the desired number of layers
# for param in parameters:
#     param.requires_grad = True
#     unlocked_layers += 1
#     if unlocked_layers == num_layers_to_unfreeze_begin:
#         break
    
    
# # Replace the last layer for binary classification
# model.classifier = nn.Linear(model.classifier.in_features, num_classes-1)



# # VGG model from scratch
# input_shape = (3, 100, 200)  # input shape (channels, height, width)

# model = VGG16Binary(input_shape=input_shape, num_classes=1)

# CRNN model from scratch
# model = InterleavedCRNN(num_classes=1)

model = CRNN(num_classes=1)

# ======================================================================== #


# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# Define the optimizer
# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
optimizer = optim.Adam(model.parameters(), lr=0.001)

model = model.to(device)
88/393:
# Iterate over the parameters and print the ones with requires_grad=True
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)
88/394:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/block_crnn_from_scratch_w_erosion_256.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/395:
# Training loop
num_epochs = 50

patience = 10
best_val_loss = float('inf')

# Specify the directory where you want to store the logs
writer = SummaryWriter('logs')


for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    
    with tqdm(enumerate(train_loader), total=math.ceil(train_size/batch_size)) as pbar:

#         train_pred = []
#         train_true = []
        
        for batch_idx, (images, labels) in pbar:

            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)
            
            optimizer.zero_grad()

            # Forward pass
            outputs = model(images)

            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
            train_correct += (predicted == labels).sum().item()
            
#             train_true.extend(labels) # Save Truth
#             train_pred.extend(predicted) # Save Prediction

        # Validation loop
        model.eval()
        val_loss = 0.0
        val_correct = 0

        y_pred = []
        y_true = []

        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device)
                labels = labels.float().unsqueeze(1).to(device)

                outputs = model(images)

                loss = criterion(outputs, labels)

                val_loss += loss.item()

                predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
                val_correct += (predicted == labels).sum().item()
                
#                 y_true.extend(labels) # Save Truth
#                 y_pred.extend(predicted) # Save Prediction

                
        # Calculate average loss and accuracy
        train_loss /= train_size
        train_accuracy = train_correct / train_size

        val_loss /= val_size
        val_accuracy = val_correct / val_size
        
        # Compute F1-scores after moving tensors to CPU
#         train_true_cpu = [tensor.cpu().detach().numpy() for tensor in train_true]
#         train_pred_cpu = [tensor.cpu().detach().numpy() for tensor in train_pred]
        
#         y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
#         y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
        
#         train_f1 = f1_score(train_true_cpu, train_pred_cpu)
#         val_f1 = f1_score(y_true_cpu, y_pred_cpu)

#         print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
#         f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1-score: {train_f1:.4f}, \n'
#         f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f} \n'
#         )
        
        print(f'Epoch [{epoch + 1}/{num_epochs}], \n'
        f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \n'
        f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f} \n'
        )
        
        with writer:
            # Write loss and accuracy for training
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)

            # Write loss and accuracy for validation
            writer.add_scalar('Loss/Validation', val_loss, epoch)
            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)
            
            # Define hyperparameters dictionary
            hparam_dict = {
                "learning_rate": 0.001,
                "batch_size": batch_size,
                "num_epochs": num_epochs
            }

            # Define metric dictionary
            metric_dict = {
                "accuracy": val_accuracy,
                "loss": val_loss,
                # "precision": 0.78
            }

            # Write the parameters used for training
            writer.add_hparams(
                metric_dict = metric_dict,
                hparam_dict = hparam_dict
            )
            
            writer.add_text("train_preprocess", transforms_json_train)


        # Save the best model based on validation loss and early stopping
        if val_loss <= best_val_loss:
            best_val_loss = val_loss
            counter = 0
            # Save the model
            print("Saving model...")
            torch.save(model.state_dict(), 'model/block_crnn_from_scratch_w_erosion_256.pth')
        else:
            counter += 1
            # Check if the counter reaches the patience limit
            if counter >= patience:
                print('Early stopping triggered...')
                break

print("\n=== Training complete! ===\n")
writer.close()
88/396:
# Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

# CRNN
# best_model = InterleavedCRNN(num_classes=1)
best_model = CRNN(num_classes=1)

best_model.load_state_dict(torch.load('model/block_crnn_from_scratch_w_erosion_256.pth'))
best_model = best_model.to(device)
88/397:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/398:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/399:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/400:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/401:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/402: test_correct/len(test_loader.dataset)
88/403: # y_true_cpu, y_pred_cpu
88/404:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
88/405:
# Load the best model
best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# Modify the first convolutional layer to accept grayscale images
best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# Modify the last fully connected layer to match the number of classes in your task
num_features = best_model.classifier[6].in_features
best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

# CRNN
# best_model = InterleavedCRNN(num_classes=1)
# best_model = CRNN(num_classes=1)

best_model.load_state_dict(torch.load('model/vgg16_best_model_0-82_128x300.pth'))
best_model = best_model.to(device)
88/406:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/407:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/408:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/409:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/410:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/411: test_correct/len(test_loader.dataset)
88/412: # y_true_cpu, y_pred_cpu
88/413:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
88/414:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/415:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/416:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/417:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/418:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/419: test_correct/len(test_loader.dataset)
88/420: # y_true_cpu, y_pred_cpu
88/421:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
88/422:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/423:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/424:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/425:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/426:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/427: test_correct/len(test_loader.dataset)
88/428: # y_true_cpu, y_pred_cpu
88/429:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
88/430:
# # Load the best model
# best_model = models.vgg16(weights="VGG16_Weights.IMAGENET1K_V1")

# # Modify the first convolutional layer to accept grayscale images
# best_model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)

# # Modify the last fully connected layer to match the number of classes in your task
# num_features = best_model.classifier[6].in_features
# best_model.classifier[6] = torch.nn.Linear(num_features, 1)


# best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# # Modify the model for binary classification
# num_ftrs = best_model.classifier.in_features
# best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

# CRNN
# best_model = InterleavedCRNN(num_classes=1)
best_model = CRNN(num_classes=1)

best_model.load_state_dict(torch.load('model/block_crnn_from_scratch_w_erosion_256.pth'))
best_model = best_model.to(device)
88/431:
# 
test_preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#     PartialErosion(iterations=2),
    transforms.ToTensor(),
#     transforms.Grayscale(num_output_channels=3),
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.Grayscale(),
    transforms.Normalize([0.5], [0.5]),
])
88/432:
# Evaluate the best model on the test dataset
test_dataset = ImageFolder("data/test_data/", transform=test_preprocess)
test_loader = DataLoader(test_dataset, batch_size=32)
88/433:
# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
    print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
    print(images.shape)

    # Plot the images
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))
    for i in range(4):
        axes[i].imshow(images[i])
        axes[i].axis('off')
    plt.show()
    break
88/434:
best_model.eval()
y_pred = []
y_true = []

test_loss = 0
test_correct = 0

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        outputs = best_model(images)

        loss = criterion(outputs, labels)

        test_loss += loss.item()

        predicted = torch.round(torch.sigmoid(outputs))  # Apply sigmoid activation and round to 0 or 1
        test_correct += (predicted == labels).sum().item()

        y_true.extend(labels) # Save Truth
        y_pred.extend(predicted) # Save Prediction


# Compute F1-scores
y_true_cpu = [tensor.cpu().detach().numpy() for tensor in y_true]
y_pred_cpu = [tensor.cpu().detach().numpy() for tensor in y_pred]
88/435:
test_f1 = f1_score(y_true_cpu, y_pred_cpu)
test_f1
88/436: test_correct/len(test_loader.dataset)
88/437: # y_true_cpu, y_pred_cpu
88/438:
import warnings

# Disable the warning
warnings.filterwarnings("ignore", category=UserWarning)

img_no = 0

# Iterate over the data loader to get a batch of transformed images
for images, labels in test_loader:
#     print(images.numpy().shape)
    # Convert images to numpy array and transpose dimensions from 
    # (batch_size, channels, height, width) to (batch_size, height, width, channels)
    images = images.numpy().transpose(0, 2, 3, 1)
    
#     print(images.shape)
    
    num_rows = 8

    # Plot the images
    fig, axes = plt.subplots(num_rows, 4, figsize=(22, 22))
    for i in range(num_rows):
        for j in range(4):
            try:
                axes[i][j].imshow(images[img_no%32])
                axes[i][j].axis('off')
            except IndexError:
                break
            
            edge_color = 'g'
            
            if img_no == len(test_loader.dataset):
                break_flag = True
                break
            if(y_true_cpu[img_no] != y_pred_cpu[img_no]):
                edge_color = 'r'
            
            
            # Create a Rectangle patch
            rect = patches.Rectangle((0, 0), 200, 100, linewidth=10, edgecolor=edge_color, facecolor='none')
            axes[i][j].add_patch(rect)
            img_no += 1
    plt.show()
89/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

from utils.utils import *
89/2:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
89/3:
# vgg_from_scratch_w_erosion_v2.pth

best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked.pth'))
best_model = best_model.to(device)
89/4:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

from utils.utils import *

from collections import defaultdict
89/5:
score_sum_dict = defaultdict(tuple) # file_name: [hw_confidence, typed_confidence]
score_len_dict = defaultdict(tuple) # file_name: [hw_count, typed_count]
89/6: score_sum_dict
89/7: score_sum_dict["lame"] = (10, 10)
89/8: score_sum_dict
89/9: score_sum_dict["lame"]
89/10:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for root, dirs, files in os.walk(craft_output_dir):
    for dir_ in dirs:
        print(dir_)
89/11:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for root, dirs, files in os.walk(craft_output_dir):
    for dir_ in dirs:
        print(dir_.split("_")[0])
89/12:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for root, dirs, files in os.walk(craft_output_dir):
    for dir_ in dirs:
        print(dir_.split("_")[0])
        for file in os.listdir(dir_):
            print(file)
89/13: next(os.walk(craft_output_dir))
89/14: next(next(os.walk(craft_output_dir)))
89/15:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for root, dirs, files in os.walk(craft_output_dir):
    for dir_ in dirs:
        print(dir_.split("_")[0])
    for file in files:
        print(file)
89/16:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dirs in os.listdir(craft_output_dir):
    for dir_ in dirs:
        print(dir_.split("_")[0])
        for file in os.listdir(dir_):
            print(file)
89/17:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dirs in os.listdir(craft_output_dir):
    for dir_ in dirs:
        print(dir_)
        for file in os.listdir(dir_):
            print(file)
89/18:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    print(dir_)
    for file in os.listdir(dir_):
        print(file)
89/19:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    print(dir_)
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        print(file)
89/20:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    print(dir_)
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        print(os.path.join(craft_output_dir, dir_+file))
89/21:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    print(dir_)
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        print(os.path.join(craft_output_dir, dir_+file))
89/22:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    print(dir_)
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        print(craft_output_dir, dir_+file)
89/23:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    print(dir_)
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        print(craft_output_dir, dir_+file)
89/24:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    print(dir_)
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
#         print(craft_output_dir, dir_+file)
89/25:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    print(dir_)
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        pass
#         print(craft_output_dir, dir_+file)
89/26:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    print(dir_)
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        print(dir_+file)
89/27:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    print(dir_)
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        print("/",dir_+file)
89/28:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    print(dir_)
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        print(dir_+"/"+file)
89/29:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        print(dir_+"/"+file)
        
        img = Image.open(craft_output_dir+dir_+"/"+file)
88/439: y_true_cpu, y_pred_cpu
89/30:
score_sum_dict = defaultdict(lambda: (0, 0)) # file_name: (hw_confidence, typed_confidence)
score_len_dict = defaultdict(lambda: (0, 0)) # file_name: (hw_count, typed_count)

score_sum_dict
score_len_dict
89/31:
score_sum_dict = defaultdict(lambda: (0, 0)) # file_name: (hw_confidence, typed_confidence)
score_len_dict = defaultdict(lambda: (0, 0)) # file_name: (hw_count, typed_count)
89/32:
# vgg_from_scratch_w_erosion_v2.pth

best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked.pth'))
best_model = best_model.to(device)
89/33:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

from utils.utils import *

from collections import defaultdict
89/34:
output_dir_craft = "data/Doc_Classification/input"
workdir = "/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/"
# workdir = "data/Doc_Classification/output"

# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft, 
              export_extra = False, 
              text_threshold = .7, 
              link_threshold = .4, 
              crop_type="poly", 
              low_text = .3, 
              cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
    
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
89/35:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

from utils.utils import *

from collections import defaultdict

from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
89/36:
output_dir_craft = "data/Doc_Classification/input"
workdir = "/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/"
# workdir = "data/Doc_Classification/output"

# initialize the CRAFT model
craft = Craft(output_dir = output_dir_craft, 
              export_extra = False, 
              text_threshold = .7, 
              link_threshold = .4, 
              crop_type="poly", 
              low_text = .3, 
              cuda = True)

# CRAFT on images to get bounding boxes
images = []
corrupted_images = []
no_segmentations = []
boxes = {}
count= 0
img_name = []
box = []
file_types = (".jpg", ".jpeg",".png")
    
for filename in tqdm(sorted(os.listdir(workdir))):
    if filename.endswith(file_types):
        image = workdir+filename
        try:
            img = Image.open(image) 
            img.verify() # Check that the image is valid
            bounding_areas = craft.detect_text(image)
            if len(bounding_areas['boxes']): #check that a segmentation was found
                images.append(image)
                boxes[image] = bounding_areas['boxes']
                
            else:
                no_segmentations.append(image)
        except (IOError, SyntaxError) as e:
            corrupted_images.append(image)
89/37:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
89/38:
# vgg_from_scratch_w_erosion_v2.pth

best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked.pth'))
best_model = best_model.to(device)
90/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

from utils.utils import *

from collections import defaultdict

from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
90/2:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
90/3:
# vgg_from_scratch_w_erosion_v2.pth

best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked.pth'))
best_model = best_model.to(device)
90/4:
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    utils.PartialErosion(iterations=2),
    transforms.ToTensor(),
    transforms.Grayscale(num_output_channels=3),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
90/5:
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),
    transforms.Grayscale(num_output_channels=3),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
90/6:
score_sum_dict = defaultdict(lambda: (0, 0)) # file_name: (hw_confidence, typed_confidence)
score_len_dict = defaultdict(lambda: (0, 0)) # file_name: (hw_count, typed_count)
90/7:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        print(dir_+"/"+file)
        
        key = dir_.split("_")[0]
        
        img = Image.open(craft_output_dir+dir_+"/"+file)
        
        best_model.eval()

        # Define the loss function
        criterion = nn.BCEWithLogitsLoss()

        with torch.no_grad():
            img = img.to(img)
            outputs = best_model(img)
            
            pred_confidence = torch.sigmoid(outputs)
            predicted = torch.round(pred_confidence)
            
            score_sum_dict[key] = (1-pred_confidence, pred_confidence)
            
            if(predicted == 0):
                score_len_dict[key][0] += 1
            if(predicted == 1):
                score_len_dict[key][1] += 1
90/8:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        print(dir_+"/"+file)
        
        key = dir_.split("_")[0]
        
        img = Image.open(craft_output_dir+dir_+"/"+file)
        
        best_model.eval()

        # Define the loss function
        criterion = nn.BCEWithLogitsLoss()

        with torch.no_grad():
            img = img.to(device)
            outputs = best_model(img)
            
            pred_confidence = torch.sigmoid(outputs)
            predicted = torch.round(pred_confidence)
            
            score_sum_dict[key] = (1-pred_confidence, pred_confidence)
            
            if(predicted == 0):
                score_len_dict[key][0] += 1
            if(predicted == 1):
                score_len_dict[key][1] += 1
90/9:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        print(dir_+"/"+file)
        
        key = dir_.split("_")[0]
        
        img = Image.open(craft_output_dir+dir_+"/"+file)
        
        best_model.eval()

        # Define the loss function
        criterion = nn.BCEWithLogitsLoss()

        with torch.no_grad():
            outputs = best_model(img)
            
            pred_confidence = torch.sigmoid(outputs)
            predicted = torch.round(pred_confidence)
            
            score_sum_dict[key] = (1-pred_confidence, pred_confidence)
            
            if(predicted == 0):
                score_len_dict[key][0] += 1
            if(predicted == 1):
                score_len_dict[key][1] += 1
90/10:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        print(dir_+"/"+file)
        
        key = dir_.split("_")[0]
        
        img = Image.open(craft_output_dir+dir_+"/"+file)
        
        best_model.eval()

        # Define the loss function
        criterion = nn.BCEWithLogitsLoss()
        
        input_tensor = preprocess(img)

        # Expand dimensions
        input_tensor = torch.unsqueeze(input_tensor, 0)

        with torch.no_grad():
            outputs = best_model(input_tensor)
            
            pred_confidence = torch.sigmoid(outputs)
            predicted = torch.round(pred_confidence)
            
            score_sum_dict[key] = (1-pred_confidence, pred_confidence)
            
            if(predicted == 0):
                score_len_dict[key][0] += 1
            if(predicted == 1):
                score_len_dict[key][1] += 1
90/11:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        print(dir_+"/"+file)
        
        key = dir_.split("_")[0]
        
        img = Image.open(craft_output_dir+dir_+"/"+file)
        
        best_model.eval()

        # Define the loss function
        criterion = nn.BCEWithLogitsLoss()
        
        input_tensor = preprocess(img)

        # Expand dimensions
        input_tensor = torch.unsqueeze(input_tensor, 0)
        input_tensor = input_tensor.to('cuda')

        with torch.no_grad():
            outputs = best_model(input_tensor)
            
            pred_confidence = torch.sigmoid(outputs)
            predicted = torch.round(pred_confidence)
            
            score_sum_dict[key] = (1-pred_confidence, pred_confidence)
            
            if(predicted == 0):
                score_len_dict[key][0] += 1
            if(predicted == 1):
                score_len_dict[key][1] += 1
90/12:
score_sum_dict = defaultdict(lambda: [0, 0]) # file_name: (hw_confidence, typed_confidence)
score_len_dict = defaultdict(lambda: [0, 0]) # file_name: (hw_count, typed_count)
90/13:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        print(dir_+"/"+file)
        
        key = dir_.split("_")[0]
        
        img = Image.open(craft_output_dir+dir_+"/"+file)
        
        best_model.eval()

        # Define the loss function
        criterion = nn.BCEWithLogitsLoss()
        
        input_tensor = preprocess(img)

        # Expand dimensions
        input_tensor = torch.unsqueeze(input_tensor, 0)
        input_tensor = input_tensor.to('cuda')

        with torch.no_grad():
            outputs = best_model(input_tensor)
            
            pred_confidence = torch.sigmoid(outputs)
            predicted = torch.round(pred_confidence)
            
            score_sum_dict[key] = (1-pred_confidence, pred_confidence)
            
            if(predicted == 0):
                score_len_dict[key][0] += 1
            if(predicted == 1):
                score_len_dict[key][1] += 1
90/14:
print(dict(score_sum_dict))
print(dict(score_len_dict))
90/15:
import os
from collections import defaultdict
from tqdm import tqdm
from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.transforms import transforms

from utils.utils import *
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
90/16:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
90/17:
# vgg_from_scratch_w_erosion_v2.pth

best_model = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = best_model.classifier.in_features
best_model.classifier = nn.Linear(num_ftrs, 1)

# best_model = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)  # Create an instance of the model

best_model.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked.pth'))
best_model = best_model.to(device)
90/18:
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
    PartialErosion(iterations=2),
    transforms.ToTensor(),
    transforms.Grayscale(num_output_channels=3),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
90/19:
score_sum_dict = defaultdict(lambda: [0, 0]) # file_name: (hw_confidence, typed_confidence)
score_len_dict = defaultdict(lambda: [0, 0]) # file_name: (hw_count, typed_count)
90/20:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        print(dir_+"/"+file)
        
        key = dir_.split("_")[0]
        
        img = Image.open(craft_output_dir+dir_+"/"+file)
        
        best_model.eval()

        # Define the loss function
        criterion = nn.BCEWithLogitsLoss()
        
        input_tensor = preprocess(img)

        # Expand dimensions
        input_tensor = torch.unsqueeze(input_tensor, 0)
        input_tensor = input_tensor.to('cuda')

        with torch.no_grad():
            outputs = best_model(input_tensor)
            
            pred_confidence = torch.sigmoid(outputs)
            predicted = torch.round(pred_confidence)
            
            score_sum_dict[key][0] += 1-pred_confidence
            score_sum_dict[key][1] += pred_confidence
            
            if(predicted == 0):
                score_len_dict[key][0] += 1
            if(predicted == 1):
                score_len_dict[key][1] += 1
90/21:
print(dict(score_sum_dict))
print(dict(score_len_dict))
90/22:
print(dict(score_sum_dict))
print()
print(dict(score_len_dict))
90/23:
score_avg_dict = defaultdict(lambda: [0, 0])

for sum_, len_ in zip(score_sum_dict, score_avg_dict):
    print(sum_, len_)
90/24: score_avg_dict = defaultdict(lambda: [0, 0])
90/25:
for sum_, len_ in zip(score_sum_dict, score_avg_dict):
    print(sum_, len_)
90/26:
for sum_, len_ in zip(score_sum_dict.items(), score_avg_dict.items()):
    print(sum_, len_)
90/27:
for sum_, len_ in zip(score_sum_dict.items(), score_avg_dict.items()):
    print(sum_.key(), len_)
90/28:
for sum_, len_ in zip(score_sum_dict.items(), score_avg_dict.items()):
    print(score_sum_dict)
90/29:
score_sum_dict = dict(score_sum_dict)
score_len_dict = dict(score_len_dict)
90/30: score_avg_dict = defaultdict(lambda: [0, 0])
90/31:
for sum_, len_ in zip(score_sum_dict.items(), score_avg_dict.items()):
    print(score_sum_dict)
90/32: score_sum_dict
90/33: score_sum_dict.items()
90/34:
for sum_, len_ in zip(score_sum_dict.items(), score_avg_dict.items()):
    print(score_sum_dict)
90/35:
for sum_, len_ in zip(score_sum_dict.items(), score_len_dict.items()):
    print(score_sum_dict)
90/36:
for sum_, len_ in zip(score_sum_dict.items(), score_len_dict.items()):
    print(sum_, len_)
90/37:
for sum_, len_ in zip(score_sum_dict.items(), score_len_dict.items()):
    print(sum_, len_)
    score_avg_dict[sum_[0]] = [sum_[1][0]/len_[1][0], sum_[1][1]/len_[1][1]]
90/38: score_avg_dict.items()
90/39:
for file_name, scores in score_avg_dict.items():
    print(file_name, scores)
90/40:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        # print(dir_+"/"+file)
        
        key = dir_.split("_")[0]
        
        img = Image.open(craft_output_dir+dir_+"/"+file)
        
        best_model.eval()

        # Define the loss function
        criterion = nn.BCEWithLogitsLoss()
        
        input_tensor = preprocess(img)

        # Expand dimensions
        input_tensor = torch.unsqueeze(input_tensor, 0)
        input_tensor = input_tensor.to('cuda')

        with torch.no_grad():
            outputs = best_model(input_tensor)
            
            pred_confidence = torch.sigmoid(outputs)
            predicted = torch.round(pred_confidence)
            
            if(predicted == 0):
                score_sum_dict[key][0] += 1-pred_confidence
                score_len_dict[key][0] += 1
            if(predicted == 1):
                score_sum_dict[key][1] += pred_confidence
                score_len_dict[key][1] += 1
90/41:
score_sum_dict = dict(score_sum_dict)
score_len_dict = dict(score_len_dict)
90/42: score_avg_dict = defaultdict(lambda: [0, 0])
90/43:
for sum_, len_ in zip(score_sum_dict.items(), score_len_dict.items()):
    print(sum_, len_)
    score_avg_dict[sum_[0]] = [sum_[1][0]/len_[1][0], sum_[1][1]/len_[1][1]]
90/44:
for file_name, scores in score_avg_dict.items():
    print(file_name, scores)
90/45:
score_sum_dict = defaultdict(lambda: [0, 0]) # file_name: (hw_confidence, typed_confidence)
score_len_dict = defaultdict(lambda: [0, 0]) # file_name: (hw_count, typed_count)
90/46:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        # print(dir_+"/"+file)
        
        key = dir_.split("_")[0]
        
        img = Image.open(craft_output_dir+dir_+"/"+file)
        
        best_model.eval()

        # Define the loss function
        criterion = nn.BCEWithLogitsLoss()
        
        input_tensor = preprocess(img)

        # Expand dimensions
        input_tensor = torch.unsqueeze(input_tensor, 0)
        input_tensor = input_tensor.to('cuda')

        with torch.no_grad():
            outputs = best_model(input_tensor)
            
            pred_confidence = torch.sigmoid(outputs)
            predicted = torch.round(pred_confidence)
            
            if(predicted == 0):
                score_sum_dict[key][0] += 1-pred_confidence
                score_len_dict[key][0] += 1
            if(predicted == 1):
                score_sum_dict[key][1] += pred_confidence
                score_len_dict[key][1] += 1
90/47:
score_sum_dict = dict(score_sum_dict)
score_len_dict = dict(score_len_dict)
90/48: score_avg_dict = defaultdict(lambda: [0, 0])
90/49:
for sum_, len_ in zip(score_sum_dict.items(), score_len_dict.items()):
    score_avg_dict[sum_[0]] = [sum_[1][0]/len_[1][0], sum_[1][1]/len_[1][1]]
90/50:
hw_score, typed_score = 0

for sum_, len_ in zip(score_sum_dict.items(), score_len_dict.items()):
    if(len_[1][0] == 0):
        hw_score = 0
    elif(len_[1][1] == 0):
        typed_score = 0
    else:
        hw_score = sum_[1][0]/len_[1][0]
        typed_score = sum_[1][1]/len_[1][1]
    score_avg_dict[sum_[0]] = [hw_score, typed_score]
90/51:
hw_score, typed_score = 0, 0

for sum_, len_ in zip(score_sum_dict.items(), score_len_dict.items()):
    if(len_[1][0] == 0):
        hw_score = 0
    elif(len_[1][1] == 0):
        typed_score = 0
    else:
        hw_score = sum_[1][0]/len_[1][0]
        typed_score = sum_[1][1]/len_[1][1]
    score_avg_dict[sum_[0]] = [hw_score, typed_score]
90/52:
for file_name, scores in score_avg_dict.items():
    print(file_name, scores)
90/53:
import os
from collections import defaultdict
from tqdm import tqdm
from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.transforms import transforms
import torch.nn.functional as F

from utils.utils import *
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
90/54:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        # print(dir_+"/"+file)
        
        key = dir_.split("_")[0]
        
        img = Image.open(craft_output_dir+dir_+"/"+file)
        
        best_model.eval()

        # Define the loss function
        criterion = nn.BCEWithLogitsLoss()
        
        input_tensor = preprocess(img)

        # Expand dimensions
        input_tensor = torch.unsqueeze(input_tensor, 0)
        input_tensor = input_tensor.to('cuda')

        with torch.no_grad():
            outputs = best_model(input_tensor)
            
            pred_confidence = F.softmax(outputs, dim=1)
            predicted = torch.round(pred_confidence)
            
            if(predicted == 0):
                score_sum_dict[key][0] += 1-pred_confidence
                score_len_dict[key][0] += 1
            if(predicted == 1):
                score_sum_dict[key][1] += pred_confidence
                score_len_dict[key][1] += 1
90/55:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
        # print(dir_+"/"+file)
        
        key = dir_.split("_")[0]
        
        img = Image.open(craft_output_dir+dir_+"/"+file)
        
        best_model.eval()

        # Define the loss function
        criterion = nn.BCEWithLogitsLoss()
        
        input_tensor = preprocess(img)

        # Expand dimensions
        input_tensor = torch.unsqueeze(input_tensor, 0)
        input_tensor = input_tensor.to('cuda')

        with torch.no_grad():
            outputs = best_model(input_tensor)
            
            pred_probability = F.softmax(outputs, dim=1)
            
            print(pred_probability)
            
            # Get the predicted class and corresponding confidence for the model
            pred_confidence, predicted_class = torch.max(pred_probability, dim=1)
            pred_confidence = pred_confidence.item()
            predicted_class = predicted_class.item()
            
            if(predicted_class == 0):
                score_sum_dict[key][0] += 1-pred_confidence
                score_len_dict[key][0] += 1
            if(predicted_class == 1):
                score_sum_dict[key][1] += pred_confidence
                score_len_dict[key][1] += 1
90/56:
craft_output_dir = "/projectnb/sparkgrp/kabilanm/goodfilescraft/"

for dir_ in os.listdir(craft_output_dir):
    for file in os.listdir(os.path.join(craft_output_dir, dir_)):
#         print(dir_+"/"+file)
        
        key = dir_.split("_")[0]
        
        img = Image.open(craft_output_dir+dir_+"/"+file)
        
        best_model.eval()

        # Define the loss function
        criterion = nn.BCEWithLogitsLoss()
        
        input_tensor = preprocess(img)

        # Expand dimensions
        input_tensor = torch.unsqueeze(input_tensor, 0)
        input_tensor = input_tensor.to('cuda')

        with torch.no_grad():
            outputs = best_model(input_tensor)
            
            pred_confidence = torch.sigmoid(outputs)
            predicted = torch.round(pred_confidence)
            
            if(predicted == 0):
                score_sum_dict[key][0] += 1-pred_confidence
                score_len_dict[key][0] += 1
            if(predicted == 1):
                score_sum_dict[key][1] += pred_confidence
                score_len_dict[key][1] += 1
90/57:
score_sum_dict = dict(score_sum_dict)
score_len_dict = dict(score_len_dict)
90/58: score_avg_dict = defaultdict(lambda: [0, 0])
90/59:
hw_score, typed_score = 0, 0

for sum_, len_ in zip(score_sum_dict.items(), score_len_dict.items()):
    if(len_[1][0] == 0):
        hw_score = 0
    elif(len_[1][1] == 0):
        typed_score = 0
    else:
        hw_score = sum_[1][0]/len_[1][0]
        typed_score = sum_[1][1]/len_[1][1]
    score_avg_dict[sum_[0]] = [hw_score, typed_score]
90/60:
for file_name, scores in score_avg_dict.items():
    print(file_name, scores)
90/61:
workdir = "/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/"

for file in os.listdir(workdir):
    print(file)
90/62:
for file_name, scores in score_avg_dict.items():
    print(file_name+".jpg", scores)
90/63:
workdir = "/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/"

for file_name, scores in score_avg_dict.items():
    print(workdir+file_name+".jpg", scores)
90/64:
workdir = "/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/"

for file_name, scores in score_avg_dict.items():
#     print(workdir+file_name+".jpg", scores)
    print(file_name, scores)
90/65:
workdir = "/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/"

for file_name, avg_scores in score_avg_dict.items():
#     print(workdir+file_name+".jpg", scores)
    print(file_name, avg_scores)
    
    if(avg_scores[0] == avg_scores[1]):
        print("Handwritten")
    if(avg_scores[0] > avg_scores[1]):
        print("Handwritten")
    if(avg_scores[0] < avg_scores[1]):
        print("Typed")
90/66:
import os
import shutil
from collections import defaultdict
from tqdm import tqdm
from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.transforms import transforms
import torch.nn.functional as F

from utils.utils import *
from craft_text_detector import Craft # Need to edit the saving function to prepend 0's
90/67:
input_dir = "/projectnb/sparkgrp/ml-herbarium-grp/ml-herbarium-data/TROCR_Training/goodfiles/"
output_dir = "data/Doc_Classification/output/"

for file_name, avg_scores in score_avg_dict.items():
#     print(input_dir+file_name+".jpg", scores)
    print(file_name, avg_scores)
    
    source_file = input_dir+file_name+".jpg"
    
    # Copy the file using shutil.copy2 to the corresponding directory
    # based on the average prediction score
    if(avg_scores[0] >= avg_scores[1]):
        print("handwritten")
        shutil.copy2(source_file, os.path.join(output_dir, "handwritten"))
    if(avg_scores[0] < avg_scores[1]):
        print("typed")
        shutil.copy2(source_file, os.path.join(output_dir, "typed"))
90/68:
! rm -rf data/Doc_Classification/output/handwritten/*
! rm -rf data/Doc_Classification/output/typed/*
90/69: ! ls data/Doc_Classification/output/handwritten/
90/70:
! ls data/Doc_Classification/output/handwritten/
! ls data/Doc_Classification/output/typed/
90/71:
# --in-dir
# --out-dir
# --model-path
91/1:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_w_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/2:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

# ! pip install tensorboard jupyter-tensorboard
from torch.utils.tensorboard import SummaryWriter

from utils.utils import *
91/3:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_w_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/4:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/5:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/6:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

# ! pip install tensorboard jupyter-tensorboard
from torch.utils.tensorboard import SummaryWriter

from utils.utils import *
91/7:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/8:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

# ! pip install tensorboard jupyter-tensorboard
from torch.utils.tensorboard import SummaryWriter

from utils.utils import *
91/9:
# densenet_10_5_layers_unlocked_w_erosion.pth
# vgg_from_scratch_w_erosion_v2.pth
# densenet_25_10_layers_unlocked_no_erosion.pth
91/10:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/11:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter

from utils.utils import *
91/12:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/13:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = utils.VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/14:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter

from utils.utils import *
91/15:
# densenet_10_5_layers_unlocked_w_erosion.pth
# vgg_from_scratch_w_erosion_v2.pth
# densenet_25_10_layers_unlocked_no_erosion.pth
91/16:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = utils.VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/17:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/18:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter

from utils.utils import PartialErosion, VGG16Binary
91/19:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter

from utils.utils import PartialErosion, VGG16Binary
91/20:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter

from utils.utils import PartialErosion, VGG16Binary
91/21:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter

from utils.utils import *
91/22:
# densenet_10_5_layers_unlocked_w_erosion.pth
# vgg_from_scratch_w_erosion_v2.pth
# densenet_25_10_layers_unlocked_no_erosion.pth
91/23:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/24:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
92/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

# ! pip install tensorboard jupyter-tensorboard
from torch.utils.tensorboard import SummaryWriter

from utils.utils import *
91/25:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/26:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter

from utils.utils import *
91/27:
# densenet_10_5_layers_unlocked_w_erosion.pth
# vgg_from_scratch_w_erosion_v2.pth
# densenet_25_10_layers_unlocked_no_erosion.pth
91/28:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/29:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter

from utils import utils
91/30:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/31:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = utils.VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/32:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter

from utils.utils import VGG16Binary
91/33:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter

from utils.utils import VGG16Binary
91/34:
class VGG16Binary(nn.Module):
    def __init__(self, input_shape, num_classes=1):
        super(VGG16Binary, self).__init__()
        self.features = nn.Sequential(
            
            nn.Conv2d(input_shape[0], 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

        )
        self.classifier = nn.Sequential(
            nn.Linear(256 * (input_shape[1] // 8) * (input_shape[2] // 8), 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.3),
            nn.Linear(2048, 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.3),
            nn.Linear(2048, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
91/35:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = utils.VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/36:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
91/37:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter

from utils.utils import PartialErosion
92/2:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler

# ! pip install tensorboard jupyter-tensorboard
from torch.utils.tensorboard import SummaryWriter

from utils.utils import *
91/38:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter

from utils.utils import PartialErosion, VGG16BinaryClassification
91/39:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter

from utils.utils import PartialErosion, VGG16BinaryClassification
91/40:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter

from utils.utils import PartialErosion, VGG16Binary
93/1:
import os
import json
import math

# ! pip install seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sn

from tqdm import tqdm

from PIL import Image
from sklearn.metrics import confusion_matrix, f1_score

from skimage import morphology, color

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torchvision.transforms.functional import pad, to_pil_image, to_tensor
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.utils.tensorboard import SummaryWriter

from utils.utils import PartialErosion, VGG16Binary
93/2:
# DenseNet model 1
model1 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model1.classifier.in_features
model1.classifier = nn.Linear(num_ftrs, 1)
model1.load_state_dict(torch.load('model/densenet_10_5_layers_unlocked_no_erosion.pth'))

# DenseNet model 2
model2 = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
# Modify the model for binary classification
num_ftrs = model2.classifier.in_features
model2.classifier = nn.Linear(num_ftrs, 1)
model2.load_state_dict(torch.load('model/densenet_25_10_layers_unlocked_no_erosion.pth'))

# VGG16 model trained from scratch
model3 = VGG16Binary(input_shape=(3, 100, 200), num_classes=1)
model3.load_state_dict(torch.load('model/vgg_from_scratch_w_erosion_v2.pth'))
93/3:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
    transforms.Grayscale(),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
93/4:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#     transforms.Grayscale(num_output_channels=3),
    transforms.Grayscale(),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
93/5:
# Define the percentage of samples for the validation set
val_split = 0.20  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))

print("Total number of images: ", len(dataset), 
    "\nValidation images: ", val_size)

# Create indices for validation set
indices = list(range(len(dataset)))
val_indices = indices[train_size:]

# Create subset samplers for validation set
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loader validation set
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
93/6:
# Define the percentage of samples for the validation set
val_split = 0.20  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nValidation images: ", val_size)

# Create indices for validation set
indices = list(range(len(dataset)))
val_indices = indices[train_size:]

# Create subset samplers for validation set
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loader validation set
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
93/7:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
93/8:
# Generate Predictions

all_models = [model1, model2, model3]
all_predictions = []
all_labels = []

# Generate predictions using the trained base models
model.eval()

with torch.no_grad():
    for images, labels in val_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)
        
        all_labels.extend(labels)
        
        for i in range(len(all_models)):

            outputs = all_models[i](images)
            predicted = torch.round(torch.sigmoid(outputs))
            
            try:
                all_predictions[i].extend(predicted)
            except:
                all_predictions.append(predicted)
93/9:
# Generate Predictions

all_models = [model1, model2, model3]
all_predictions = []
all_labels = []

# Generate predictions using the trained base models

with torch.no_grad():
    for images, labels in val_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)
        
        all_labels.extend(labels)
        
        for i in range(len(all_models)):
            all_models[i].eval()

            outputs = all_models[i](images)
            predicted = torch.round(torch.sigmoid(outputs))
            
            try:
                all_predictions[i].extend(predicted)
            except:
                all_predictions.append(predicted)
93/10:
# Generate Predictions

all_models = [model1, model2, model3]
all_predictions = []
all_labels = []

# Generate predictions using the trained base models

with torch.no_grad():
    for images, labels in val_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)
        
        all_labels.extend(labels)
        
        for i in range(len(all_models)):
            print(all_models[i])
            all_models[i].eval()

            outputs = all_models[i](images)
            predicted = torch.round(torch.sigmoid(outputs))
            
            try:
                all_predictions[i].extend(predicted)
            except:
                all_predictions.append(predicted)
93/11:
dataset_path = "data/all_preprocessed_data/"

# Transforms pipeline for train and validation data loaders
preprocess = transforms.Compose([
    transforms.Resize([100, ]),
    transforms.Lambda(
        lambda img: pad(img, padding=(0, 0, max(0, 200 - img.width), max(0, 100 - img.height)), 
                                              fill=(255, 255, 255))),
    transforms.CenterCrop((100, 200)),
#   (erosion followed by !dilation is operated on the grayscale image)
    PartialErosion(iterations=2),
#     Skeletonize(),
    transforms.ToTensor(),

#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.Grayscale(num_output_channels=3),
#     transforms.Grayscale(),
#     transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# Load the entire dataset (since we don't have a train and validation split in the )
dataset = ImageFolder(dataset_path, transform=preprocess)
93/12:
# Define the percentage of samples for the validation set
val_split = 0.20  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nValidation images: ", val_size)

# Create indices for validation set
indices = list(range(len(dataset)))
val_indices = indices[train_size:]

# Create subset samplers for validation set
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loader validation set
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
93/13:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
93/14:
# Generate Predictions

all_models = [model1, model2, model3]
all_predictions = []
all_labels = []

# Generate predictions using the trained base models

with torch.no_grad():
    for images, labels in val_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)
        
        all_labels.extend(labels)
        
        for i in range(len(all_models)):
            print(all_models[i])
            all_models[i].eval()

            outputs = all_models[i](images)
            predicted = torch.round(torch.sigmoid(outputs))
            
            try:
                all_predictions[i].extend(predicted)
            except:
                all_predictions.append(predicted)
93/15:
# Generate Predictions

all_models = [model1, model2, model3]
all_predictions = []
all_labels = []

# Generate predictions using the trained base models

with torch.no_grad():
    for images, labels in val_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)
        
        all_labels.extend(labels)
        
        for i in range(len(all_models)):
#             print(all_models[i])
            all_models[i].eval()

            outputs = all_models[i](images)
            predicted = torch.round(torch.sigmoid(outputs))
            
            try:
                all_predictions[i].extend(predicted)
            except:
                all_predictions.append(predicted)
93/16:
# Generate Predictions

all_models = [model1, model2, model3]
all_predictions = []
all_labels = []

# store models on GPU
for i in range(len(all_models)):
    all_models[i] = all_models[i].to(device)

# Generate predictions using the trained base models

with torch.no_grad():
    for images, labels in val_loader:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)
        
        all_labels.extend(labels)
        
        for i in range(len(all_models)):
#             print(all_models[i])
            all_models[i].eval()

            outputs = all_models[i](images)
            predicted = torch.round(torch.sigmoid(outputs))
            
            try:
                all_predictions[i].extend(predicted)
            except:
                all_predictions.append(predicted)
93/17:
# Generate Predictions

all_models = [model1, model2, model3]
all_predictions = []
all_labels = []

# store models on GPU
for i in range(len(all_models)):
    all_models[i] = all_models[i].to(device)

# Generate predictions using the trained base models

with torch.no_grad():
    with tqdm(enumerate(val_loader), total=math.ceil(val_size/batch_size)) as pbar:
        
        for batch_idx, (images, labels) in pbar:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)

            all_labels.extend(labels)

            for i in range(len(all_models)):
                all_models[i].eval()

                outputs = all_models[i](images)
                predicted = torch.round(torch.sigmoid(outputs))

                try:
                    all_predictions[i].extend(predicted)
                except:
                    all_predictions.append(predicted)
93/18:
# Generate Predictions

all_models = [model1, model2, model3]
all_predictions = []
all_labels = []

# store models on GPU
for i in range(len(all_models)):
    all_models[i] = all_models[i].to(device)

# Generate predictions using the trained base models

with torch.no_grad():
    with tqdm(enumerate(val_loader), total=math.ceil(val_size/batch_size)) as pbar:
        
        for batch_idx, (images, labels) in pbar:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)

            all_labels.extend(labels)

            for i in range(len(all_models)):
                all_models[i].eval()

                outputs = all_models[i](images)
                predicted = torch.round(torch.sigmoid(outputs))

                try:
                    all_predictions[i].extend(predicted)
                except:
                    all_predictions.append(predicted)
93/19:
# Create Meta-Training Set

# Create the meta-training set by concatenating predicted outputs and ground truth labels
meta_train_input = np.concatenate(all_predictions, axis=1)
meta_train_labels = all_labels

# Train the Meta-Model

# Define the meta-model architecture
class MetaModel(nn.Module):
    def __init__(self):
        super(MetaModel, self).__init__()
        self.fc1 = nn.Linear(in_features=2, out_features=10)
        self.fc2 = nn.Linear(in_features=10, out_features=2)

    def forward(self, x):
        x = self.fc1(x)
        x = nn.functional.relu(x)
        x = self.fc2(x)
        return x

# Define the loss function and optimizer for the meta-model
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(meta_model.parameters(), lr=0.001)

def train_meta_model(model, inputs, labels):
    model.train()
    inputs = torch.tensor(inputs, dtype=torch.float32)
    labels = torch.tensor(labels, dtype=torch.long)
    
    for epoch in range(num_epochs):
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

# Train the meta-model using the meta-training set
meta_model = MetaModel()
train_meta_model(meta_model, meta_train_input, meta_train_labels)
93/20: all_predictions[0:10]
93/21: len(all_predictions)
93/22:
# Generate Predictions

all_models = [model1, model2, model3]
all_predictions = []
all_labels = []

# store models on GPU
for i in range(len(all_models)):
    all_models[i] = all_models[i].to(device)

# Generate predictions using the trained base models

with torch.no_grad():
    with tqdm(enumerate(val_loader), total=math.ceil(val_size/batch_size)) as pbar:
        
        for batch_idx, (images, labels) in pbar:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)

            all_labels.extend(labels)

            for i in range(len(all_models)):
                all_models[i].eval()

                outputs = all_models[i](images)
                predicted = torch.round(torch.sigmoid(outputs))
                
                all_predictions[i].extend(predicted)


#                 try:
#                     all_predictions[i].extend(predicted)
#                 except:
#                     all_predictions.append(predicted)
93/23:
# Generate Predictions

all_models = [model1, model2, model3]
all_predictions = []
all_labels = []

# store models on GPU
for i in range(len(all_models)):
    all_models[i] = all_models[i].to(device)
    all_predictions.append([])

# Generate predictions using the trained base models

with torch.no_grad():
    with tqdm(enumerate(val_loader), total=math.ceil(val_size/batch_size)) as pbar:
        
        for batch_idx, (images, labels) in pbar:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)

            all_labels.extend(labels)

            for i in range(len(all_models)):
                all_models[i].eval()

                outputs = all_models[i](images)
                predicted = torch.round(torch.sigmoid(outputs))
                
                all_predictions[i].extend(predicted)


#                 try:
#                     all_predictions[i].extend(predicted)
#                 except:
#                     all_predictions.append(predicted)
93/24: len(all_predictions)
93/25: all_predictions[0:10]
93/26:
# Define the percentage of samples for the validation set
val_split = 0.20  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nValidation images: ", val_size)

# Create indices for validation set
indices = list(range(len(dataset)))
val_indices = indices[train_size:]

# Create subset samplers for validation set
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 32

# Create data loader validation set
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
93/27:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
93/28:
# Generate Predictions

all_models = [model1, model2, model3]
all_predictions = []
all_labels = []

# store models on GPU
for i in range(len(all_models)):
    all_models[i] = all_models[i].to(device)
    all_predictions.append([])

# Generate predictions using the trained base models

with torch.no_grad():
    with tqdm(enumerate(val_loader), total=math.ceil(val_size/batch_size)) as pbar:
        
        for batch_idx, (images, labels) in pbar:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)

            all_labels.extend(labels)

            for i in range(len(all_models)):
                all_models[i].eval()

                outputs = all_models[i](images)
                predicted = torch.round(torch.sigmoid(outputs))
                
                all_predictions[i].extend(predicted)


#                 try:
#                     all_predictions[i].extend(predicted)
#                 except:
#                     all_predictions.append(predicted)
93/29:
# Define the percentage of samples for the validation set
val_split = 0.20  # 20% for validation

# Calculate the sizes of the train and validation sets
val_size = int(val_split * len(dataset))
train_size = len(dataset) - val_size

print("Total number of images: ", len(dataset), 
      "\nValidation images: ", val_size)

# Create indices for validation set
indices = list(range(len(dataset)))
val_indices = indices[train_size:]

# Create subset samplers for validation set
val_sampler = SubsetRandomSampler(val_indices)

batch_size = 64

# Create data loader validation set
val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)
93/30:
# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device = torch.device('cpu')
device
93/31:
# Generate Predictions

all_models = [model1, model2, model3]
all_predictions = []
all_labels = []

# store models on GPU
for i in range(len(all_models)):
    all_models[i] = all_models[i].to(device)
    all_predictions.append([])

# Generate predictions using the trained base models

with torch.no_grad():
    with tqdm(enumerate(val_loader), total=math.ceil(val_size/batch_size)) as pbar:
        
        for batch_idx, (images, labels) in pbar:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)

            all_labels.extend(labels)

            for i in range(len(all_models)):
                all_models[i].eval()

                outputs = all_models[i](images)
                predicted = torch.round(torch.sigmoid(outputs))
                
                all_predictions[i].extend(predicted)
93/32: all_predictions[0]
93/33:
all_predictions_cpu = []

for item in all_predictions:
    all_predictions_cpu.append([tensor.cpu().detach().numpy() for tensor in item])

all_labels_cpu = [tensor.cpu().detach().numpy() for tensor in all_labels]
93/34: all_predictions_cpu[0]
93/35: all_labels_cpu[0]
93/36:
# Create Meta-Training Set

# Create the meta-training set by concatenating predicted outputs and ground truth labels
meta_train_input = np.concatenate(all_predictions, axis=1)
meta_train_labels = all_labels

# Train the Meta-Model

# Define the meta-model architecture
class MetaModel(nn.Module):
    def __init__(self):
        super(MetaModel, self).__init__()
        self.fc1 = nn.Linear(in_features=2, out_features=10)
        self.fc2 = nn.Linear(in_features=10, out_features=2)

    def forward(self, x):
        x = self.fc1(x)
        x = nn.functional.relu(x)
        x = self.fc2(x)
        return x

# Define the loss function and optimizer for the meta-model
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(meta_model.parameters(), lr=0.001)

def train_meta_model(model, inputs, labels):
    model.train()
    inputs = torch.tensor(inputs, dtype=torch.float32)
    labels = torch.tensor(labels, dtype=torch.long)
    
    for epoch in range(num_epochs):
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

# Train the meta-model using the meta-training set
meta_model = MetaModel()
train_meta_model(meta_model, meta_train_input, meta_train_labels)
93/37:
# Create Meta-Training Set

# Create the meta-training set by concatenating predicted outputs and ground truth labels
meta_train_input = np.concatenate(all_predictions_cpu, axis=1)
meta_train_labels = all_labels

# Train the Meta-Model

# Define the meta-model architecture
class MetaModel(nn.Module):
    def __init__(self):
        super(MetaModel, self).__init__()
        self.fc1 = nn.Linear(in_features=2, out_features=10)
        self.fc2 = nn.Linear(in_features=10, out_features=2)

    def forward(self, x):
        x = self.fc1(x)
        x = nn.functional.relu(x)
        x = self.fc2(x)
        return x

# Define the loss function and optimizer for the meta-model
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(meta_model.parameters(), lr=0.001)

def train_meta_model(model, inputs, labels):
    model.train()
    inputs = torch.tensor(inputs, dtype=torch.float32)
    labels = torch.tensor(labels, dtype=torch.long)
    
    for epoch in range(num_epochs):
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

# Train the meta-model using the meta-training set
meta_model = MetaModel()
train_meta_model(meta_model, meta_train_input, meta_train_labels)
93/38:
# Create Meta-Training Set

# Create the meta-training set by concatenating predicted outputs and ground truth labels
meta_train_input = np.concatenate(all_predictions_cpu, axis=1)
meta_train_labels = all_labels

# Train the Meta-Model

# Define the meta-model architecture
class MetaModel(nn.Module):
    def __init__(self):
        super(MetaModel, self).__init__()
        self.fc1 = nn.Linear(in_features=2, out_features=10)
        self.fc2 = nn.Linear(in_features=10, out_features=2)

    def forward(self, x):
        x = self.fc1(x)
        x = nn.functional.relu(x)
        x = self.fc2(x)
        return x
93/39:
# Train the meta-model using the meta-training set
meta_model = MetaModel()

# Define the loss function and optimizer for the meta-model
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(meta_model.parameters(), lr=0.001)

model.train()
inputs = torch.tensor(inputs, dtype=torch.float32)
labels = torch.tensor(labels, dtype=torch.long)

for epoch in range(num_epochs):
    print("Epoch: ", epoch)
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
93/40:
# Train the meta-model using the meta-training set
meta_model = MetaModel()

# Define the loss function and optimizer for the meta-model
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(meta_model.parameters(), lr=0.001)

meta_model.train()
inputs = torch.tensor(inputs, dtype=torch.float32)
labels = torch.tensor(labels, dtype=torch.long)

for epoch in range(num_epochs):
    print("Epoch: ", epoch)
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
93/41:
# Create Meta-Training Set

# Create the meta-training set by concatenating predicted outputs and ground truth labels
meta_train_input = np.concatenate(all_predictions_cpu, axis=1)
meta_train_labels = all_labels

# Train the Meta-Model

# Define the meta-model architecture
class MetaModel(nn.Module):
    def __init__(self):
        super(MetaModel, self).__init__()
        self.fc1 = nn.Linear(in_features=2, out_features=10)
        self.fc2 = nn.Linear(in_features=10, out_features=1)

    def forward(self, x):
        x = self.fc1(x)
        x = nn.functional.relu(x)
        x = self.fc2(x)
        return x
93/42:
# Train the meta-model using the meta-training set
meta_model = MetaModel()

# Define the loss function and optimizer for the meta-model
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(meta_model.parameters(), lr=0.001)

meta_model.train()
inputs = torch.tensor(meta_train_input, dtype=torch.float32)
labels = torch.tensor(meta_train_labels, dtype=torch.long)

for epoch in range(num_epochs):
    print("Epoch: ", epoch)
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
93/43:
# Create Meta-Training Set

# Create the meta-training set by concatenating predicted outputs and ground truth labels
meta_train_input = np.concatenate(all_predictions_cpu, axis=1)
meta_train_labels = all_labels_cpu

# Train the Meta-Model

# Define the meta-model architecture
class MetaModel(nn.Module):
    def __init__(self):
        super(MetaModel, self).__init__()
        self.fc1 = nn.Linear(in_features=2, out_features=10)
        self.fc2 = nn.Linear(in_features=10, out_features=1)

    def forward(self, x):
        x = self.fc1(x)
        x = nn.functional.relu(x)
        x = self.fc2(x)
        return x
93/44:
# Train the meta-model using the meta-training set
meta_model = MetaModel()

# Define the loss function and optimizer for the meta-model
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(meta_model.parameters(), lr=0.001)

meta_model.train()
inputs = torch.tensor(meta_train_input, dtype=torch.float32)
labels = torch.tensor(meta_train_labels, dtype=torch.long)

for epoch in range(num_epochs):
    print("Epoch: ", epoch)
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
93/45:
# Train the meta-model using the meta-training set
meta_model = MetaModel()

# Define the loss function and optimizer for the meta-model
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(meta_model.parameters(), lr=0.001)

meta_model.train()
inputs = torch.tensor(meta_train_input, dtype=torch.float32)
labels = torch.tensor(meta_train_labels, dtype=torch.long)

num_epochs = 10

for epoch in range(num_epochs):
    print("Epoch: ", epoch)
    optimizer.zero_grad()
    outputs = model(inputs)
    outputs = torch.round(nn.Sigmoid(outputs))
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
93/46:
# Train the meta-model using the meta-training set
meta_model = MetaModel()

# Define the loss function and optimizer for the meta-model
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(meta_model.parameters(), lr=0.001)

meta_model.train()
inputs = torch.tensor(meta_train_input, dtype=torch.float32)
labels = torch.tensor(meta_train_labels, dtype=torch.long)

num_epochs = 10

for epoch in range(num_epochs):
    print("Epoch: ", epoch)
    optimizer.zero_grad()
    outputs = meta_model(inputs)
    outputs = torch.round(nn.Sigmoid(outputs))
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
93/47:
# Create Meta-Training Set

# Create the meta-training set by concatenating predicted outputs and ground truth labels
meta_train_input = np.concatenate(all_predictions_cpu, axis=1)
meta_train_labels = all_labels_cpu

# Train the Meta-Model

# Define the meta-model architecture
class MetaModel(nn.Module):
    def __init__(self):
        super(MetaModel, self).__init__()
        self.fc1 = nn.Linear(in_features=3, out_features=10)
        self.fc2 = nn.Linear(in_features=10, out_features=1)

    def forward(self, x):
        x = self.fc1(x)
        x = nn.functional.relu(x)
        x = self.fc2(x)
        return x
93/48:
# Train the meta-model using the meta-training set
meta_model = MetaModel()

# Define the loss function and optimizer for the meta-model
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(meta_model.parameters(), lr=0.001)

meta_model.train()
inputs = torch.tensor(meta_train_input, dtype=torch.float32)
labels = torch.tensor(meta_train_labels, dtype=torch.long)

num_epochs = 10

for epoch in range(num_epochs):
    print("Epoch: ", epoch)
    optimizer.zero_grad()
    outputs = meta_model(inputs)
    outputs = torch.round(nn.Sigmoid(outputs))
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
93/49:
# Train the meta-model using the meta-training set
meta_model = MetaModel()

# Define the loss function and optimizer for the meta-model
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(meta_model.parameters(), lr=0.001)

meta_model.train()
inputs = torch.tensor(meta_train_input, dtype=torch.float32)
labels = torch.tensor(meta_train_labels, dtype=torch.long)

num_epochs = 10

for epoch in range(num_epochs):
    print("Epoch: ", epoch)
    optimizer.zero_grad()
    outputs = meta_model(inputs)
    outputs = torch.round(torch.sigmoid(outputs))
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
93/50:
# Train the meta-model using the meta-training set
meta_model = MetaModel()

# Define the loss function and optimizer for the meta-model
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(meta_model.parameters(), lr=0.001)

meta_model.train()
inputs = torch.tensor(meta_train_input, dtype=torch.float32)
labels = torch.tensor(meta_train_labels, dtype=torch.float32)

num_epochs = 10

for epoch in range(num_epochs):
    print("Epoch: ", epoch)
    optimizer.zero_grad()
    outputs = meta_model(inputs)
    outputs = torch.round(torch.sigmoid(outputs))
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
93/51: torch.save(meta_model.state_dict(), 'model/meta_model_ann.pth')
93/52:
# Create Meta-Training Set

# Create the meta-training set by concatenating predicted outputs and ground truth labels
meta_train_input = np.concatenate(all_predictions_cpu, axis=1)
meta_train_labels = all_labels_cpu

# Train the Meta-Model

# Define the meta-model architecture
class MetaModel(nn.Module):
    def __init__(self):
        super(MetaModel, self).__init__()
        self.fc1 = nn.Linear(in_features=3, out_features=10)
        self.fc2 = nn.Linear(in_features=10, out_features=15)
        self.fc3 = nn.Linear(in_features=15, out_features=5)
        self.fc4 = nn.Linear(in_features=5, out_features=1)

    def forward(self, x):
        x = self.fc1(x)
        x = nn.functional.relu(x)
        x = self.fc2(x)
        x = nn.functional.relu(x)
        x = self.fc3(x)
        x = nn.functional.relu(x)
        x = self.fc4(x)
        return x
93/53:
# Train the meta-model using the meta-training set
meta_model = MetaModel()

# Define the loss function and optimizer for the meta-model
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(meta_model.parameters(), lr=0.001)

meta_model.train()
inputs = torch.tensor(meta_train_input, dtype=torch.float32)
labels = torch.tensor(meta_train_labels, dtype=torch.float32)

num_epochs = 10

for epoch in range(num_epochs):
    print("Epoch: ", epoch)
    optimizer.zero_grad()
    outputs = meta_model(inputs)
    outputs = torch.round(torch.sigmoid(outputs))
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
93/54: torch.save(meta_model.state_dict(), 'model/meta_model_ann.pth')
93/55:
# Validate and Test

# Generate predictions using the trained meta-model
meta_predictions_validation = meta_model.predict(meta_train_input)
meta_predictions_test = meta_model.predict(test_set_input)

# Evaluate the performance of the stacked model
validate(meta_predictions_validation, validation_set.labels)
test(meta_predictions_test, test_set.labels)
93/56: meta_predictions_validation
93/57:
# Validate and Test

# Generate predictions using the trained meta-model
meta_predictions_validation = meta_model(meta_train_input)
meta_predictions_test = meta_model(test_set_input)

# Evaluate the performance of the stacked model
validate(meta_predictions_validation, validation_set.labels)
test(meta_predictions_test, test_set.labels)
93/58:
# Train the meta-model using the meta-training set
meta_model = MetaModel()

# Define the loss function and optimizer for the meta-model
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(meta_model.parameters(), lr=0.001)

meta_model.train()
inputs = torch.tensor(meta_train_input, dtype=torch.float32)
labels = torch.tensor(meta_train_labels, dtype=torch.float32)

num_epochs = 10

for epoch in range(num_epochs):
    optimizer.zero_grad()
    outputs = meta_model(inputs)
    outputs = torch.round(torch.sigmoid(outputs))
    loss = criterion(outputs, labels)
    
    print("Epoch: ", epoch, " Loss: ", loss)
    
    loss.backward()
    optimizer.step()
93/59:
# Train the meta-model using the meta-training set
meta_model = MetaModel()

# Define the loss function and optimizer for the meta-model
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(meta_model.parameters(), lr=0.001)

meta_model.train()
inputs = torch.tensor(meta_train_input, dtype=torch.float32)
labels = torch.tensor(meta_train_labels, dtype=torch.float32)

num_epochs = 10

for epoch in range(num_epochs):
    optimizer.zero_grad()
    outputs = meta_model(inputs)
    outputs = torch.round(torch.sigmoid(outputs))
    loss = criterion(outputs, labels)
    
    print("Epoch: ", epoch, " Loss: ", loss.item())
    
    loss.backward()
    optimizer.step()
93/60:
# Train the meta-model using the meta-training set
meta_model = MetaModel()

# Define the loss function and optimizer for the meta-model
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(meta_model.parameters(), lr=0.001)

meta_model.train()
inputs = torch.tensor(meta_train_input, dtype=torch.float32)
labels = torch.tensor(meta_train_labels, dtype=torch.float32)

num_epochs = 10

for epoch in range(num_epochs):
    optimizer.zero_grad()
    outputs = meta_model(inputs)
    outputs = torch.round(torch.sigmoid(outputs))
    loss = criterion(outputs, labels)
    
    print("Epoch: ", epoch+1, " Loss: ", loss.item())
    
    loss.backward()
    optimizer.step()
93/61:
# Validate and Test

# Generate predictions using the trained meta-model
meta_predictions_validation = meta_model(inputs)
meta_predictions_test = meta_model(test_set_input)

# Evaluate the performance of the stacked model
validate(meta_predictions_validation, validation_set.labels)
test(meta_predictions_test, test_set.labels)
93/62: meta_predictions_validation
93/63:
# Validate and Test

# Generate predictions using the trained meta-model
meta_predictions_validation = torch.round(torch.sigmoid(meta_model(inputs)))
meta_predictions_test = meta_model(test_set_input)

# Evaluate the performance of the stacked model
validate(meta_predictions_validation, validation_set.labels)
test(meta_predictions_test, test_set.labels)
93/64: meta_predictions_validation
93/65: meta_predictions_validation == labels
93/66: (meta_predictions_validation == labels).sum().item()
93/67: (meta_predictions_validation != labels).sum().item()
93/68: inputs
96/1:
import os
import json
import re
import sys
import shutil
import csv
import glob

from PIL import Image, UnidentifiedImageError
import torch
from torchvision.transforms import transforms
from torchvision.transforms.functional import affine, adjust_sharpness

%matplotlib inline
import numpy as np
# ! pip install scikit-image
import skimage.io as io
import matplotlib.pyplot as plt
import pylab

from utils import utils
from craft_text_detector import Craft
from tqdm import tqdm
95/1:
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.ensemble import AdaBoostClassifier

# Define your base neural network model
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()

    def forward(self, x):
        pass

# Create a list to store the base models
base_models = []

# Create a list to store the weights of each base model
base_model_weights = []

# Create a list to store the training data weights
data_weights = []

# Initialize the training data weights with equal weights
# You can modify this based on your specific dataset
num_samples = len(train_data)
initial_weight = 1 / num_samples
data_weights = [initial_weight] * num_samples

# Define the number of base models
num_base_models = 10
95/2:
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.ensemble import AdaBoostClassifier
95/3:
# Define your base neural network model
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()

    def forward(self, x):
        pass
93/69:
class DeeperHybridModel(nn.Module):
    def __init__(self, num_numeric_features, num_classes):
        super(DeeperHybridModel, self).__init__()

        # Image input (assuming image of size 64x64, and grayscale)
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Flatten()  # Flatten the 3D tensor output from CNN
        )

        # Numeric input
        self.fc1 = nn.Sequential(
            nn.Linear(num_numeric_features, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
        )

        # Combined input
        self.fc2 = nn.Sequential(
            nn.Linear(2048 + 128, 256),  # Adjust based on previous layer outputs
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes),
            nn.Sigmoid()
        )

    def forward(self, image_input, numeric_input):
        x1 = self.cnn(image_input)
        x2 = self.fc1(numeric_input)
        x = torch.cat((x1, x2), dim=1)  # Concatenate the output tensors along dimension 1
        x = self.fc2(x)
        return x
101/1: %history -g
101/2: %history -g | grep "Document"
101/3: %history -g | grep "return x"
101/4: %history -g -f Document_Classification.ipynb
101/5: xit
   1: %history -g -f history.txt
